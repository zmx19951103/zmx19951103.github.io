<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[unordered_map实现]]></title>
    <url>%2F2019%2F12%2F05%2Funordered-map%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[weak_ptr实现]]></title>
    <url>%2F2019%2F12%2F05%2Fweak-ptr%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言weak_ptr是为了解决shared_ptr存在的循环引用问题而诞生的。它依赖于shared_ptr,不能直接管理一块动态开辟的空间。 循环引用问题weak_ptr实现参考]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++模板类的分离式编译]]></title>
    <url>%2F2019%2F12%2F05%2Fc-%E6%A8%A1%E6%9D%BF%E7%B1%BB%E7%9A%84%E5%88%86%E7%A6%BB%E5%BC%8F%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[前言在实现自己的shared_ptr的时候，习惯性的将模板类的接口和实现分别放在对应的.h文件和.cc文件中，然后在test.cc文件中包含.h头文件，进行测试，结果总是报undefinend reference的错误，一开始感觉很奇怪，我明明在编译的时候将模板类的.cc文件给包含进来了，可是居然在链接的时候找不到。最后查到了原因。原来编译器对模板类的分离式编译支持的非常差，比如STL的代码中都是将模板类的接口和实现放在一起的。虽然有可以分离编译的方法，但是主流还是将接口和实现放在同一个.h文件中，简单而且跨平台兼容。 之所以分离式编译的时候出现问题，是因为我们的模板类的.cc文件中只include模板类的.h文件，然后也没有对具体typename类型的调用(没有调用所以没有隐式实例化)，所以编译成的模板类的.o文件中是没有具体实例化的模板类的代码的，所以链接的时候找不到实例化类的代码。 分离式编译[1]DSharedPtr.h #include &lt;unordered_map&gt;#include &lt;iostream&gt;template&lt;typename T&gt;class DSharedPtr&#123;private:T * _ptr;static std::unordered_map&lt;T*,int&gt; _cnt;public:DSharedPtr(T* ptr=nullptr);~DSharedPtr();DSharedPtr(DSharedPtr&lt;T&gt;&amp; src);DSharedPtr&amp; operator=(DSharedPtr&lt;T&gt;&amp; src);T&amp; operator*();T* operator-&gt;();&#125;; [2]DSharedPtr.cc#include "./DSharedPtr.h"template&lt;typename T&gt;std::unordered_map&lt;T*,int&gt; DSharedPtr&lt;T&gt;::_cnt;template&lt;typename T&gt;DSharedPtr&lt;T&gt;::DSharedPtr(T* ptr)&#123; _ptr = ptr;if(_cnt.count(ptr)==0)_cnt[ptr] = 1;else _cnt[ptr]+=1;&#125;template&lt;typename T&gt;DSharedPtr&lt;T&gt;::~DSharedPtr()&#123;--_cnt[_ptr];if(_cnt[_ptr]&lt;=0) &#123;_cnt.erase(_ptr);delete _ptr; // delete nullptr is ok &#125;&#125;template&lt;typename T&gt;DSharedPtr&lt;T&gt;::DSharedPtr(DSharedPtr&lt;T&gt;&amp; src)&#123;_ptr = src._ptr;_cnt[_ptr]+=1;&#125;template&lt;typename T&gt;DSharedPtr&lt;T&gt;&amp; DSharedPtr&lt;T&gt;::operator=(DSharedPtr&lt;T&gt;&amp; src)&#123;if(_ptr==src._ptr) return *this;--_cnt[_ptr];if (_cnt[_ptr]&lt;=0)&#123;_cnt.erase(_ptr);delete _ptr;&#125;_ptr = src._ptr;_cnt[_ptr]+=1;return *this;&#125;template&lt;typename T&gt;T&amp; DSharedPtr&lt;T&gt;::operator*()&#123;return *_ptr;&#125;//why template&lt;typename T&gt;T* DSharedPtr&lt;T&gt;::operator-&gt;()&#123;return _ptr;&#125;templateclass DSharedPtr&lt;int&gt;; // 类模板的显示实例化 [3]test.cc#include "./DSharedPtr.h"#include &lt;iostream&gt;using namespace std;int main()&#123;int *p = new int(10);DSharedPtr&lt;int&gt; mshared_p1(p);DSharedPtr&lt;int&gt; mshared_p2(new int(20));cout &lt;&lt; *mshared_p1 &lt;&lt; endl;cout &lt;&lt; *mshared_p2 &lt;&lt; endl;return 0;&#125; 因为有类模板的显示实例化，所以上面的程序可以编译运行。 将接口和实现放在同一个.h文件中不过最主流的做法还是将模板类的接口和实现放在同一个.h文件中,如下: [1]DSharedPtr.h #include &lt;unordered_map&gt;#include &lt;iostream&gt;template&lt;typename T&gt;class DSharedPtr&#123;private:T * _ptr;static std::unordered_map&lt;T*,int&gt; _cnt;public:DSharedPtr(T* ptr=nullptr);~DSharedPtr();DSharedPtr(DSharedPtr&lt;T&gt;&amp; src);DSharedPtr&amp; operator=(DSharedPtr&lt;T&gt;&amp; src);T&amp; operator*();T* operator-&gt;();&#125;;emplate&lt;typename T&gt;std::unordered_map&lt;T*,int&gt; DSharedPtr&lt;T&gt;::_cnt;template&lt;typename T&gt;DSharedPtr&lt;T&gt;::DSharedPtr(T* ptr)&#123; _ptr = ptr;if(_cnt.count(ptr)==0)_cnt[ptr] = 1;else _cnt[ptr]+=1;&#125;template&lt;typename T&gt;DSharedPtr&lt;T&gt;::~DSharedPtr()&#123;--_cnt[_ptr];if(_cnt[_ptr]&lt;=0) &#123;_cnt.erase(_ptr);delete _ptr; // delete nullptr is ok &#125;&#125;template&lt;typename T&gt;DSharedPtr&lt;T&gt;::DSharedPtr(DSharedPtr&lt;T&gt;&amp; src)&#123;_ptr = src._ptr;_cnt[_ptr]+=1;&#125;template&lt;typename T&gt;DSharedPtr&lt;T&gt;&amp; DSharedPtr&lt;T&gt;::operator=(DSharedPtr&lt;T&gt;&amp; src)&#123;if(_ptr==src._ptr) return *this;--_cnt[_ptr];if (_cnt[_ptr]&lt;=0)&#123;_cnt.erase(_ptr);delete _ptr;&#125;_ptr = src._ptr;_cnt[_ptr]+=1;return *this;&#125;template&lt;typename T&gt;T&amp; DSharedPtr&lt;T&gt;::operator*()&#123;return *_ptr;&#125;//why template&lt;typename T&gt;T* DSharedPtr&lt;T&gt;::operator-&gt;()&#123;return _ptr;&#125; 这样就不用类模板的显示实例化。上面的shared_ptr代码需要进一步改进，来支持多线程下的线程安全以及存在循环引用的问题。进一步改进的代码会在中smart pointer进行更新。 参考c++模板类的分离式编译]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[new与malloc的区别]]></title>
    <url>%2F2019%2F12%2F04%2Fnew%E4%B8%8Emalloc%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[shared_ptr实现]]></title>
    <url>%2F2019%2F12%2F04%2Fshared-ptr%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[前言c++面试经常会问到智能指针，我们来实现一下shared_ptr，探究一下智能指针的原理。shared_ptr的核心就是【引用计数】的实现，还有shared_ptr貌似在多线程的环境下是有问题的。智能指针主要是用来解决原生指针可能造成的问题，比如忘记手动释放内存造成内存泄漏等等。 参考]]></content>
      <tags>
        <tag>STL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[多个进程调用同一个.so文件]]></title>
    <url>%2F2019%2F12%2F04%2F%E5%A4%9A%E4%B8%AA%E8%BF%9B%E7%A8%8B%E8%B0%83%E7%94%A8%E5%90%8C%E4%B8%80%E4%B8%AA-so%E6%96%87%E4%BB%B6%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[vtable]]></title>
    <url>%2F2019%2F12%2F04%2Fvtable%2F</url>
    <content type="text"><![CDATA[前言我们知道c++的运行时动态是由虚函数来实现的,而虚函数的实现方式不同编译器不同，一个比较典型的实现方式就是使用vtable,vtabl中存储的是每个虚函数的地址, 并在对象中存储一个vptr,指向vtable.我们可以用指针来获取vtable，并且调用vtable中的成员函数。 vtable#include &lt;iostream&gt;#include&lt;cstdio&gt;using namespace std;class Base&#123;public: Base()&#123;;&#125; virtual void f() &#123; cout &lt;&lt;"Base:hello world" &lt;&lt;endl; &#125; virtual void g() &#123; cout &lt;&lt;"Base:g" &lt;&lt;endl; &#125; virtual void h() &#123; cout &lt;&lt;"Base:h" &lt;&lt;endl; &#125;&#125;;class Base_2&#123;public:Base_2()&#123;&#125;virtual void y()&#123;cout&lt;&lt; "Base_2:y()" &lt;&lt;endl;&#125;&#125;;class Derive: public Base, public Base_2&#123;public: Derive()&#123; a = 6;&#125; void f() &#123; cout &lt;&lt;"hello world" &lt;&lt;endl; cout &lt;&lt;a&lt;&lt;endl; &#125; void g() &#123; cout&lt;&lt;"Derive:g"&lt;&lt;endl; &#125; virtual void t()&#123; cout&lt;&lt;"zmx"&lt;&lt;endl; &#125; int a;&#125;;typedef void(*Fun)(Derive* ptr);int main() &#123; Derive *q = new Derive(); Derive *p = new Derive(); // Derive *q = dynamic_cast&lt;Derive*&gt;( p); long address = *(long*)q; long address2 = *(long*)p; cout&lt;&lt; address&lt;&lt; endl; cout&lt;&lt; address2&lt;&lt; endl; Fun fun= (Fun)(*(long*)address); fun(q); fun = (Fun)(*(((long*)address)+1)); fun(q); fun = (Fun)(*(((long*)address)+2)); fun(q); fun = (Fun)(*(((long*)address)+3)); fun(q); address = *((long*)q+1); cout&lt;&lt;address&lt;&lt;endl; fun= (Fun)(*(long*)address); fun(q); // fun = (Fun)(*(((long*)address)+1)); // fun(q); // fun = (Fun)(*(((long*)address)+3)); // fun();/* printf("sizeof(int)) :%d\n",sizeof(int)); printf("sizeof(long) :%d\n",sizeof(long)); printf("sizeof(short) :%d\n",sizeof(short)); printf("sizeof(char) :%d\n",sizeof(char)); printf("sizeof(float) :%d\n",sizeof(float)); printf("sizeof(double):%d\n",sizeof(double)); printf("sizeof(void*):%d\n",sizeof(void*)); printf("sizeof(long long):%d\n",sizeof(long long));*/ return 0;&#125; 程序的运行结果如下： 在Linux64机器下指针的大小和long的大小一样都是8字节，我们从图中可以看到两个Derive对象中的vptr是一样的，也就是指向同一个vtable,也就是说同类对象的vtable是同一个。而且我们可以看到如果一个派生类有两个基类，两个基类都有virtual function，那么对应于每一个基类都会有各自的虚函数表(vtable),而且如果派生类中还有新的virtual function,会放在第一个基类的vtable中(按照基类的声明顺序),当然如果派生类有重写基类的虚方法，那么vtable中对应的函数会被替换成派生类中的。因为我们的两个基类都没有数据成员，所以派生类对象中的基类部分只有vptr,所以我在获取第二个基类的vptr的时候，直接在第一个基类的地址上移动了一个位置。如果Base类中有一个long类型的成员数据，那么我就得移动两个位置，那才是Base_2类的vptr。可以直接在上面的程序上修改并且运行看看结果。 总结对c++对象模型有了实际的体验，有了进一步的理解. 参考vtablevtablevtable]]></content>
      <tags>
        <tag>c++</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[mmap]]></title>
    <url>%2F2019%2F12%2F03%2Fmmap%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Raft]]></title>
    <url>%2F2019%2F11%2F30%2FRaft%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[实现一个协程dreamcoroutine]]></title>
    <url>%2F2019%2F11%2F30%2F%E5%AE%9E%E7%8E%B0%E4%B8%80%E4%B8%AA%E5%8D%8F%E7%A8%8Bdreamcoroutine%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[强化学习调优]]></title>
    <url>%2F2019%2F11%2F29%2F%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[bayesian optimization]]></title>
    <url>%2F2019%2F11%2F29%2Fbayesian-optimization%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[深度学习框架的Layer和Net抽象:装饰器模式]]></title>
    <url>%2F2019%2F11%2F28%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E7%9A%84Layer%E5%92%8CNet%E6%8A%BD%E8%B1%A1-%E8%A3%85%E9%A5%B0%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言现在完成了GraphExecutor,测试了最简单的一个例子，另外op的正确性先不测试，我打算先把Layer,Net和Solver的整体结构搭建出来。打算参考Caffe，但是肯定要简化好多好多。 LayerNet参考]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++可调用对象:仿函数，lambda表达式]]></title>
    <url>%2F2019%2F11%2F27%2Fc-%E5%8F%AF%E8%B0%83%E7%94%A8%E5%AF%B9%E8%B1%A1-%E4%BB%BF%E5%87%BD%E6%95%B0%EF%BC%8Clambda%E8%A1%A8%E8%BE%BE%E5%BC%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[/dev/urandom]]></title>
    <url>%2F2019%2F11%2F27%2Fdev-urandom%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[caffe的随机数产生方式]]></title>
    <url>%2F2019%2F11%2F27%2Fcaffe%E7%9A%84%E9%9A%8F%E6%9C%BA%E6%95%B0%E4%BA%A7%E7%94%9F%E6%96%B9%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[前言完成了DL框架的执行器，现在需要测试一下，需要为参数给定随机初始值，这里参考caffe的初始值生成方式,从源码来看使用的是boost库，caffe库的代码很不错，后续还要看考它的layer, net的设计。 boost::mt19937参考boost::mt19937]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[虚函数表]]></title>
    <url>%2F2019%2F11%2F26%2F%E8%99%9A%E5%87%BD%E6%95%B0%E8%A1%A8%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[noexcept]]></title>
    <url>%2F2019%2F11%2F26%2Fnoexcept%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[std::function]]></title>
    <url>%2F2019%2F11%2F26%2Fstd-function%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[memory pool]]></title>
    <url>%2F2019%2F11%2F26%2Fmemory-pool%2F</url>
    <content type="text"><![CDATA[前言最近继续写DL框架，因为涉及到频繁的new内存，开销很大会拖慢速度，所以准备用memory pool, 先开一个比较大的空间，这样的程序性能应该能提高。 理论首先我们来学习一下内存池的理论，从字面意思上来看，内存池里应该有很多的内存，当我们需要一块内存的时候我们不用new或者malloc,而是从内存池里去拿就行，这样就减少了分配内存的开销，从而提升性能，不知道一次new操作会带来多大的开销呢? 参考]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[tensorflow Distribution Strategy API]]></title>
    <url>%2F2019%2F11%2F25%2Ftensorflow-Distribution-Strategy-API%2F</url>
    <content type="text"><![CDATA[前言参考Distribution Strategy API]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[SparseTensor]]></title>
    <url>%2F2019%2F11%2F24%2FSparseTensor%2F</url>
    <content type="text"><![CDATA[前言最近看论文发现tf的Tensor分为 dense tensor 和 sparse tensor. 而 这篇论文就是对不同类型的tensor使用不同的梯度聚合架构，从而提出了一个混合架构,用于分布式深度学习数据并行训练。 参考Update only part of the word embedding matrix in Tensorflow]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[vector clock]]></title>
    <url>%2F2019%2F11%2F24%2Fvector-clock%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[distributed kv store]]></title>
    <url>%2F2019%2F11%2F24%2Fdistributed-kv-store%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[paxos and raft]]></title>
    <url>%2F2019%2F11%2F24%2Fpaxos-and-raft%2F</url>
    <content type="text"><![CDATA[参考github paxos raft]]></content>
      <tags>
        <tag>distributed system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[in graph and between graph]]></title>
    <url>%2F2019%2F11%2F24%2Fin-graph-and-between-graph%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[MIT6.824 lab1 mapreduce]]></title>
    <url>%2F2019%2F11%2F24%2FMIT6-824-lab1-mapreduce%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[std::mutex]]></title>
    <url>%2F2019%2F11%2F24%2Fstd-mutex%2F</url>
    <content type="text"><![CDATA[前言最近看ps-lite代码，因为代码很多地方会涉及到多线程，有互斥量，条件变量等等。对c++11的多线程没怎么学过，所以就学习了一下。 std::mutex参考]]></content>
      <tags>
        <tag>c++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MPI_Irecv]]></title>
    <url>%2F2019%2F11%2F22%2FMPI-Irecv%2F</url>
    <content type="text"><![CDATA[前言看baidu的allreduce实现，第一部分的ring reduce scatter用到了这个函数，所以来学习一下。 参考MPI_Irecv]]></content>
      <tags>
        <tag>MPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[a hybrid framework providing ps and allreduce architecture for distributed deep learning]]></title>
    <url>%2F2019%2F11%2F22%2Fa-hybrid-framework-providing-ps-and-allreduce-architecture-for-distributed-deep-learning%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[byteps源码解析(-)]]></title>
    <url>%2F2019%2F11%2F22%2Fbyteps%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[ps-lite实现异步sgd]]></title>
    <url>%2F2019%2F11%2F22%2Fps-lite%E5%AE%9E%E7%8E%B0%E5%BC%82%E6%AD%A5sgd%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[ps-lite的通信初始化]]></title>
    <url>%2F2019%2F11%2F22%2Fps-lite%E7%9A%84%E9%80%9A%E4%BF%A1%E5%88%9D%E5%A7%8B%E5%8C%96%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[在ps-lite中添加对变长value的支持]]></title>
    <url>%2F2019%2F11%2F22%2F%E5%9C%A8ps-lite%E4%B8%AD%E6%B7%BB%E5%8A%A0%E5%AF%B9%E5%8F%98%E9%95%BFvalue%E7%9A%84%E6%94%AF%E6%8C%81%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[MPI SendRecv]]></title>
    <url>%2F2019%2F11%2F19%2FMPI-SendRecv%2F</url>
    <content type="text"><![CDATA[前言baidu allreduce 代码中用SendRecv实现了ring Allgather, 如果用 Send和Receive来实现的话，因为两个都是阻塞的，所以会死锁，一个解决方法是使用Irecv和Send,另一种方法就是使用SendRecv。 实践该函数的接口如下:int MPI_Sendrecv(const void *sendbuf, int sendcount, MPI_Datatype sendtype, int dest, int sendtag, void *recvbuf, int recvcount, MPI_Datatype recvtype, int source, int recvtag, MPI_Comm comm, MPI_Status *status) 每个参数都顾名思义，sendbuf是要发送的数据的地址，sendcount和sendtype是要发送数据的数量和类型，dest是目的进程的rank, sendtag和recvtag对应就好。recvbuf和recvcount是接收数据的缓冲区和接收的数量以及接收的类型,source表示从哪个进程接收,status应该是receive操作的状态。 我们用一个ring的例子来实践一下该函数,代码如下：#include&lt;stdio.h&gt;#include "mpi.h"#include &lt;math.h&gt;int main(int argc, char* argv[])&#123; int nProcs, Rank, i; double A0,A1; MPI_Status status; MPI_Init(&amp;argc, &amp;argv); MPI_Comm_size(MPI_COMM_WORLD, &amp;nProcs); MPI_Comm_rank(MPI_COMM_WORLD, &amp;Rank); // for(int i=0; i&lt;n; i++)&#123; // A0[i] = Rank; // A1[i] = Rank; // &#125; printf("\nBefore exchange A0 A1:\n"); //for(i=0;i&lt;n;i++)&#123; printf("rank:%d\t%f\t%f\n",Rank, A0, A1); //&#125; int rightrank = (Rank + 1) % nProcs; int leftrank = (Rank + nProcs-1)%nProcs; MPI_Barrier(MPI_COMM_WORLD); MPI_Sendrecv(&amp;A0, 1, MPI_DOUBLE, rightrank,990, &amp;A1, 1, MPI_DOUBLE, leftrank,990, MPI_COMM_WORLD,&amp;status); MPI_Finalize(); printf("After exchange A0 A1\n"); // for(i=0;i&lt;n;i++)&#123; printf("rank:%d %f\t%f\n",Rank, A0, A1); // &#125;&#125; 该代码的运行结果如下，可以看到每个进程都从它左边的进程接收了A1,因为每个进程都发送A0,所以A0是不变的 参考MPI_SendRecvMPI_SendRecv]]></content>
      <tags>
        <tag>MPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MPI Bcast,Gather,AllGather]]></title>
    <url>%2F2019%2F11%2F19%2FMPI-Bcast-Gather-AllGather%2F</url>
    <content type="text"><![CDATA[前言这几天看allreduce算法，发现基本的概念还是MPI那一套，而NCCL,baidu-allreduce都是集合通信的具体实现。所以回顾一下各个函数的意义。 MPI_ReduceMPI_Scatterscatter是把root进程拥有的一个数组分块，块数等于进程数，并根据进程的秩来发送对应块，第一块发送给进程0，以此类推。函数接口如下：MPI_Scatter( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator) 测试代码如下： 但是存在一个问题，如果数组的长度不能被进程数整除怎么办。 MPI_Bcastbroadcast就是把root进程的数据发送到其他的所有进程。示意图如下:函数接口如下： MPI_GatherMPI_Gather的作用就是把其他所有进程的数据收集到root进程来，和reduce不同的是没有reduce操作,只是单纯的收集。示意图如下： 接口如下:MPI_Gather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, int root, MPI_Comm communicator) send_data, send_count以及send_datatype都是发送数据的参数，意思也是很明显的。receive同理，这里有一个root参数用来指定root进程。对于root进程需要秩指定recv_data,其他进程可以传递NULL给recv_data,实践发现对于其他进程recv_count和recv_datatype也是可以胡乱指定的。程序如下： #include&lt;mpi.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;stdexcept&gt;#include&lt;cstring&gt;int main(int argc,char* argv[])&#123;int mpi_error ;mpi_error = MPI_Init(NULL,NULL);if(mpi_error!=MPI_SUCCESS)&#123; throw std::runtime_error("MPI_Init failed with an error");&#125;int rank,size;mpi_error = MPI_Comm_rank(MPI_COMM_WORLD,&amp;rank);if(mpi_error!=MPI_SUCCESS)&#123;throw std::runtime_error("MPI_Comm_rank failed with an error");&#125;mpi_error = MPI_Comm_size(MPI_COMM_WORLD,&amp;size);if(mpi_error!=MPI_SUCCESS)&#123;throw std::runtime_error("MPI_Comm_size failed with an error");&#125;size_t length = rank;std::vector&lt;size_t&gt; lengths(size);//MPI_Allgather(&amp;length,1,MPI_UNSIGNED_LONG,&amp;lengths[0],1,MPI_UNSIGNED_LONG,MPI_COMM_WORLD);if(rank==0)&#123;MPI_Allgather(&amp;length,1,MPI_UNSIGNED_LONG,&amp;lengths[0],1,MPI_UNSIGNED_LONG,MPI_COMM_WORLD);for(auto t:lengths)&#123; std::cout&lt;&lt; t &lt;&lt; std::endl;&#125;&#125;else&#123;MPI_Allgather(&amp;length,1,MPI_UNSIGNED_LONG,NULL,1,MPI_UNSIGNED_LONG,MPI_COMM_WORLD);&#125;MPI_Finalize();return 0;&#125; MPI_AllgatherMPI_Gather将其他所有进程的数据收集到root进程上,与MPI_Scatter相反。而MPI_Allgather也是收集所有进程上的数据，但是它不仅仅将数据收集到root进程，而是收集到所有的进程，图示如下：在每个进程上，收到的数据都会按照发送该数据的进程的秩来排序。Allgather的接口如下:MPI_Allgather( void* send_data, int send_count, MPI_Datatype send_datatype, void* recv_data, int recv_count, MPI_Datatype recv_datatype, MPI_Comm communicator) send_data,send_count,send_datatype分别是每个进程要发送的数据地址，发送的数据量以及数据的类型，需要注意的是接收部分的参数，recv_data是接收数据缓冲区，recv_count表示的是从每个进程接收多少数据而不是从其他进程接收的总的数据，这个要注意。其他的都很清楚。我们用一个简单的程序来实践一下，我们用MPI process的秩来初始化该进程的length变量,然后将length发送出去，当Allgather过程结束后，每个进程应该会有一个 数组，长度为总的进程数，然后里面的值为每个进程的length变量值，按照进程的秩排好序的。程序如下：#include&lt;mpi.h&gt;#include&lt;iostream&gt;#include&lt;vector&gt;#include&lt;stdexcept&gt;#include&lt;cstring&gt;int main(int argc,char* argv[])&#123;int mpi_error ;mpi_error = MPI_Init(NULL,NULL);if(mpi_error!=MPI_SUCCESS)&#123; throw std::runtime_error("MPI_Init failed with an error");&#125;int rank,size;mpi_error = MPI_Comm_rank(MPI_COMM_WORLD,&amp;rank);if(mpi_error!=MPI_SUCCESS)&#123;throw std::runtime_error("MPI_Comm_rank failed with an error");&#125;mpi_error = MPI_Comm_size(MPI_COMM_WORLD,&amp;size);if(mpi_error!=MPI_SUCCESS)&#123;throw std::runtime_error("MPI_Comm_size failed with an error");&#125;size_t length = rank;std::vector&lt;size_t&gt; lengths(size);MPI_Allgather(&amp;length,1,MPI_UNSIGNED_LONG,&amp;lengths[0],1,MPI_UNSIGNED_LONG,MPI_COMM_WORLD);if(rank==0)&#123;for(auto t:lengths)&#123; std::cout&lt;&lt; t &lt;&lt; std::endl;&#125;&#125;MPI_Finalize();return 0;&#125; 开了10个进程，运行结果如下： 参考MPI AllgatherMPI scatter gather allgather]]></content>
      <tags>
        <tag>MPI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[horovod]]></title>
    <url>%2F2019%2F11%2F17%2Fhorovod%2F</url>
    <content type="text"><![CDATA[参考horovod 知乎]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[给ps-lite添加Angel中的psFunc]]></title>
    <url>%2F2019%2F11%2F16%2F%E7%BB%99ps-lite%E6%B7%BB%E5%8A%A0Angel%E4%B8%AD%E7%9A%84psFunc%2F</url>
    <content type="text"><![CDATA[前言实验室项目想把ps设计的更好，要求借鉴Angel提供psFunc,以便后续能提供对一些传统ML算法的支持，包括GBDT,LDA等。一开始并不是很懂这个psFunc有什么用，看了一下Angel的文档，当然这个Angel也是实验室和腾讯合作搞出来的。 psFuncpsFunc是在sever端的一些自定义函数，最简单的ps只提供模型的存储功能，但是针对一些特定的算法我们需要一些优化，如果我们要对模型参数进行一些操作得到某个结果，如果ps只提供了最简单的pull功能，那我们就要把模型参数从ps上全部拉过来，然后在worker上计算得到结果。这样的做法网络通信开销很大，如果server端能够提供这样的功能，那worker发一个请求，server端执行计算返回一个结果数值就好了，这样网络通信开销会大幅度降低，毕竟server也是有计算能力的，浪费掉很可惜啊。这样的想法很直观。感觉优化提升应该也不错。如下是Angel的文档: 设计参考Angel 2.0 用户手册]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReduceScatter+Allgather]]></title>
    <url>%2F2019%2F11%2F16%2FReduceScatter-Allgather%2F</url>
    <content type="text"><![CDATA[前言Ring Allreduce架构也是一种常用的梯度同步架构，它的通信成本与gpu的数量无关。它是Allreduce算法的一种高效实现。目前常用的分布式深度学习的框架基本就是ps架构和Ring Allreduce架构。在这篇博客中，我会首先来回顾一下最简单的Allreduce算法的实现，然后是Ring Allreduce算法的实现，并给出baidu all-reduce的源码分析。 理论Allreduce 是一种集合通信原语，它的规范的定义如下： AllReduce is an operation that reduces the target arrays in all processes to a single array and returns the resultant array to all processes. 简单来说就是有很多个进程，每个进程都有一个目标数组，Allreduce要做的就是将所有进程中的数组收集起来做一个reduce操作成单个的数组，然后将这单个的数组重新发送给所有的进程。就是一个reduce+broadcast的操作。很容易理解，要实现Allreduce一个最简单的算法就是用一个process当master，其他的进程把自己的数组发送给master, master收集完后做一个reduce操作，然后再将结果数组发送给其余的所有进程，和ps架构一样。图示如下： 我们计算一下这种实现的通信量,假设数组的长度是N,总共有P个进程,那么第一步有P-1个进程将数组发送到master进程,然后master进程将reduce后的数组发送给P-1个进程，所以master总共的通信量是2(P-1)*N. 可以看到master的通信量和gpu的数量是线性关系的。而且master的带宽限制使得master成了一个瓶颈。 而Ring Allreduce作为Allreduce算法的一个高效实现，它消除了master的瓶颈，使得系统的通信成本和gpu的数量无关，是一个恒定值。在该算法中，所有的gpu排列成一个环，我们给gpu标号，从0~N-1.按照顺时针的方向，每个gpu会有一个左边的邻居和右边的邻居，每个迭代中，当前gpu从左邻居中接受数据，发送数据给右边的邻居.图示如下： 我们来具体介绍算法流程: 首先，在每个gpu上，把长度为N的数组切分成P个chunks,P是进程的数量。接下来进行P-1次Scatter-Reduce迭代，第一次迭代，编号为n的gpu,将第n块发送给它的右邻居，从它的左邻居接收第n-1块。后面的每次迭代，每块gpu都将上一轮迭代接收到的块进行reduce操作后发送给它的右邻居。以下是第一次迭代，第二次迭代和最后一次迭代的图示： 可以看到P-1次Reduce-Scatter迭代完成后，每个gpu都拥有最终结果的其中一块。我们需要把每个gpu中的这个块发送到其他gpu，这次每个gpu接收到块以后不是reduce操作，是直接覆盖对应块的内容即可。方法是通过P-1次Allgather迭代，第一次迭代，第n个gpu发送第n+1个块接收第n个块.后续的迭代和Reduce-Scatter一样，也是发送上一轮迭代接收的到的块.下面是第一轮，第二轮以及最后的图示： 我们分析这个ring算法的总的通信量, 总共是2(p-1)次迭代，每个迭代每块gpu都发送 N/P 的数据，所以每个gpu发送的数据量和接收的数据量都是 2(P-1)N/P, 最后的结果与P无关。通过对比可以看到，ring allreduce算法的通信量更少而且与gpu数量无关，扩展性更好。 其实allreduce算法有很多种，还有 tree-allreduce等，像Rabit库中实现的应该是tree-allreduce, 后续可以补充。ring allreduce在通信上应该是最优的. 本篇博客中的所有图都截取自参考资料[5]. 实践我们通过对baidu all-reduce源码的解读和应用来实践ring all-reduce算法.baidu allreduce源码的层次结构如下：可以看到源码的量很少，其中最关键的就是collectives.cu文件，实现了 Ring Allreduce和 Ring Allgather两个函数。下面我们来看一些help function,理解了这些help function,再看最重要的部分就都能理解了。 Reference[1]Ring Allreduce[2]All Reduce[3]Rabit[4]baidu all-reduce[5]Bringing HPC Techniques to Deep Learning]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[bytescheduler在byteps中的实现]]></title>
    <url>%2F2019%2F11%2F16%2Fbytescheduler%E5%9C%A8byteps%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[cuda Stream]]></title>
    <url>%2F2019%2F11%2F14%2Fcuda-Stream%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[python的with语句以及__enter__和__exit__]]></title>
    <url>%2F2019%2F11%2F10%2Fpython%E7%9A%84with%E8%AF%AD%E5%8F%A5%E4%BB%A5%E5%8F%8A-enter-%E5%92%8C-exit%2F</url>
    <content type="text"><![CDATA[前言想写一个DL的框架，前端想模仿tensorflow来写，看到 with tf.Graph().as_default, 就想看看Graph类如何定义，看到一个simpleflow的github项目，里面Graph的定义里有 enter和exit方法，不明白是什么意思就随手一查，反而让我知道了with语法的内部机制,算是一个比较大的收获。 参考python enter 与 exit的作用，以及与 with 语句的关系]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读论文《A Generic Communication Scheduler for Distributed DNN Training Acceleration》]]></title>
    <url>%2F2019%2F10%2F28%2F%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E3%80%8AA-Generic-Communication-Scheduler-for-Distributed-DNN-Training-Acceleration%E3%80%8B%2F</url>
    <content type="text"><![CDATA[AbstractIntroduction]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ typename的用法]]></title>
    <url>%2F2019%2F09%2F24%2Fc-typename%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[生产者消费者模型]]></title>
    <url>%2F2019%2F09%2F19%2F%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[c++11 std::thread]]></title>
    <url>%2F2019%2F09%2F18%2Fc-11-std-thread%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[同步sgd/异步sgd]]></title>
    <url>%2F2019%2F09%2F10%2F%E5%90%8C%E6%AD%A5sgd-%E5%BC%82%E6%AD%A5sgd%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读: Parameter Server for Distributed Machine Learning]]></title>
    <url>%2F2019%2F08%2F30%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Parameter-Server-for-Distributed-Machine-Learning%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[参数同步架构和算法]]></title>
    <url>%2F2019%2F08%2F30%2F%E5%8F%82%E6%95%B0%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[ring AllReduce]]></title>
    <url>%2F2019%2F08%2F27%2Fring-AllReduce%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[FM/DeepFM/NFM]]></title>
    <url>%2F2019%2F08%2F22%2FFM-DeepFM-NFM%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[分布式爬虫+推荐]]></title>
    <url>%2F2019%2F08%2F16%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB-%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[分布式文件系统]]></title>
    <url>%2F2019%2F08%2F16%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[GBDT]]></title>
    <url>%2F2019%2F08%2F07%2FGBDT%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[meta learning]]></title>
    <url>%2F2019%2F08%2F05%2Fmeta-learning%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[自动微分]]></title>
    <url>%2F2019%2F08%2F05%2F%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%2F</url>
    <content type="text"><![CDATA[符号微分数值微分前向自动微分反向自动微分参考一天实现自己的自动微分自动微分简介tensorflow 自动微分c++ 实现自动微分wikpedia automatic differentiation自动微分法是如何用c++实现的]]></content>
      <tags>
        <tag>deep learning system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beam search]]></title>
    <url>%2F2019%2F08%2F05%2Fbeam-search%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型]]></title>
    <url>%2F2019%2F08%2F05%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[RNN梯度消失和爆炸]]></title>
    <url>%2F2019%2F08%2F04%2FRNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8%2F</url>
    <content type="text"></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BPTT]]></title>
    <url>%2F2019%2F08%2F04%2FBP%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-BPTT%2F</url>
    <content type="text"><![CDATA[BPTT (BackPropagation Through Time)用于循环神经网络的BP算法.首先我们回顾一下RNN,RNN的网络结构图如下： 标准RNN的数学表达式如下: s(t) = Ux(t)+Wh(t-1)+bh(t) = tanh(s(t))z(t) = Vh(t)+c\hat{y(t)} = softmax(z(t))$ U $ , $ W $ , $ V $ 是待优化的参数. 上面的表达式可以和图对应,RNN网络的参数在所有时间步都是共享的.我们参考材料[2]的记法，将公式细化如下:首先我们计算s(t)的第j个元素,我们假设输入 $ x(t) $的维度是 $ l $ , 隐藏层的状态 $ h(t) $ 的维度是 $ m $ : s_j(t) = \sum_{i=1}^{l} {x_i(t)u_{ji} + \sum_{k=1}^{m} { h_k(t-1)wjk }}+b_j下面我们计算 $ z(t) $ 的第j个元素: z_j(t) = \sum_{q=1}^{m} {h_q(t)v_{jq}}+c_j因为每个时间步的输出都是一个概率向量，表示该时间步的词所属于各标签的概率分布，如果用one-hot向量表示真实标签$ y_t $，那么输出$ \hat{y_t} $和$ y_t $可以计算交叉熵损失$ E_t $, 公式如下： E_t = -{y_t}^T \ln( \hat{y_t})E = \sum_{t=1}^{T} {E_t}我们要用损失函数 $ E $ 对 $ U $ , $ W $ , $ V $ 求偏导数，然后更新参数,我么使用sgd 算法所以每次更新使用一个样本. 我们拿出一个时间步的损失，并就该损失求$ U $ , $ W $ , $ V $ 的更新，结构图如下：我们假设输出的概率分布的维度是 $ o $ , 并把时间维度 $ t $ 舍去, 那么: E_t = -{y_t}^T \ln( \hat{y_t}) = \sum_{k=1}^{o} { -y_k \ln \hat{y_k} }\frac{ \partial E_t }{ \partial \hat{y_k} } = - \frac{y_k}{ \hat{y_k} }\frac{ \partial \hat{y_k} }{ \partial z_j } = \begin{cases} \hat{y_j}(1- \hat{y_j} ) & \text{ k = j }\\ - \hat{y_k} { \hat{y_j }}& \text{ j != k } \end{cases}综上： \frac{ \partial E_t }{ \partial z_j } = \sum_{k=1}^{o} { \frac{ \partial E_t}{ \partial \hat{y_k} } \frac{ \partial \hat{y_k} }{ \partial z_j } } = y_j - \hat{y_j}\frac{ \partial z_j }{ \partial v_{jq}} = h_q\frac{ \partial E_t }{ \partial v_{jq}} = \frac{ \partial E_t }{ \partial z_j } \frac{ \partial z_j }{ \partial v_{jq}} =( y_j - \hat{y_j} )h_q根据上面的式子，我们能对矩阵v的所有元素进行更新.下面我们计算 $ U $ 和 $ W $ 的更新式子: \frac{ \partial E_t }{ \partial h_q } = \sum_{j=1}^{o} {\frac{ \partial E_t}{ \partial z_j} \frac{ \partial z_j}{ \partial h_q} } = \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}}\frac{ \partial E_t }{ \partial s_q } = \frac{ \partial E_t }{ \partial h_q } \frac{ \partial h_q }{ \partial s_q } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2)\frac{ \partial E_t }{ \partial u_{qi} } = \frac{ \partial E_t }{ \partial s_q } \frac{ \partial s_q }{ \partial u_{qi} } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2) x_i\frac{ \partial E_t }{ \partial w_{qk} } = \frac{ \partial E_t }{ \partial s_q } \frac{ \partial s_q }{ \partial w_{qk} } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2) h_k(t-1)W更新\frac{ \partial E}{ \partial W} = \sum_{t=1}^{T} { \frac{ \partial E_t }{ \partial W }}\frac{ \partial E_t }{ \partial W } = \frac{ \partial E_t }{ \partial \hat{y_t} } \frac{ \partial \hat{y_t} }{ \partial W }\frac{ \partial E_t }{ \partial \hat{y_t} } = - \frac{y_t}{ \hat{y_t}}这里的除法是element-wise的.然后是$ \frac{ \partial \hat{y_t} }{ \partial W } $ ，这是一个向量对一个矩阵求偏导 参考BPTT blogbptt]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[simpledb(1)]]></title>
    <url>%2F2019%2F08%2F04%2Fsimpledb-1%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[c++11之std::move]]></title>
    <url>%2F2019%2F07%2F29%2Fc-11%E4%B9%8Bstd-move%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[c++11之多线程std::thread]]></title>
    <url>%2F2019%2F07%2F29%2Fc-11%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8Bstd-thread%2F</url>
    <content type="text"></content>
      <tags>
        <tag>c++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlite architecture]]></title>
    <url>%2F2019%2F07%2F29%2Fsqlite-architecture%2F</url>
    <content type="text"></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[database index]]></title>
    <url>%2F2019%2F07%2F29%2Fdatabase-index%2F</url>
    <content type="text"></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B/B+ Tree]]></title>
    <url>%2F2019%2F07%2F27%2FB-B-Tree%2F</url>
    <content type="text"></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习分布式训练]]></title>
    <url>%2F2019%2F07%2F26%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[并行模式数据并行模型并行混合并行参考]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[architerture of sqlite]]></title>
    <url>%2F2019%2F07%2F26%2Farchiterture-of-sqlite%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[sqlite源码剖析(1)]]></title>
    <url>%2F2019%2F07%2F26%2Fsqlite%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-1%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[傅立叶变换及其加强版FFT]]></title>
    <url>%2F2019%2F07%2F26%2F%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%8A%A0%E5%BC%BA%E7%89%88FFT%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[caffe中的卷积计算的实现im2col]]></title>
    <url>%2F2019%2F07%2F26%2Fcaffe%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0im2col%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[GPU内存层次]]></title>
    <url>%2F2019%2F07%2F25%2FGPU%E5%86%85%E5%AD%98%E5%B1%82%E6%AC%A1%2F</url>
    <content type="text"></content>
      <tags>
        <tag>CUDA Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[knowledge distillation]]></title>
    <url>%2F2019%2F07%2F24%2Fknowledge-distillation%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[cudnn实践]]></title>
    <url>%2F2019%2F07%2F23%2Fcudnn%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[矩阵乘法优化cublas]]></title>
    <url>%2F2019%2F07%2F23%2F%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E4%BC%98%E5%8C%96cublas%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[SIMD]]></title>
    <url>%2F2019%2F07%2F23%2Fintel%E5%90%91%E9%87%8F%E5%8C%96%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[SIMD 向量化]]></content>
      <tags>
        <tag>parallel programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[model parallelism/data parallelism]]></title>
    <url>%2F2019%2F07%2F23%2Fmodel-parallelism-data-parallelism%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[BSP/SSP/A-SGD]]></title>
    <url>%2F2019%2F07%2F23%2FBSP-SSP-A-SGD%2F</url>
    <content type="text"><![CDATA[参考Angel 异步控制]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BatchNormalization]]></title>
    <url>%2F2019%2F07%2F23%2FBatchNormalization%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Dropout]]></title>
    <url>%2F2019%2F07%2F23%2FDropout%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[mixed precision]]></title>
    <url>%2F2019%2F07%2F23%2Fmixed-precision%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[REVISITING DISTRIBUTED SYNCHRONOUS SGD]]></title>
    <url>%2F2019%2F07%2F23%2FREVISITING-DISTRIBUTED-SYNCHRONOUS-SGD%2F</url>
    <content type="text"><![CDATA[Referencerevisiting distributed synchronous sgdDC-ASGD]]></content>
      <tags>
        <tag>distributed machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe源码(1)]]></title>
    <url>%2F2019%2F07%2F22%2Fcaffe%E6%BA%90%E7%A0%81-1%2F</url>
    <content type="text"></content>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词向量]]></title>
    <url>%2F2019%2F07%2F20%2F%E8%AF%8D%E5%90%91%E9%87%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[模型压缩]]></title>
    <url>%2F2019%2F07%2F20%2F%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[bert]]></title>
    <url>%2F2019%2F07%2F20%2Fbert%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[attention is all you need]]></title>
    <url>%2F2019%2F07%2F20%2Fattention-is-all-you-need%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[各种激活函数总结]]></title>
    <url>%2F2019%2F07%2F20%2F%E5%90%84%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[sequence2sequence实现机器翻译]]></title>
    <url>%2F2019%2F07%2F12%2Fsequence2sequence%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[bilstm+CRF实现命名实体识别]]></title>
    <url>%2F2019%2F07%2F12%2Fbilstm-CRF%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[BiLSTMCRFTensorflow implementation参考Bidirectional LSTM-CRF Models for Sequence Tagging]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++11之Deducing Types]]></title>
    <url>%2F2019%2F07%2F12%2Fc-11%E4%B9%8BDeducing-Types%2F</url>
    <content type="text"></content>
      <tags>
        <tag>c++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CUDA in Actions]]></title>
    <url>%2F2019%2F07%2F12%2FCUDA-in-Actions%2F</url>
    <content type="text"><![CDATA[简介&nbsp;&nbsp;之前我们用cuda实现了打印hello world以及向量相加. 这篇博客，我们来探究如何利用gpu的并行性. gpu的power就在于它的并行性. &lt;&lt;&lt;…&gt;&gt;&gt;语法&nbsp;&nbsp;&lt;&lt;&lt;…&gt;&gt;&gt;是kernel函数执行的设置，比如用几个线程来执行核函数. cuda 将线程组织成线程块(thread block)，kernel可以启动很多线程块，并且把它们组织成grid数据结构(grid).&nbsp;&nbsp;核函数启动设置的语法是&lt;&lt;&lt; M , T &gt;&gt;&gt; &nbsp;&nbsp;表示kernel函数执行启动了一个grid,这个grid有M个线程块(thread block),每个线程块有T个线程. threadIdx.x, blockDim.x 以及 blockIdx.x&nbsp;&nbsp;cuda提供了内置变量来获取线程信息，这里我们使用两个. threadIdx.x 表示线程块中的线程编号(以0开始), blockIdx.x表示线程块中的线程数.因为可以获取线程信息，所以函数内部可以根据当前的线程来决定执行怎样的操作.比如向量相加，不同的线程计算不同区间的向量相加，先获取当前线程号，根据线程号确定向量相加的范围，执行计算.blockDim.x表示一个thread block的线程数量. 并行计算 vector addition假设一个线程块有256个线程 __global__ void vector_add(float *out, float *a, float *b, int n) &#123; int index = threadIdx.x; int stride = blockDim.x; for(int i = index; i &lt; n; i += stride)&#123; out[i] = a[i] + b[i]; &#125;&#125; 上面的代码的想法可以用如下的图来说明: 不同线程写不同位置，不存在冲突，这里突然想起来，如果按照一个线程的写法但是开多个线程运行，是不是应该会冲突？？之后可以学习一下gpu线程的同步问题. 我们用nvprof来验证用1个线程和256个线程完成向量相加的执行情况，主要看执行时间： 可以看到执行时间明显缩短了.现在的程序只开了一个thread block,我们尝试开启多个thread block. 多个thread block并行计算向量相加&nbsp;&nbsp;cuda提供了内置的变量来获取thread block的信息，包括block的编号(blockIdx.x)，一个grid有多少个blocks(gridDim.x).&nbsp;&nbsp;使用多个grid来并行化向量相加的示意图如下所示: &nbsp;&nbsp;想法就是每个block有256个thread,每个thread负责一个计算一个元素相加，然后总共有 N/256个thread block,因为N不一定是256的倍数，所以在核函数中还要判断index是否小于N..这样每个元素都是同时计算的,进一步加大并行化.期望的执行时间也应该减少. 对应的程序如下：__global__ void vector_add(float *out, float *a, float *b, int n) &#123; int i = blockIdx.x*blockDim.x+threadIdx.x; if(i&lt;n)out[i] = a[i] + b[i]; 程序运行性能如下: 性能比较N=700000 version Execution Time(ms) Speedup 1 thread 977.26 1.00x 1 block 5.5417 176.35x Multiple blocks 0.13274 7360.25x 总结这篇博客分别通过用一个thread block,每个thread block 256个线程和多个thread block，每个thread block 256个线程计算向量相加来展示如何使用gpu来并行计算.其中强调了三个概念 grid , threadblock以及 thread….接下来我们学习一个gpu的架构. 参考Tutorial 02 CUDA in Actions]]></content>
      <tags>
        <tag>CUDA Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM,RNN,Bi-LSTM,On-LSTM]]></title>
    <url>%2F2019%2F07%2F12%2FLSTM-RNN-Bi-LSTM-On-LSTM%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[CUDA线程层次-从硬件和软件角度]]></title>
    <url>%2F2019%2F07%2F10%2FGPU%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Streaming MultiprocessorsGPU是由流多处理器构成的，每个SM内部有多个core, 每个core跑一个thread. The idea is that the CPU spawns a thread per element, and the GPU then executes those threads.Not all of the thousands or millions of threads actually run in parallel, but many do.Specifically, an NVIDIA GPU contains several largely independent processors called “Streaming Multiprocessors” (SMs), each SM hosts several “cores”, and each “core” runs a thread.For instance, Fermi has up to 16 SMs with 32 cores per SM – so up to 512 thread can run in parallel. All threads running on the cores of an SM at a given cycle are executing the same instruction – hence Single Instruction, Multiple Threads. However, each thread has its own registers, so these instructions process different data. &nbsp;&nbsp;可以看到一个thread block跑在一个SM上面. 一个block执行完毕，SM可以调度另一个block执行，在一个SM上,block的执行是顺序的. &nbsp;&nbsp;一个SM只有一个指令单元，SM内的所有thread共享这个指令单元. Warps&nbsp;&nbsp;一个thread block在一个SM上执行,block中的thread可以继续分成warp,一个warp包括32个线程.warp是SM调度运行的基本单元,一个warp中的所有线程执行相同的指令.任意时刻，SM中只会有一个warp在运行，其余的warp都处于就绪等其他状态. 不同架构的gpu一个SM包含不同数量的cuda核心，Turing架构的一个SM包含128个cuda core,也就是4个warp. SM中有硬件warp scheduler，用来调度warp运行. thread block organization&nbsp;&nbsp;grid由多个blocks组成，每个kernel function的调用都会create一个grid,所以&lt;&lt;&lt;…&gt;&gt;&gt;语法指定的是block的数量和每个block的线程数量,因为这个kernel函数只会在一个grid上运行. &nbsp;&nbsp;同一个block内的所有thread可以合作(因为有个Shared Memory??)，不同block的thread不可以合作. &nbsp;&nbsp;从上图中可以看到每个block都有一个Shared Memory,是所有block中的thread所共享的,每个thread有自己的Local Memory和Registers. 一个Grid中的所有blocks共享Global Memory, Constant Memory以及Texture Memory. 所有blocks中的thread都共享这三种Memory,内存层次会在后序的博客中进行总结. uint3和Dim3 数据结构uint3 dim3 block and grid dimensions#include &lt;cuda_runtime.h&gt;#include &lt;stdio.h&gt;__global__ void checkIndex(void) &#123;printf("threadIdx:(%d, %d, %d) blockIdx:(%d, %d, %d) blockDim:(%d, %d, %d) ""gridDim:(%d, %d, %d)\n", threadIdx.x, threadIdx.y, threadIdx.z, blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z, gridDim.x,gridDim.y,gridDim.z);&#125;int main(int argc, char **argv) &#123; // define total data elementint nElem = 6;// define grid and block structuredim3 block (3);dim3 grid ((nElem+block.x-1)/block.x);// check grid and block dimension from host sideprintf("grid.x %d grid.y %d grid.z %d\n",grid.x, grid.y, grid.z);printf("block.x %d block.y %d block.z %d\n",block.x, block.y, block.z);// check grid and block dimension from device side checkIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();// reset device before you leavecudaDeviceReset();return(0);&#125; &nbsp;&nbsp;可以看到上面的程序定义了两个dim3类型的变量block和grid. 因为nElem的值是6,所以grid的x分量是2,因为这两个变量都只是指定了x分量，所以其余的分量都初始化为1.目前来看&lt;&lt;…&gt;&gt;&gt;传入的两个参数都是dim3类型的，之前的程序中直接传入int类型的,它应该会转换成dim3类型的.&nbsp;&nbsp;一个很直观的想法是可以把一个grid想像成一个三维的直角坐标系，然后一个block就是坐标系中的一个点,如果指定gridDim的x,y,z分别为2,3,4那么就有24个block,每个block可以用一个三维坐标来表示,同理可以可以把一个block想象成一个直角坐标系，thread也是其中的点. 运行结果 cudaDeviceReset可以看到这个函数是用来销毁一个CUDA的上下文的. It will reset the device immediately.虽然它表现上具有同步的功能，但是靠这个函数来同步是不安全的，所以建议如果是想同步的话，使用cudaDeviceSynchronize或者cudaMemcpy. 总结&nbsp;&nbsp;这篇博客总结了cuda线程的组织层次，从逻辑上讲一个kernel launch启动一个grid，可以用&lt;&lt;&lt;…&gt;&gt;&gt;语法设置gridDim和blockDim，从3个维度上进行设置. 一个grid包含多个block，这就是线程的两个层次的组织结构. 从硬件角度来讲，gpu的计算核心是多个SMs（streaming multiprocessors)，一个block在一个SM上运行，SM内部包含多个cuda core(不同的gpu架构的数目不同，Turing包含128个),32个cuda core构成一个warp.任意时刻一个SM中只会有一个warp处于活跃状态，其他的warp处于就绪或者挂起状态，从这点来看, SM非常类似于cpu中的core. 参考Programming Massively Parallel ProcessorsWhat is the role of cudaDeviceReset() in Cuda CUDA C Programming Guide]]></content>
      <tags>
        <tag>CUDA Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CUDA Hello World]]></title>
    <url>%2F2019%2F07%2F09%2FCUDA-Hello-World%2F</url>
    <content type="text"><![CDATA[简介&nbsp;&nbsp;第一次尝试编写CUDA C程序，主要是和以后的希望从事的方向有关，想从事大规模机器学习和深度学习系统的开发，CUDA是不可避免的需要掌握的并行计算的框架. 正好实验室有gpu服务器，趁着暑假来学习一波CUDA编程.&nbsp;&nbsp;CUDA是NVIDIA推出的运算平台，是一种并行计算的架构，使用GPU来进行通用计算. 编译CUDA程序的流程&nbsp;&nbsp;编译一个CUDA程序和C程序一样，CUDA程序的编译器是nvcc, CUDA程序文件的后缀是.cu. 开设我们有一个CUDA程序文件，命名为hello.cu, 那么我们用nvcc将它编译为可执行文件的命令如下: nvcc hello.cu -o hello CUDA Hello World&nbsp;&nbsp;学习任何程序设计语言的入门都是打印Hello World.CUDA程序也不例外， 下面我们以打印Hello World为例来解释CUDA程序的要素. #include&lt;stdio.h&gt;__global__ void hello()&#123; printf("hello world from GPU\n");&#125;int main()&#123; printf("hello world from CPU\n"); hello&lt;&lt;&lt;1, 10&gt;&gt;&gt;(); cudaDeviceSynchronize(); return 0;&#125; 我们用与之功能相近的普通C程序作为对比#include &lt;stdio.h&gt;void hello()&#123; printf("Hello World!\n");&#125;int main() &#123; hello(); return 0;&#125; 可以看到cuda程序相对于普通的c程序，有几点不同: __global__ 限定符&nbsp;&nbsp;在cuda程序中，cpu和gpu都用来做计算。我们把cpu叫做host，gpu叫做device. cpu和gpu拥有各自的存储空间。通常我们在cpu上顺序执行代码，在gpu上进行并行计算(Typically, we run serial workload on CPU and offload parallel computation to GPUs).&nbsp;&nbsp;__global__限定符表示hello函数是在gpu上执行的，是device代码,而且被该修饰符修饰的函数可以被host上的代码调用(这一点很重要，之后我们会看到，有的限定符表示只能被device代码或者只能被host代码调用)，我们的例子里hello函数就是被host上的main函数调用的,这样的函数也叫”kernels”. A kernel function must have a void return type(核函数的返回类型必须是void) &lt;&lt;&lt;…&gt;&gt;&gt; 语法&nbsp;&nbsp;当我们调用kernel的时候，它的执行的配置是通过&lt;&lt;&lt;…&gt;&gt;&gt;语法提供的，所谓的配置包括执行这个kernel用几个线程块，每个线程块开几个线程（这个涉及到gpu的结构）.比如上面的例子中，hello&lt;&lt;&lt;1, 10&gt;&gt;&gt;();. 在cuda中，这个叫做”kernel launch”(核启动). 具体参数之后的博客来说明. cudaDeviceSynchronize&nbsp;&nbsp; a kernel launch is asynchronous.因为kernel launch是异步执行的，当执行到device code的时候，在gpu上开启进程的时候，程序控制权就会回到cpu，不管gpu上的程序是否执行完毕。在我们的cuda程序中，如果没有cudaDeviceSynchronize函数，我们的程序就结束退出了，这样的话gpu端打印的hello world就不能打印到标准输出了. 而有了cudaDeviceSynchronize，cpu端的程序就会等device上的程序执行完后才退出，所以cudaDeviceSynchronize函数会阻塞直到device上的代码执行完毕. Vector Addition&nbsp;&nbsp;下面我们来看使用gpu进行向量相加运算的代码. 首先是使用cpu进行运算的代码：#define N 10000000void vector_add(float *out, float *a, float *b, int n) &#123; for(int i = 0; i &lt; n; i++)&#123; out[i] = a[i] + b[i]; &#125;&#125;int main()&#123; float *a, *b, *out; // Allocate memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); out = (float*)malloc(sizeof(float) * N); // Initialize array for(int i = 0; i &lt; N; i++)&#123; a[i] = 1.0f; b[i] = 2.0f; &#125; // Main function vector_add(out, a, b, N);&#125; 下面我们把向量相加的部分放到gpu上进行并行运算:#define N 100000__global__ void vector_add(float *out, float *a, float *b, int n) &#123; for(int i = 0; i &lt; n; i++)&#123; out[i] = a[i] + b[i]; &#125;&#125;int main()&#123; float *a, *b, *out; // Allocate memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); out = (float*)malloc(sizeof(float) * N); // Initialize array for(int i = 0; i &lt; N; i++)&#123; a[i] = 1.0f; b[i] = 2.0f; &#125; float *d_a; float *d_b; float *d_out; cudaMalloc((void**)&amp;d_a, sizeof(float) * N); cudaMalloc((void**)&amp;d_b, sizeof(float) * N); cudaMalloc((void**)&amp;d_out, sizeof(float) * N); cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_out, out, sizeof(float) * N, cudaMemcpyHostToDevice); // Main function vector_add&lt;&lt;&lt;1,10&gt;&gt;&gt;(d_out, d_a, d_b, N); //cudaMemcpy(a, d_a, sizeof(float) * N, cudaMemcpyDeviceToHost); //cudaMemcpy(b, d_b, sizeof(float) * N, cudaMemcpyDeviceToHost); cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost); for(int i=0;i&lt; N;i++)&#123; printf("%f\n",out[i]); &#125; printf("end!!!!"); cudaFree(d_a); cudaFree(d_b); cudaFree(d_out); free(a); free(b); free(out);&#125; &nbsp;&nbsp;可能一开始，我们会向hello world程序一样，给vector_add函数添加__global__修饰符.然后在main函数中调用vector_add函数的地方添加&lt;&lt;&lt;…&gt;&gt;&gt;. 这样以后我们编译运行程序但是发现程序的执行结果和我们预想的不一样. 是什么原因导致的呢？&nbsp;&nbsp;原因是cpu和gpu是各自拥有自己的存储空间,cpu无法直接获取gpu存储上的内容，gpu也无法直接获取到cpu存储上的内容. 在 cuda的术语里, cpu的存储叫做host memory, gpu的存储叫做device memory. 指向cpu内存的指针叫做host pointer, 指向gpu内存的指针叫做device pointer. 如果要让gpu能够获取到数据，那么数据必须在device memory上，cuda提供了分配device memory和在host和device之间进行数据迁移的api,cuda 程序的一个常见的流程如下: 分配host memory并初始化host上的数据 分配device memory(cudaMalloc) 将kernel函数要用的数据从host迁移到device上(cudaMemcpy) 执行kernel函数 将kernel函数的输出从device迁移到host上(cudaMemcpy) vec_add在gpu上运行，而out,a,b这三个向量，传入的是cpu上的地址空间，因此结果和我们预期的不一样. 这里还有个疑问，那为啥程序不报错呢，传入的是cpu上的存储地址.核函数内部居然不报错？ 这个程序相比于前面的hello world程序有很多新的函数需要解释： cudaMalloc和cudaFree&nbsp;&nbsp;这两个函数类似于c语言中的malloc和free函数，只不过这两个函数是在device memory上分配空间, 他们的函数原型如下：cudaMalloc(void **devPtr, size_t count);cudaFree(void *devPtr); cudaMalloc函数在device memory上分配size为count的空间，然后让devPtr指向分配的空间. 而 cudaFree将devPtr指向的空间给free了. cudaMemcpy&nbsp;&nbsp;cudaMemcpy函数用来在host和device之间传递数据，和c中的memcpy函数很像. 语法如下：cudaMemcpy(void *dst, void *src, size_t count, cudaMemcpyKind kind) 这个函数将size为count的存储从src复制到dst,kind指示复制的方向，最常用的值是cudaMemcpyHostToDevice 以及 cudaMemcpyDeviceToHost，分别表示从host复制到device以及从device复制到host. 参考Tutorial 01:Say Hello to CUDAwhy do we need cudaDeviceSynchronizecudaDeviceSynchronize and performing sequential work]]></content>
      <tags>
        <tag>CUDA Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归交叉熵损失函数梯度推导]]></title>
    <url>%2F2019%2F07%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[what is softmax&nbsp;&nbsp;softmax通常用于神经网络的输出层，用于多分类任务, 为每个类别产生一个概率，公式如下: softmax(\overrightarrow{z})=\overrightarrow{s}\overrightarrow{z} \in R^n , \overrightarrow{s} \in R^ns_i = \frac{e^{z_i}}{ \sum_{k=1}^{n} {e^{z_k}} }what is logistic regression&nbsp;&nbsp;逻辑回归是一个分类模型,假设输入$ \overrightarrow{x} \in R^n $, 模型参数$ \overrightarrow{w} \in R^n $, $ b \in R $ ,真实标签$ y \in \{0,1\} $ ,模型的预测输出是$ \hat{y} \in R $ 表示模型预测该实例为1的概率: P(Y=1|x) = \hat{y} = sigmoid(w*x+b) = \frac{1}{1+e^{-(w*x+b)}} =\frac{e^{w*x+b}}{1+e^{w*x+b}}从而: P(Y=0|x) = \frac{1}{1+e^{w*x+b}}sigmoid损失函数&nbsp;&nbsp;sigmoid损失函数的公式和形状如下，它能将输入的范围转化到[0,1]之间,作为概率值. sigmoid(z) = \frac{1}{1+e^{-z}} 极大似然估计&nbsp;&nbsp;极大似然估计是一种模型参数的估计方法，是频率学派的方法（另一个是贝叶斯学派，它们的参数估计方法是最大后验概率估计,两者的区别在于极大似然估计假设模型参数是一个固定值而最大后验概率估计假设模型参数也服从一定的分布，称为先验分布). 极大似然估计假设模型的参数是$ \Theta $,那么我们观察到的数据可以表示成参数的函数$ f(xi|\Theta) $,一般我们是有一个数据集，根据这个数据集合来估计模型参数，那么使得这个数据集合被我们观察到的概率是 f(x_1,x_2,x_3,...x_n|\Theta)因为数据集的数据是独立同分布的，所以下面的式子成立: f(x_1,x_2,x_3,...x_n|\Theta) = \prod_{i=1}^{n} {f(x_i|\Theta)}上面的式子就是似然函数，它是一个关于参数$ \Theta $的函数,而我们要选取使得该似然函数取得最大值的参数，这就是极大似然估计,也就是选取参数使得我们观察到的结果的可能性最大. 逻辑回归的损失函数推导&nbsp;&nbsp;逻辑回归的参数估计可以采用极大似然法，似然函数如下： \prod_{i=1}^{n} {P(Y=1|x)^{y_i}P(Y=0|x)^{1-y_i} } = \prod_{i=1}^{n} {\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}}我们使用负对数似然，对数函数不改变单调性，取负数使得该值大于0，那么损失函数如下： L = - \ln( \prod_{i=1}^{n} {\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}} ) = - \sum_{i=1}^{n} {y_i\ln\hat{y_i}+(1-y_i)\ln(1-\hat{y_i})}我们采用梯度下降法sgd来求解模型参书$ w $, $ b $: \frac{\partial L}{\partial w} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial w}}\frac{\partial L}{\partial \hat{y_i} } = - [ y_i \frac{\partial \ln\hat{y_i}}{\partial \hat{y_i}}+(1-y_i) \frac{\partial \ln(1- \hat{y_i})}{\partial \hat{y_i}} ] = - [ y_i \frac{1}{ \hat{y_i} } +(1-y_i) \frac{-1}{1- \hat{y_i}} ] = - \frac{y_i - \hat{y_i} }{ \hat{y_i} (1- \hat{y_i})}\frac{ \partial \hat{y_i} }{ \partial w} = \hat{y_i}(1- \hat{y_i} ) x_i综上: \frac{\partial L}{\partial w} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial w}} = - \sum_{i=1}^{n} {(y_i - \hat{y_i})x_i }$ w $的更新公式为: w = w + \eta \sum_{i=1}^{n}{(y_i- \hat{y_i})x_i }同理, $ b $的梯度如下: \frac{\partial L}{\partial b} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial b}} = - \sum_{i=1}^{n} {(y_i - \hat{y_i}) }b的更新公式如下: b = b + \eta \sum_{i=1}^{n}{(y_i- \hat{y_i}) }交叉熵损失函数以及softmax函数的梯度推导&nbsp;&nbsp;交叉熵用来衡量两个分布之间的距离,假设两个分布$ p $和$ q $,交叉熵的计算公式如下： H(p,q) = \sum_{i=1}^{n} {p_i \log \frac{1}{q_i}} = \sum_{i=1}^{n} {- p_i \log q_i}&nbsp;&nbsp;softmax的输出是一个概率分布，而真实标签的one-hot向量也是一个概率分布，真实类别的概率为1，其余类别的概率是0，所以用交叉熵来衡量两个分布之间的距离作为损失函数是合适的，其实该损失函数也可以使用极大似然估计推导出来，下面是推导过程: L = - \ln(\prod_{i=1}^{n} {\prod_{j=1}^{c} { \hat{y_{ij}}^{y_{ij}}} }) = \sum_{i=1}^{n} { [\sum_{j=1}^{c} {-y_{ij} \ln \hat{y_{ij}}} ] }其实数据集的损失是每个数据点的损失之和，[]内部就是交叉熵，下面我们推导损失函数的梯度，首先我们假设全连接中$ z_{ij} = w_j * x_i + b_j $ , 这里的 $ wj $ 表示所有与输出层的第$ j $个神经元连接的权值，这些权值构成一个向量: \frac{\partial L}{\partial w_j } = \sum_{i=1}^{n} {\frac{ \partial L }{ \partial z_{ij} } \frac{\partial z_{ij} }{\partial w_j }}\frac{ \partial L }{ \partial z_{ij} } = \sum_{k=1}^{c} { \frac{ \partial L}{ \partial \hat{y_{ik}} } \frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} }\frac{\partial L }{\partial \hat{y_{ik}}} = \frac{ \partial [ -y_{ik} \ln \hat{y_{ik}}] }{ \partial \hat{y_{ik}}}= \frac{-y_{ik}}{ \hat{y_{ik}}}下面得分两种情况考虑:如果 $ k=j $, \frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} = \frac{ \partial \frac{e^{z_{ij}}}{ \sum_{q=1}^{c} {e^{z_{iq}}}} }{ \partial z_{ij} } = \frac{ e^{z_{ij}} \sum_{q=1}^{c}{ e^{z_{iq}}} - e^{z_{ij}} e^{z_{ij}} }{ [ \sum_{q=1}^{c} {e^{z_{iq}}} ]^{2} } = \hat{y_{ij}} (1- \hat{y_{ij}} )如果 $ k \ne j $, \frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} = \frac{ \partial \frac{e^{z_{ik}}}{ \sum_{q=1}^{c} {e^{z_{iq}}}} }{ \partial z_{ij} } = \frac{ 0- e^{z_{ik}} e^{z_{ij}} }{ [ \sum_{q=1}^{c} {e^{z_{iq}}} ]^{2} } = - \hat{y_{ik}} \hat{y_{ij}}将上面的两个式子综合一下，得到如下的式子： \frac{ \partial L }{ \partial z_{ij} } = - y_{ij} (1- \hat{ y_{ij} }) + \sum_{k \ne j}^{c}{ y_{ik} \hat{y_{ij}} } = -y_{ij} + \hat{y_{ij}} \sum_{q=1}^{c} {y_{ik}} = \hat{ y_{ij}} -y_{ij}因为: \frac{\partial z_{ij}}{ \partial w_j } = x_i\frac{\partial z_{ij}}{ \partial b_j } = 1综上,$ w_j $的更新公式如下： w_j = w_j - \eta \sum_{i=1}^{n}{ ( \hat{ y_{ij}} -y_{ij}) x_i }同理,b的更新公式如下： b_j = b_j - \eta \sum_{i=1}^{n}{ ( \hat{ y_{ij}} -y_{ij}) }上面的公式可以由下面的代码来验证: 参考]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
