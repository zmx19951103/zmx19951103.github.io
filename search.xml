<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[GPU架构]]></title>
    <url>%2F2019%2F07%2F10%2FGPU%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F07%2F09%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
  <entry>
    <title><![CDATA[CUDA Hello World]]></title>
    <url>%2F2019%2F07%2F09%2FCUDA-Hello-World%2F</url>
    <content type="text"><![CDATA[简介&nbsp;&nbsp;第一次尝试编写CUDA C程序，主要是和以后的希望从事的方向有关，想从事大规模机器学习和深度学习系统的开发，CUDA是不可避免的需要掌握的并行计算的框架. 正好实验室有gpu服务器，趁着暑假来学习一波CUDA编程.&nbsp;&nbsp;CUDA是NVIDIA推出的运算平台，是一种并行计算的架构，使用GPU来进行通用计算. 编译CUDA程序的流程&nbsp;&nbsp;编译一个CUDA程序和C程序一样，CUDA程序的编译器是nvcc, CUDA程序文件的后缀是.cu. 开设我们有一个CUDA程序文件，命名为hello.cu, 那么我们用nvcc将它编译为可执行文件的命令如下: nvcc hello.cu -o hello CUDA Hello World&nbsp;&nbsp;学习任何程序设计语言的入门都是打印Hello World.CUDA程序也不例外， 下面我们以打印Hello World为例来解释CUDA程序的要素. 123456789101112 #include&lt;stdio.h&gt;__global__ void hello()&#123; printf("hello world from GPU\n");&#125;int main()&#123; printf("hello world from CPU\n"); hello&lt;&lt;&lt;1, 10&gt;&gt;&gt;(); cudaDeviceSynchronize(); return 0;&#125; 我们用与之功能相近的普通C程序作为对比 123456789#include &lt;stdio.h&gt;void hello()&#123; printf("Hello World!\n");&#125;int main() &#123; hello(); return 0;&#125; 可以看到cuda程序相对于普通的c程序，有几点不同: __global__ 限定符&nbsp;&nbsp;在cuda程序中，cpu和gpu都用来做计算。我们把cpu叫做host，gpu叫做device. cpu和gpu拥有各自的存储空间。通常我们在cpu上顺序执行代码，在gpu上进行并行计算(Typically, we run serial workload on CPU and offload parallel computation to GPUs).&nbsp;&nbsp;__global__限定符表示hello函数是在gpu上执行的，是device代码,而且*被该修饰符修饰的函数可以被host上的代码调用(这一点很重要，之后我们会看到，有的限定符表示只能被device代码或者只能被host代码调用)，我们的例子里hello函数就是被host上的main函数调用的,这样的函数也叫”kernels”. &lt;&lt;&lt;…&gt;&gt;&gt; 语法&nbsp;&nbsp;当我们调用kernel的时候，它的执行的配置是通过&lt;&lt;&lt;…&gt;&gt;&gt;语法提供的，所谓的配置包括执行这个kernel用几个线程块，每个线程块开几个线程（这个涉及到gpu的结构）.比如上面的例子中，hello&lt;&lt;&lt;1, 10&gt;&gt;&gt;();. 在cuda中，这个叫做”kernel launch”(核启动). 具体参数之后的博客来说明. cudaDeviceSynchronize&nbsp;&nbsp; a kernel launch is asynchronous.因为kernel launch是异步执行的，当执行到device code的时候，在gpu上开启进程的时候，程序控制权就会回到cpu，不管gpu上的程序是否执行完毕。在我们的cuda程序中，如果没有cudaDeviceSynchronize函数，我们的程序就结束退出了，这样的话gpu端打印的hello world就不能打印到标准输出了. 而有了cudaDeviceSynchronize，cpu端的程序就会等device上的程序执行完后才退出，所以cudaDeviceSynchronize函数会阻塞直到device上的代码执行完毕. Vector Addition&nbsp;&nbsp;下面我们来看使用gpu进行向量相加运算的代码. 首先是使用cpu进行运算的代码： 123456789101112131415161718192021222324#define N 10000000void vector_add(float *out, float *a, float *b, int n) &#123; for(int i = 0; i &lt; n; i++)&#123; out[i] = a[i] + b[i]; &#125;&#125;int main()&#123; float *a, *b, *out; // Allocate memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); out = (float*)malloc(sizeof(float) * N); // Initialize array for(int i = 0; i &lt; N; i++)&#123; a[i] = 1.0f; b[i] = 2.0f; &#125; // Main function vector_add(out, a, b, N);&#125; 下面我们把向量相加的部分放到gpu上进行并行运算: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#define N 100000__global__ void vector_add(float *out, float *a, float *b, int n) &#123; for(int i = 0; i &lt; n; i++)&#123; out[i] = a[i] + b[i]; &#125;&#125;int main()&#123; float *a, *b, *out; // Allocate memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); out = (float*)malloc(sizeof(float) * N); // Initialize array for(int i = 0; i &lt; N; i++)&#123; a[i] = 1.0f; b[i] = 2.0f; &#125; float *d_a; float *d_b; float *d_out; cudaMalloc((void**)&amp;d_a, sizeof(float) * N); cudaMalloc((void**)&amp;d_b, sizeof(float) * N); cudaMalloc((void**)&amp;d_out, sizeof(float) * N); cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_out, out, sizeof(float) * N, cudaMemcpyHostToDevice); // Main function vector_add&lt;&lt;&lt;1,10&gt;&gt;&gt;(d_out, d_a, d_b, N); //cudaMemcpy(a, d_a, sizeof(float) * N, cudaMemcpyDeviceToHost); //cudaMemcpy(b, d_b, sizeof(float) * N, cudaMemcpyDeviceToHost); cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost); for(int i=0;i&lt; N;i++)&#123; printf("%f\n",out[i]); &#125; printf("end!!!!"); cudaFree(d_a); cudaFree(d_b); cudaFree(d_out); free(a); free(b); free(out);&#125; &nbsp;&nbsp;可能一开始，我们会向hello world程序一样，给vector_add函数添加__global__修饰符.然后在main函数中调用vector_add函数的地方添加&lt;&lt;&lt;…&gt;&gt;&gt;. 这样以后我们编译运行程序但是发现程序的执行结果和我们预想的不一样. 是什么原因导致的呢？&nbsp;&nbsp;原因是cpu和gpu是各自拥有自己的存储空间,cpu无法直接获取gpu存储上的内容，gpu也无法直接获取到cpu存储上的内容. 在 cuda的术语里, cpu的存储叫做host memory, gpu的存储叫做device memory. 指向cpu内存的指针叫做host pointer, 指向gpu内存的指针叫做device pointer. 如果要让gpu能够获取到数据，那么数据必须在device memory上，cuda提供了分配device memory和在host和device之间进行数据迁移的api,cuda 程序的一个常见的流程如下: 分配host memory并初始化host上的数据 分配device memory(cudaMalloc) 将kernel函数要用的数据从host迁移到device上(cudaMemcpy) 执行kernel函数 将kernel函数的输出从device迁移到host上(cudaMemcpy) vec_add在gpu上运行，而out,a,b这三个向量，传入的是cpu上的地址空间，因此结果和我们预期的不一样. 这里还有个疑问，那为啥程序不报错呢，传入的是cpu上的存储地址.核函数内部居然不报错？ 这个程序相比于前面的hello world程序有很多新的函数需要解释： cudaMalloc和cudaFreecudaMemcpy参考Tutorial 01:Say Hello to CUDAwhy do we need cudaDeviceSynchronizecudaDeviceSynchronize and performing sequential work]]></content>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归交叉熵损失函数梯度推导]]></title>
    <url>%2F2019%2F07%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"></content>
  </entry>
</search>
