<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[阅读论文《A Generic Communication Scheduler for Distributed DNN Training Acceleration》]]></title>
    <url>%2F2019%2F10%2F28%2F%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E3%80%8AA-Generic-Communication-Scheduler-for-Distributed-DNN-Training-Acceleration%E3%80%8B%2F</url>
    <content type="text"><![CDATA[AbstractIntroduction]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++ typename的用法]]></title>
    <url>%2F2019%2F09%2F24%2Fc-typename%E7%9A%84%E7%94%A8%E6%B3%95%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[生产者消费者模型]]></title>
    <url>%2F2019%2F09%2F19%2F%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[c++11 std::thread]]></title>
    <url>%2F2019%2F09%2F18%2Fc-11-std-thread%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[同步sgd/异步sgd]]></title>
    <url>%2F2019%2F09%2F10%2F%E5%90%8C%E6%AD%A5sgd-%E5%BC%82%E6%AD%A5sgd%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[论文阅读: Parameter Server for Distributed Machine Learning]]></title>
    <url>%2F2019%2F08%2F30%2F%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Parameter-Server-for-Distributed-Machine-Learning%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[参数同步架构和算法]]></title>
    <url>%2F2019%2F08%2F30%2F%E5%8F%82%E6%95%B0%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[ring AllReduce]]></title>
    <url>%2F2019%2F08%2F27%2Fring-AllReduce%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[FM/DeepFM/NFM]]></title>
    <url>%2F2019%2F08%2F22%2FFM-DeepFM-NFM%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[分布式爬虫+推荐]]></title>
    <url>%2F2019%2F08%2F16%2F%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB-%E6%8E%A8%E8%8D%90%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[分布式文件系统]]></title>
    <url>%2F2019%2F08%2F16%2F%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[GBDT]]></title>
    <url>%2F2019%2F08%2F07%2FGBDT%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[meta learning]]></title>
    <url>%2F2019%2F08%2F05%2Fmeta-learning%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[自动微分]]></title>
    <url>%2F2019%2F08%2F05%2F%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86%2F</url>
    <content type="text"><![CDATA[符号微分数值微分前向自动微分反向自动微分参考一天实现自己的自动微分自动微分简介tensorflow 自动微分c++ 实现自动微分wikpedia automatic differentiation自动微分法是如何用c++实现的]]></content>
      <tags>
        <tag>deep learning system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[beam search]]></title>
    <url>%2F2019%2F08%2F05%2Fbeam-search%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[隐马尔可夫模型]]></title>
    <url>%2F2019%2F08%2F05%2F%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[RNN梯度消失和爆炸]]></title>
    <url>%2F2019%2F08%2F04%2FRNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8%2F</url>
    <content type="text"></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[BPTT]]></title>
    <url>%2F2019%2F08%2F04%2FBP%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-BPTT%2F</url>
    <content type="text"><![CDATA[BPTT (BackPropagation Through Time)用于循环神经网络的BP算法.首先我们回顾一下RNN,RNN的网络结构图如下： 标准RNN的数学表达式如下: s(t) = Ux(t)+Wh(t-1)+bh(t) = tanh(s(t))z(t) = Vh(t)+c\hat{y(t)} = softmax(z(t))$ U $ , $ W $ , $ V $ 是待优化的参数. 上面的表达式可以和图对应,RNN网络的参数在所有时间步都是共享的.我们参考材料[2]的记法，将公式细化如下:首先我们计算s(t)的第j个元素,我们假设输入 $ x(t) $的维度是 $ l $ , 隐藏层的状态 $ h(t) $ 的维度是 $ m $ : s_j(t) = \sum_{i=1}^{l} {x_i(t)u_{ji} + \sum_{k=1}^{m} { h_k(t-1)wjk }}+b_j下面我们计算 $ z(t) $ 的第j个元素: z_j(t) = \sum_{q=1}^{m} {h_q(t)v_{jq}}+c_j因为每个时间步的输出都是一个概率向量，表示该时间步的词所属于各标签的概率分布，如果用one-hot向量表示真实标签$ y_t $，那么输出$ \hat{y_t} $和$ y_t $可以计算交叉熵损失$ E_t $, 公式如下： E_t = -{y_t}^T \ln( \hat{y_t})E = \sum_{t=1}^{T} {E_t}我们要用损失函数 $ E $ 对 $ U $ , $ W $ , $ V $ 求偏导数，然后更新参数,我么使用sgd 算法所以每次更新使用一个样本. 我们拿出一个时间步的损失，并就该损失求$ U $ , $ W $ , $ V $ 的更新，结构图如下：我们假设输出的概率分布的维度是 $ o $ , 并把时间维度 $ t $ 舍去, 那么: E_t = -{y_t}^T \ln( \hat{y_t}) = \sum_{k=1}^{o} { -y_k \ln \hat{y_k} }\frac{ \partial E_t }{ \partial \hat{y_k} } = - \frac{y_k}{ \hat{y_k} }\frac{ \partial \hat{y_k} }{ \partial z_j } = \begin{cases} \hat{y_j}(1- \hat{y_j} ) & \text{ k = j }\\ - \hat{y_k} { \hat{y_j }}& \text{ j != k } \end{cases}综上： \frac{ \partial E_t }{ \partial z_j } = \sum_{k=1}^{o} { \frac{ \partial E_t}{ \partial \hat{y_k} } \frac{ \partial \hat{y_k} }{ \partial z_j } } = y_j - \hat{y_j}\frac{ \partial z_j }{ \partial v_{jq}} = h_q\frac{ \partial E_t }{ \partial v_{jq}} = \frac{ \partial E_t }{ \partial z_j } \frac{ \partial z_j }{ \partial v_{jq}} =( y_j - \hat{y_j} )h_q根据上面的式子，我们能对矩阵v的所有元素进行更新.下面我们计算 $ U $ 和 $ W $ 的更新式子: \frac{ \partial E_t }{ \partial h_q } = \sum_{j=1}^{o} {\frac{ \partial E_t}{ \partial z_j} \frac{ \partial z_j}{ \partial h_q} } = \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}}\frac{ \partial E_t }{ \partial s_q } = \frac{ \partial E_t }{ \partial h_q } \frac{ \partial h_q }{ \partial s_q } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2)\frac{ \partial E_t }{ \partial u_{qi} } = \frac{ \partial E_t }{ \partial s_q } \frac{ \partial s_q }{ \partial u_{qi} } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2) x_i\frac{ \partial E_t }{ \partial w_{qk} } = \frac{ \partial E_t }{ \partial s_q } \frac{ \partial s_q }{ \partial w_{qk} } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2) h_k(t-1)W更新\frac{ \partial E}{ \partial W} = \sum_{t=1}^{T} { \frac{ \partial E_t }{ \partial W }}\frac{ \partial E_t }{ \partial W } = \frac{ \partial E_t }{ \partial \hat{y_t} } \frac{ \partial \hat{y_t} }{ \partial W }\frac{ \partial E_t }{ \partial \hat{y_t} } = - \frac{y_t}{ \hat{y_t}}这里的除法是element-wise的.然后是$ \frac{ \partial \hat{y_t} }{ \partial W } $ ，这是一个向量对一个矩阵求偏导 参考BPTT blogbptt]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[simpledb(1)]]></title>
    <url>%2F2019%2F08%2F04%2Fsimpledb-1%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[c++11之std::move]]></title>
    <url>%2F2019%2F07%2F29%2Fc-11%E4%B9%8Bstd-move%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[c++11之多线程std::thread]]></title>
    <url>%2F2019%2F07%2F29%2Fc-11%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8Bstd-thread%2F</url>
    <content type="text"></content>
      <tags>
        <tag>c++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[sqlite architecture]]></title>
    <url>%2F2019%2F07%2F29%2Fsqlite-architecture%2F</url>
    <content type="text"></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[database index]]></title>
    <url>%2F2019%2F07%2F29%2Fdatabase-index%2F</url>
    <content type="text"></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[B/B+ Tree]]></title>
    <url>%2F2019%2F07%2F27%2FB-B-Tree%2F</url>
    <content type="text"></content>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习分布式训练]]></title>
    <url>%2F2019%2F07%2F26%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83%2F</url>
    <content type="text"><![CDATA[并行模式数据并行模型并行混合并行参考]]></content>
      <tags>
        <tag>DDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[architerture of sqlite]]></title>
    <url>%2F2019%2F07%2F26%2Farchiterture-of-sqlite%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[sqlite源码剖析(1)]]></title>
    <url>%2F2019%2F07%2F26%2Fsqlite%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-1%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[傅立叶变换及其加强版FFT]]></title>
    <url>%2F2019%2F07%2F26%2F%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%8A%A0%E5%BC%BA%E7%89%88FFT%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[caffe中的卷积计算的实现im2col]]></title>
    <url>%2F2019%2F07%2F26%2Fcaffe%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0im2col%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[GPU内存层次]]></title>
    <url>%2F2019%2F07%2F25%2FGPU%E5%86%85%E5%AD%98%E5%B1%82%E6%AC%A1%2F</url>
    <content type="text"></content>
      <tags>
        <tag>CUDA Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[knowledge distillation]]></title>
    <url>%2F2019%2F07%2F24%2Fknowledge-distillation%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[cudnn实践]]></title>
    <url>%2F2019%2F07%2F23%2Fcudnn%E5%AE%9E%E8%B7%B5%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[矩阵乘法优化cublas]]></title>
    <url>%2F2019%2F07%2F23%2F%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E4%BC%98%E5%8C%96cublas%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[SIMD]]></title>
    <url>%2F2019%2F07%2F23%2Fintel%E5%90%91%E9%87%8F%E5%8C%96%E6%8C%87%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[SIMD 向量化]]></content>
      <tags>
        <tag>parallel programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[model parallelism/data parallelism]]></title>
    <url>%2F2019%2F07%2F23%2Fmodel-parallelism-data-parallelism%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[BSP/SSP/A-SGD]]></title>
    <url>%2F2019%2F07%2F23%2FBSP-SSP-A-SGD%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[BatchNormalization]]></title>
    <url>%2F2019%2F07%2F23%2FBatchNormalization%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[Dropout]]></title>
    <url>%2F2019%2F07%2F23%2FDropout%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[mixed precision]]></title>
    <url>%2F2019%2F07%2F23%2Fmixed-precision%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[REVISITING DISTRIBUTED SYNCHRONOUS SGD]]></title>
    <url>%2F2019%2F07%2F23%2FREVISITING-DISTRIBUTED-SYNCHRONOUS-SGD%2F</url>
    <content type="text"><![CDATA[Referencerevisiting distributed synchronous sgdDC-ASGD]]></content>
      <tags>
        <tag>distributed machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[caffe源码(1)]]></title>
    <url>%2F2019%2F07%2F22%2Fcaffe%E6%BA%90%E7%A0%81-1%2F</url>
    <content type="text"></content>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[词向量]]></title>
    <url>%2F2019%2F07%2F20%2F%E8%AF%8D%E5%90%91%E9%87%8F%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[模型压缩]]></title>
    <url>%2F2019%2F07%2F20%2F%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[bert]]></title>
    <url>%2F2019%2F07%2F20%2Fbert%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[attention is all you need]]></title>
    <url>%2F2019%2F07%2F20%2Fattention-is-all-you-need%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[各种激活函数总结]]></title>
    <url>%2F2019%2F07%2F20%2F%E5%90%84%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[sequence2sequence实现机器翻译]]></title>
    <url>%2F2019%2F07%2F12%2Fsequence2sequence%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[bilstm+CRF实现命名实体识别]]></title>
    <url>%2F2019%2F07%2F12%2Fbilstm-CRF%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[BiLSTMCRFTensorflow implementation参考Bidirectional LSTM-CRF Models for Sequence Tagging]]></content>
      <tags>
        <tag>NLP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[c++11之Deducing Types]]></title>
    <url>%2F2019%2F07%2F12%2Fc-11%E4%B9%8BDeducing-Types%2F</url>
    <content type="text"></content>
      <tags>
        <tag>c++11</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CUDA in Actions]]></title>
    <url>%2F2019%2F07%2F12%2FCUDA-in-Actions%2F</url>
    <content type="text"><![CDATA[简介&nbsp;&nbsp;之前我们用cuda实现了打印hello world以及向量相加. 这篇博客，我们来探究如何利用gpu的并行性. gpu的power就在于它的并行性. &lt;&lt;&lt;…&gt;&gt;&gt;语法&nbsp;&nbsp;&lt;&lt;&lt;…&gt;&gt;&gt;是kernel函数执行的设置，比如用几个线程来执行核函数. cuda 将线程组织成线程块(thread block)，kernel可以启动很多线程块，并且把它们组织成grid数据结构(grid).&nbsp;&nbsp;核函数启动设置的语法是&lt;&lt;&lt; M , T &gt;&gt;&gt; &nbsp;&nbsp;表示kernel函数执行启动了一个grid,这个grid有M个线程块(thread block),每个线程块有T个线程. threadIdx.x, blockDim.x 以及 blockIdx.x&nbsp;&nbsp;cuda提供了内置变量来获取线程信息，这里我们使用两个. threadIdx.x 表示线程块中的线程编号(以0开始), blockIdx.x表示线程块中的线程数.因为可以获取线程信息，所以函数内部可以根据当前的线程来决定执行怎样的操作.比如向量相加，不同的线程计算不同区间的向量相加，先获取当前线程号，根据线程号确定向量相加的范围，执行计算.blockDim.x表示一个thread block的线程数量. 并行计算 vector addition假设一个线程块有256个线程 __global__ void vector_add(float *out, float *a, float *b, int n) &#123; int index = threadIdx.x; int stride = blockDim.x; for(int i = index; i &lt; n; i += stride)&#123; out[i] = a[i] + b[i]; &#125;&#125; 上面的代码的想法可以用如下的图来说明: 不同线程写不同位置，不存在冲突，这里突然想起来，如果按照一个线程的写法但是开多个线程运行，是不是应该会冲突？？之后可以学习一下gpu线程的同步问题. 我们用nvprof来验证用1个线程和256个线程完成向量相加的执行情况，主要看执行时间： 可以看到执行时间明显缩短了.现在的程序只开了一个thread block,我们尝试开启多个thread block. 多个thread block并行计算向量相加&nbsp;&nbsp;cuda提供了内置的变量来获取thread block的信息，包括block的编号(blockIdx.x)，一个grid有多少个blocks(gridDim.x).&nbsp;&nbsp;使用多个grid来并行化向量相加的示意图如下所示: &nbsp;&nbsp;想法就是每个block有256个thread,每个thread负责一个计算一个元素相加，然后总共有 N/256个thread block,因为N不一定是256的倍数，所以在核函数中还要判断index是否小于N..这样每个元素都是同时计算的,进一步加大并行化.期望的执行时间也应该减少. 对应的程序如下：__global__ void vector_add(float *out, float *a, float *b, int n) &#123; int i = blockIdx.x*blockDim.x+threadIdx.x; if(i&lt;n)out[i] = a[i] + b[i]; 程序运行性能如下: 性能比较N=700000 version Execution Time(ms) Speedup 1 thread 977.26 1.00x 1 block 5.5417 176.35x Multiple blocks 0.13274 7360.25x 总结这篇博客分别通过用一个thread block,每个thread block 256个线程和多个thread block，每个thread block 256个线程计算向量相加来展示如何使用gpu来并行计算.其中强调了三个概念 grid , threadblock以及 thread….接下来我们学习一个gpu的架构. 参考Tutorial 02 CUDA in Actions]]></content>
      <tags>
        <tag>CUDA Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LSTM,RNN,Bi-LSTM,On-LSTM]]></title>
    <url>%2F2019%2F07%2F12%2FLSTM-RNN-Bi-LSTM-On-LSTM%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[CUDA线程层次-从硬件和软件角度]]></title>
    <url>%2F2019%2F07%2F10%2FGPU%E6%9E%B6%E6%9E%84%2F</url>
    <content type="text"><![CDATA[Streaming MultiprocessorsGPU是由流多处理器构成的，每个SM内部有多个core, 每个core跑一个thread. The idea is that the CPU spawns a thread per element, and the GPU then executes those threads.Not all of the thousands or millions of threads actually run in parallel, but many do.Specifically, an NVIDIA GPU contains several largely independent processors called “Streaming Multiprocessors” (SMs), each SM hosts several “cores”, and each “core” runs a thread.For instance, Fermi has up to 16 SMs with 32 cores per SM – so up to 512 thread can run in parallel. All threads running on the cores of an SM at a given cycle are executing the same instruction – hence Single Instruction, Multiple Threads. However, each thread has its own registers, so these instructions process different data. &nbsp;&nbsp;可以看到一个thread block跑在一个SM上面. 一个block执行完毕，SM可以调度另一个block执行，在一个SM上,block的执行是顺序的. &nbsp;&nbsp;一个SM只有一个指令单元，SM内的所有thread共享这个指令单元. Warps&nbsp;&nbsp;一个thread block在一个SM上执行,block中的thread可以继续分成warp,一个warp包括32个线程.warp是SM调度运行的基本单元,一个warp中的所有线程执行相同的指令.任意时刻，SM中只会有一个warp在运行，其余的warp都处于就绪等其他状态. 不同架构的gpu一个SM包含不同数量的cuda核心，Turing架构的一个SM包含128个cuda core,也就是4个warp. SM中有硬件warp scheduler，用来调度warp运行. thread block organization&nbsp;&nbsp;grid由多个blocks组成，每个kernel function的调用都会create一个grid,所以&lt;&lt;&lt;…&gt;&gt;&gt;语法指定的是block的数量和每个block的线程数量,因为这个kernel函数只会在一个grid上运行. &nbsp;&nbsp;同一个block内的所有thread可以合作(因为有个Shared Memory??)，不同block的thread不可以合作. &nbsp;&nbsp;从上图中可以看到每个block都有一个Shared Memory,是所有block中的thread所共享的,每个thread有自己的Local Memory和Registers. 一个Grid中的所有blocks共享Global Memory, Constant Memory以及Texture Memory. 所有blocks中的thread都共享这三种Memory,内存层次会在后序的博客中进行总结. uint3和Dim3 数据结构uint3 dim3 block and grid dimensions#include &lt;cuda_runtime.h&gt;#include &lt;stdio.h&gt;__global__ void checkIndex(void) &#123;printf("threadIdx:(%d, %d, %d) blockIdx:(%d, %d, %d) blockDim:(%d, %d, %d) ""gridDim:(%d, %d, %d)\n", threadIdx.x, threadIdx.y, threadIdx.z, blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z, gridDim.x,gridDim.y,gridDim.z);&#125;int main(int argc, char **argv) &#123; // define total data elementint nElem = 6;// define grid and block structuredim3 block (3);dim3 grid ((nElem+block.x-1)/block.x);// check grid and block dimension from host sideprintf("grid.x %d grid.y %d grid.z %d\n",grid.x, grid.y, grid.z);printf("block.x %d block.y %d block.z %d\n",block.x, block.y, block.z);// check grid and block dimension from device side checkIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();// reset device before you leavecudaDeviceReset();return(0);&#125; &nbsp;&nbsp;可以看到上面的程序定义了两个dim3类型的变量block和grid. 因为nElem的值是6,所以grid的x分量是2,因为这两个变量都只是指定了x分量，所以其余的分量都初始化为1.目前来看&lt;&lt;…&gt;&gt;&gt;传入的两个参数都是dim3类型的，之前的程序中直接传入int类型的,它应该会转换成dim3类型的.&nbsp;&nbsp;一个很直观的想法是可以把一个grid想像成一个三维的直角坐标系，然后一个block就是坐标系中的一个点,如果指定gridDim的x,y,z分别为2,3,4那么就有24个block,每个block可以用一个三维坐标来表示,同理可以可以把一个block想象成一个直角坐标系，thread也是其中的点. 运行结果 cudaDeviceReset可以看到这个函数是用来销毁一个CUDA的上下文的. It will reset the device immediately.虽然它表现上具有同步的功能，但是靠这个函数来同步是不安全的，所以建议如果是想同步的话，使用cudaDeviceSynchronize或者cudaMemcpy. 总结&nbsp;&nbsp;这篇博客总结了cuda线程的组织层次，从逻辑上讲一个kernel launch启动一个grid，可以用&lt;&lt;&lt;…&gt;&gt;&gt;语法设置gridDim和blockDim，从3个维度上进行设置. 一个grid包含多个block，这就是线程的两个层次的组织结构. 从硬件角度来讲，gpu的计算核心是多个SMs（streaming multiprocessors)，一个block在一个SM上运行，SM内部包含多个cuda core(不同的gpu架构的数目不同，Turing包含128个),32个cuda core构成一个warp.任意时刻一个SM中只会有一个warp处于活跃状态，其他的warp处于就绪或者挂起状态，从这点来看, SM非常类似于cpu中的core. 参考Programming Massively Parallel ProcessorsWhat is the role of cudaDeviceReset() in Cuda CUDA C Programming Guide]]></content>
      <tags>
        <tag>CUDA Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CUDA Hello World]]></title>
    <url>%2F2019%2F07%2F09%2FCUDA-Hello-World%2F</url>
    <content type="text"><![CDATA[简介&nbsp;&nbsp;第一次尝试编写CUDA C程序，主要是和以后的希望从事的方向有关，想从事大规模机器学习和深度学习系统的开发，CUDA是不可避免的需要掌握的并行计算的框架. 正好实验室有gpu服务器，趁着暑假来学习一波CUDA编程.&nbsp;&nbsp;CUDA是NVIDIA推出的运算平台，是一种并行计算的架构，使用GPU来进行通用计算. 编译CUDA程序的流程&nbsp;&nbsp;编译一个CUDA程序和C程序一样，CUDA程序的编译器是nvcc, CUDA程序文件的后缀是.cu. 开设我们有一个CUDA程序文件，命名为hello.cu, 那么我们用nvcc将它编译为可执行文件的命令如下: nvcc hello.cu -o hello CUDA Hello World&nbsp;&nbsp;学习任何程序设计语言的入门都是打印Hello World.CUDA程序也不例外， 下面我们以打印Hello World为例来解释CUDA程序的要素. #include&lt;stdio.h&gt;__global__ void hello()&#123; printf("hello world from GPU\n");&#125;int main()&#123; printf("hello world from CPU\n"); hello&lt;&lt;&lt;1, 10&gt;&gt;&gt;(); cudaDeviceSynchronize(); return 0;&#125; 我们用与之功能相近的普通C程序作为对比#include &lt;stdio.h&gt;void hello()&#123; printf("Hello World!\n");&#125;int main() &#123; hello(); return 0;&#125; 可以看到cuda程序相对于普通的c程序，有几点不同: __global__ 限定符&nbsp;&nbsp;在cuda程序中，cpu和gpu都用来做计算。我们把cpu叫做host，gpu叫做device. cpu和gpu拥有各自的存储空间。通常我们在cpu上顺序执行代码，在gpu上进行并行计算(Typically, we run serial workload on CPU and offload parallel computation to GPUs).&nbsp;&nbsp;__global__限定符表示hello函数是在gpu上执行的，是device代码,而且被该修饰符修饰的函数可以被host上的代码调用(这一点很重要，之后我们会看到，有的限定符表示只能被device代码或者只能被host代码调用)，我们的例子里hello函数就是被host上的main函数调用的,这样的函数也叫”kernels”. A kernel function must have a void return type(核函数的返回类型必须是void) &lt;&lt;&lt;…&gt;&gt;&gt; 语法&nbsp;&nbsp;当我们调用kernel的时候，它的执行的配置是通过&lt;&lt;&lt;…&gt;&gt;&gt;语法提供的，所谓的配置包括执行这个kernel用几个线程块，每个线程块开几个线程（这个涉及到gpu的结构）.比如上面的例子中，hello&lt;&lt;&lt;1, 10&gt;&gt;&gt;();. 在cuda中，这个叫做”kernel launch”(核启动). 具体参数之后的博客来说明. cudaDeviceSynchronize&nbsp;&nbsp; a kernel launch is asynchronous.因为kernel launch是异步执行的，当执行到device code的时候，在gpu上开启进程的时候，程序控制权就会回到cpu，不管gpu上的程序是否执行完毕。在我们的cuda程序中，如果没有cudaDeviceSynchronize函数，我们的程序就结束退出了，这样的话gpu端打印的hello world就不能打印到标准输出了. 而有了cudaDeviceSynchronize，cpu端的程序就会等device上的程序执行完后才退出，所以cudaDeviceSynchronize函数会阻塞直到device上的代码执行完毕. Vector Addition&nbsp;&nbsp;下面我们来看使用gpu进行向量相加运算的代码. 首先是使用cpu进行运算的代码：#define N 10000000void vector_add(float *out, float *a, float *b, int n) &#123; for(int i = 0; i &lt; n; i++)&#123; out[i] = a[i] + b[i]; &#125;&#125;int main()&#123; float *a, *b, *out; // Allocate memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); out = (float*)malloc(sizeof(float) * N); // Initialize array for(int i = 0; i &lt; N; i++)&#123; a[i] = 1.0f; b[i] = 2.0f; &#125; // Main function vector_add(out, a, b, N);&#125; 下面我们把向量相加的部分放到gpu上进行并行运算:#define N 100000__global__ void vector_add(float *out, float *a, float *b, int n) &#123; for(int i = 0; i &lt; n; i++)&#123; out[i] = a[i] + b[i]; &#125;&#125;int main()&#123; float *a, *b, *out; // Allocate memory a = (float*)malloc(sizeof(float) * N); b = (float*)malloc(sizeof(float) * N); out = (float*)malloc(sizeof(float) * N); // Initialize array for(int i = 0; i &lt; N; i++)&#123; a[i] = 1.0f; b[i] = 2.0f; &#125; float *d_a; float *d_b; float *d_out; cudaMalloc((void**)&amp;d_a, sizeof(float) * N); cudaMalloc((void**)&amp;d_b, sizeof(float) * N); cudaMalloc((void**)&amp;d_out, sizeof(float) * N); cudaMemcpy(d_a, a, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_b, b, sizeof(float) * N, cudaMemcpyHostToDevice); cudaMemcpy(d_out, out, sizeof(float) * N, cudaMemcpyHostToDevice); // Main function vector_add&lt;&lt;&lt;1,10&gt;&gt;&gt;(d_out, d_a, d_b, N); //cudaMemcpy(a, d_a, sizeof(float) * N, cudaMemcpyDeviceToHost); //cudaMemcpy(b, d_b, sizeof(float) * N, cudaMemcpyDeviceToHost); cudaMemcpy(out, d_out, sizeof(float) * N, cudaMemcpyDeviceToHost); for(int i=0;i&lt; N;i++)&#123; printf("%f\n",out[i]); &#125; printf("end!!!!"); cudaFree(d_a); cudaFree(d_b); cudaFree(d_out); free(a); free(b); free(out);&#125; &nbsp;&nbsp;可能一开始，我们会向hello world程序一样，给vector_add函数添加__global__修饰符.然后在main函数中调用vector_add函数的地方添加&lt;&lt;&lt;…&gt;&gt;&gt;. 这样以后我们编译运行程序但是发现程序的执行结果和我们预想的不一样. 是什么原因导致的呢？&nbsp;&nbsp;原因是cpu和gpu是各自拥有自己的存储空间,cpu无法直接获取gpu存储上的内容，gpu也无法直接获取到cpu存储上的内容. 在 cuda的术语里, cpu的存储叫做host memory, gpu的存储叫做device memory. 指向cpu内存的指针叫做host pointer, 指向gpu内存的指针叫做device pointer. 如果要让gpu能够获取到数据，那么数据必须在device memory上，cuda提供了分配device memory和在host和device之间进行数据迁移的api,cuda 程序的一个常见的流程如下: 分配host memory并初始化host上的数据 分配device memory(cudaMalloc) 将kernel函数要用的数据从host迁移到device上(cudaMemcpy) 执行kernel函数 将kernel函数的输出从device迁移到host上(cudaMemcpy) vec_add在gpu上运行，而out,a,b这三个向量，传入的是cpu上的地址空间，因此结果和我们预期的不一样. 这里还有个疑问，那为啥程序不报错呢，传入的是cpu上的存储地址.核函数内部居然不报错？ 这个程序相比于前面的hello world程序有很多新的函数需要解释： cudaMalloc和cudaFree&nbsp;&nbsp;这两个函数类似于c语言中的malloc和free函数，只不过这两个函数是在device memory上分配空间, 他们的函数原型如下：cudaMalloc(void **devPtr, size_t count);cudaFree(void *devPtr); cudaMalloc函数在device memory上分配size为count的空间，然后让devPtr指向分配的空间. 而 cudaFree将devPtr指向的空间给free了. cudaMemcpy&nbsp;&nbsp;cudaMemcpy函数用来在host和device之间传递数据，和c中的memcpy函数很像. 语法如下：cudaMemcpy(void *dst, void *src, size_t count, cudaMemcpyKind kind) 这个函数将size为count的存储从src复制到dst,kind指示复制的方向，最常用的值是cudaMemcpyHostToDevice 以及 cudaMemcpyDeviceToHost，分别表示从host复制到device以及从device复制到host. 参考Tutorial 01:Say Hello to CUDAwhy do we need cudaDeviceSynchronizecudaDeviceSynchronize and performing sequential work]]></content>
      <tags>
        <tag>CUDA Programming</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑回归交叉熵损失函数梯度推导]]></title>
    <url>%2F2019%2F07%2F06%2F%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[what is softmax&nbsp;&nbsp;softmax通常用于神经网络的输出层，用于多分类任务, 为每个类别产生一个概率，公式如下: softmax(\overrightarrow{z})=\overrightarrow{s}\overrightarrow{z} \in R^n , \overrightarrow{s} \in R^ns_i = \frac{e^{z_i}}{ \sum_{k=1}^{n} {e^{z_k}} }what is logistic regression&nbsp;&nbsp;逻辑回归是一个分类模型,假设输入$ \overrightarrow{x} \in R^n $, 模型参数$ \overrightarrow{w} \in R^n $, $ b \in R $ ,真实标签$ y \in \{0,1\} $ ,模型的预测输出是$ \hat{y} \in R $ 表示模型预测该实例为1的概率: P(Y=1|x) = \hat{y} = sigmoid(w*x+b) = \frac{1}{1+e^{-(w*x+b)}} =\frac{e^{w*x+b}}{1+e^{w*x+b}}从而: P(Y=0|x) = \frac{1}{1+e^{w*x+b}}sigmoid损失函数&nbsp;&nbsp;sigmoid损失函数的公式和形状如下，它能将输入的范围转化到[0,1]之间,作为概率值. sigmoid(z) = \frac{1}{1+e^{-z}} 极大似然估计&nbsp;&nbsp;极大似然估计是一种模型参数的估计方法，是频率学派的方法（另一个是贝叶斯学派，它们的参数估计方法是最大后验概率估计,两者的区别在于极大似然估计假设模型参数是一个固定值而最大后验概率估计假设模型参数也服从一定的分布，称为先验分布). 极大似然估计假设模型的参数是$ \Theta $,那么我们观察到的数据可以表示成参数的函数$ f(xi|\Theta) $,一般我们是有一个数据集，根据这个数据集合来估计模型参数，那么使得这个数据集合被我们观察到的概率是 f(x_1,x_2,x_3,...x_n|\Theta)因为数据集的数据是独立同分布的，所以下面的式子成立: f(x_1,x_2,x_3,...x_n|\Theta) = \prod_{i=1}^{n} {f(x_i|\Theta)}上面的式子就是似然函数，它是一个关于参数$ \Theta $的函数,而我们要选取使得该似然函数取得最大值的参数，这就是极大似然估计,也就是选取参数使得我们观察到的结果的可能性最大. 逻辑回归的损失函数推导&nbsp;&nbsp;逻辑回归的参数估计可以采用极大似然法，似然函数如下： \prod_{i=1}^{n} {P(Y=1|x)^{y_i}P(Y=0|x)^{1-y_i} } = \prod_{i=1}^{n} {\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}}我们使用负对数似然，对数函数不改变单调性，取负数使得该值大于0，那么损失函数如下： L = - \ln( \prod_{i=1}^{n} {\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}} ) = - \sum_{i=1}^{n} {y_i\ln\hat{y_i}+(1-y_i)\ln(1-\hat{y_i})}我们采用梯度下降法sgd来求解模型参书$ w $, $ b $: \frac{\partial L}{\partial w} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial w}}\frac{\partial L}{\partial \hat{y_i} } = - [ y_i \frac{\partial \ln\hat{y_i}}{\partial \hat{y_i}}+(1-y_i) \frac{\partial \ln(1- \hat{y_i})}{\partial \hat{y_i}} ] = - [ y_i \frac{1}{ \hat{y_i} } +(1-y_i) \frac{-1}{1- \hat{y_i}} ] = - \frac{y_i - \hat{y_i} }{ \hat{y_i} (1- \hat{y_i})}\frac{ \partial \hat{y_i} }{ \partial w} = \hat{y_i}(1- \hat{y_i} ) x_i综上: \frac{\partial L}{\partial w} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial w}} = - \sum_{i=1}^{n} {(y_i - \hat{y_i})x_i }$ w $的更新公式为: w = w + \eta \sum_{i=1}^{n}{(y_i- \hat{y_i})x_i }同理, $ b $的梯度如下: \frac{\partial L}{\partial b} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial b}} = - \sum_{i=1}^{n} {(y_i - \hat{y_i}) }b的更新公式如下: b = b + \eta \sum_{i=1}^{n}{(y_i- \hat{y_i}) }交叉熵损失函数以及softmax函数的梯度推导&nbsp;&nbsp;交叉熵用来衡量两个分布之间的距离,假设两个分布$ p $和$ q $,交叉熵的计算公式如下： H(p,q) = \sum_{i=1}^{n} {p_i \log \frac{1}{q_i}} = \sum_{i=1}^{n} {- p_i \log q_i}&nbsp;&nbsp;softmax的输出是一个概率分布，而真实标签的one-hot向量也是一个概率分布，真实类别的概率为1，其余类别的概率是0，所以用交叉熵来衡量两个分布之间的距离作为损失函数是合适的，其实该损失函数也可以使用极大似然估计推导出来，下面是推导过程: L = - \ln(\prod_{i=1}^{n} {\prod_{j=1}^{c} { \hat{y_{ij}}^{y_{ij}}} }) = \sum_{i=1}^{n} { [\sum_{j=1}^{c} {-y_{ij} \ln \hat{y_{ij}}} ] }其实数据集的损失是每个数据点的损失之和，[]内部就是交叉熵，下面我们推导损失函数的梯度，首先我们假设全连接中$ z_{ij} = w_j * x_i + b_j $ , 这里的 $ wj $ 表示所有与输出层的第$ j $个神经元连接的权值，这些权值构成一个向量: \frac{\partial L}{\partial w_j } = \sum_{i=1}^{n} {\frac{ \partial L }{ \partial z_{ij} } \frac{\partial z_{ij} }{\partial w_j }}\frac{ \partial L }{ \partial z_{ij} } = \sum_{k=1}^{c} { \frac{ \partial L}{ \partial \hat{y_{ik}} } \frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} }\frac{\partial L }{\partial \hat{y_{ik}}} = \frac{ \partial [ -y_{ik} \ln \hat{y_{ik}}] }{ \partial \hat{y_{ik}}}= \frac{-y_{ik}}{ \hat{y_{ik}}}下面得分两种情况考虑:如果 $ k=j $, \frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} = \frac{ \partial \frac{e^{z_{ij}}}{ \sum_{q=1}^{c} {e^{z_{iq}}}} }{ \partial z_{ij} } = \frac{ e^{z_{ij}} \sum_{q=1}^{c}{ e^{z_{iq}}} - e^{z_{ij}} e^{z_{ij}} }{ [ \sum_{q=1}^{c} {e^{z_{iq}}} ]^{2} } = \hat{y_{ij}} (1- \hat{y_{ij}} )如果 $ k \ne j $, \frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} = \frac{ \partial \frac{e^{z_{ik}}}{ \sum_{q=1}^{c} {e^{z_{iq}}}} }{ \partial z_{ij} } = \frac{ 0- e^{z_{ik}} e^{z_{ij}} }{ [ \sum_{q=1}^{c} {e^{z_{iq}}} ]^{2} } = - \hat{y_{ik}} \hat{y_{ij}}将上面的两个式子综合一下，得到如下的式子： \frac{ \partial L }{ \partial z_{ij} } = - y_{ij} (1- \hat{ y_{ij} }) + \sum_{k \ne j}^{c}{ y_{ik} \hat{y_{ij}} } = -y_{ij} + \hat{y_{ij}} \sum_{q=1}^{c} {y_{ik}} = \hat{ y_{ij}} -y_{ij}因为: \frac{\partial z_{ij}}{ \partial w_j } = x_i\frac{\partial z_{ij}}{ \partial b_j } = 1综上,$ w_j $的更新公式如下： w_j = w_j - \eta \sum_{i=1}^{n}{ ( \hat{ y_{ij}} -y_{ij}) x_i }同理,b的更新公式如下： b_j = b_j - \eta \sum_{i=1}^{n}{ ( \hat{ y_{ij}} -y_{ij}) }上面的公式可以由下面的代码来验证: 参考]]></content>
      <tags>
        <tag>deep learning</tag>
      </tags>
  </entry>
</search>
