<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>MPI Bcast,Gather,AllGather</title>
      <link href="/2019/11/19/MPI-Bcast-Gather-AllGather/"/>
      <url>/2019/11/19/MPI-Bcast-Gather-AllGather/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><h2 id="MPI-Allgather"><a href="#MPI-Allgather" class="headerlink" title="MPI_Allgather"></a>MPI_Allgather</h2><p>MPI_Gather将其他所有进程的数据收集到root进程上,与MPI_Scatter相反。而MPI_Allgather也是收集所有进程上的数<br>据，但是它不仅仅将数据收集到root进程，而是收集到所有的进程，图示如下：<br><img src="/.com//MPI_Allgather/allgather.png" alt title="allgather"><br>在每个进程上，收到的数据都会按照发送该数据的进程的秩来排序。<br>我们用一个简单的程序来实践一下，我们用MPI process的秩来初始化该进程的length变量,然后将length发送出去，当Allgather过程结束后，每个进程应该会有一个 数组，长度为总的进程数，然后里面的值为每个进程的length变量值，按照进程的秩排好序的。程序如下：<br><figure class="highlight c++"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;mpi.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdexcept&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;cstring&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc,<span class="keyword">char</span>* argv[])</span></span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> mpi_error ;</span><br><span class="line"></span><br><span class="line">mpi_error = MPI_Init(<span class="literal">NULL</span>,<span class="literal">NULL</span>);</span><br><span class="line"><span class="keyword">if</span>(mpi_error!=MPI_SUCCESS)&#123;</span><br><span class="line"> <span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">"MPI_Init failed with an error"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> rank,size;</span><br><span class="line">mpi_error = MPI_Comm_rank(MPI_COMM_WORLD,&amp;rank);</span><br><span class="line"><span class="keyword">if</span>(mpi_error!=MPI_SUCCESS)&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">"MPI_Comm_rank failed with an error"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">mpi_error = MPI_Comm_size(MPI_COMM_WORLD,&amp;size);</span><br><span class="line"><span class="keyword">if</span>(mpi_error!=MPI_SUCCESS)&#123;</span><br><span class="line"><span class="keyword">throw</span> <span class="built_in">std</span>::runtime_error(<span class="string">"MPI_Comm_size failed with an error"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">size_t</span> length = rank;</span><br><span class="line"></span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;<span class="keyword">size_t</span>&gt; lengths(size);</span><br><span class="line"></span><br><span class="line">MPI_Allgather(&amp;length,<span class="number">1</span>,MPI_UNSIGNED_LONG,&amp;lengths[<span class="number">0</span>],<span class="number">1</span>,MPI_UNSIGNED_LONG,MPI_COMM_WORLD);</span><br><span class="line"><span class="keyword">if</span>(rank==<span class="number">0</span>)&#123;</span><br><span class="line"><span class="keyword">for</span>(<span class="keyword">auto</span> t:lengths)&#123;</span><br><span class="line"></span><br><span class="line">  <span class="built_in">std</span>::<span class="built_in">cout</span>&lt;&lt; t &lt;&lt; <span class="built_in">std</span>::<span class="built_in">endl</span>;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">MPI_Finalize();</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>运行结果如下：<br><img src="/images/MPI_Allgather/pro.png" alt title="result"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://scc.ustc.edu.cn/zlsc/cxyy/200910/MPICH/mpi47.htm" target="_blank" rel="noopener">MPI Allgather</a><br><a href="https://mpitutorial.com/tutorials/mpi-scatter-gather-and-allgather/" target="_blank" rel="noopener">MPI scatter gather allgather</a><br>~<br>~<br>~<br>~</p>]]></content>
      
      
      
        <tags>
            
            <tag> MPI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>horovod</title>
      <link href="/2019/11/17/horovod/"/>
      <url>/2019/11/17/horovod/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>给ps-lite添加Angel中的psFunc</title>
      <link href="/2019/11/16/%E7%BB%99ps-lite%E6%B7%BB%E5%8A%A0Angel%E4%B8%AD%E7%9A%84psFunc/"/>
      <url>/2019/11/16/%E7%BB%99ps-lite%E6%B7%BB%E5%8A%A0Angel%E4%B8%AD%E7%9A%84psFunc/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ReduceScatter+Allgather</title>
      <link href="/2019/11/16/ReduceScatter-Allgather/"/>
      <url>/2019/11/16/ReduceScatter-Allgather/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Ring Allreduce架构也是一种常用的梯度同步架构，它的通信成本与gpu的数量无关。它是Allreduce算法的一种高效实现。目前常用的分布式深度学习的框架基本就是ps架构和Ring Allreduce架构。在这篇博客中，我会首先来回顾一下最简单的Allreduce算法的实现，然后是Ring Allreduce算法的实现，并给出baidu all-reduce的源码分析。</p><h2 id="理论"><a href="#理论" class="headerlink" title="理论"></a>理论</h2><p>Allreduce 是一种集合通信原语，它的规范的定义如下：</p><blockquote><p>AllReduce is an operation that reduces the target arrays in all processes to a single array and returns the resultant array to all processes.</p></blockquote><p>简单来说就是有很多个进程，每个进程都有一个目标数组，Allreduce要做的就是将所有进程中的数组收集起来做一个reduce操作成单个的数组，然后将这单个的数组重新发送给所有的进程。就是一个reduce+broadcast的操作。很容易理解，要实现Allreduce一个最简单的算法就是用一个process当master，其他的进程把自己的数组发送给master, master收集完后做一个reduce操作，然后再将结果数组发送给其余的所有进程，和ps架构一样。图示如下：</p><p><img src="/images/ReduceScatter+Allgather/master.png" alt title="simple implementation of Allreduce"></p><p>我们计算一下这种实现的通信量,假设数组的长度是N,总共有P个进程,那么第一步有P-1个进程将数组发送到master进程,然后master进程将reduce后的数组发送给P-1个进程，所以master总共的通信量是2(P-1)*N. 可以看到master的通信量和gpu的数量是线性关系的。而且master的带宽限制使得master成了一个瓶颈。</p><p>而Ring Allreduce作为Allreduce算法的一个高效实现，它消除了master的瓶颈，使得系统的通信成本和gpu的数量无关，是一个恒定值。在该算法中，所有的gpu排列成一个环，我们给gpu标号，从0~N-1.按照顺时针的方向，每个gpu会有一个左边的邻居和右边的邻居，每个迭代中，当前gpu从左邻居中接受数据，发送数据给右边的邻居.图示如下：<br><img src="/images/ReduceScatter+Allgather/RingAllreduce.png" alt title="ring allreduce"></p><p>我们来具体介绍算法流程:</p><p>首先，在每个gpu上，把长度为N的数组切分成P个chunks,P是进程的数量。<br><img src="/images/ReduceScatter+Allgather/split.png" alt title="split array"><br>接下来进行P-1次Scatter-Reduce迭代，第一次迭代，编号为n的gpu,将第n块发送给它的右邻居，从它的左邻居接收第n-1块。后面的每次迭代，每块gpu都将上一轮迭代接收到的块进行reduce操作后发送给它的右邻居。以下是第一次迭代，第二次迭代和最后一次迭代的图示：</p><p><img src="/images/ReduceScatter+Allgather/1.png" alt title="split array"><br><img src="/images/ReduceScatter+Allgather/2.png" alt title="split array"><br><img src="/images/ReduceScatter+Allgather/last.png" alt title="split array"></p><p>可以看到P-1次Reduce-Scatter迭代完成后，每个gpu都拥有最终结果的其中一块。我们需要把每个gpu中的这个块发送到其他gpu，这次每个gpu接收到块以后不是reduce操作，是直接覆盖对应块的内容即可。<br>方法是通过P-1次Allgather迭代，第一次迭代，第n个gpu发送第n+1个块接收第n个块.后续的迭代和Reduce-Scatter一样，也是发送上一轮迭代接收的到的块.下面是第一轮，第二轮以及最后的图示：</p><p><img src="/images/ReduceScatter+Allgather/11.png" alt title="split array"><br><img src="/images/ReduceScatter+Allgather/22.png" alt title="split array"><br><img src="/images/ReduceScatter+Allgather/last_gather.png" alt title="split array"></p><p>我们分析这个ring算法的总的通信量, 总共是2<em>(p-1)次迭代，每个迭代每块gpu都发送 N/P 的数据，所以每个gpu发送的数据量和接收的数据量都是 2</em>(P-1)N/P, 最后的结果与P无关。通过对比可以看到，ring allreduce算法的通信量更少而且与gpu数量无关，扩展性更好。</p><p>其实allreduce算法有很多种，还有 tree-allreduce等，像Rabit库中实现的应该是tree-allreduce, 后续可以补充。ring allreduce在通信上应该是最优的.</p><p>本篇博客中的所有图都截取自参考资料[5].</p><h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><p>我们通过对baidu all-reduce源码的解读和应用来实践ring all-reduce算法.</p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.jianshu.com/p/8c0e7edbefb9" target="_blank" rel="noopener">[1]Ring Allreduce</a><br><a href="https://blog.csdn.net/qq_35799003/article/details/85016537" target="_blank" rel="noopener">[2]All Reduce</a><br><a href="https://github.com/dmlc/rabit" target="_blank" rel="noopener">[3]Rabit</a><br><a href="https://github.com/baidu-research/baidu-allreduce" target="_blank" rel="noopener">[4]baidu all-reduce</a><br><a href="http://andrew.gibiansky.com/" target="_blank" rel="noopener">[5]Bringing HPC Techniques to Deep Learning</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> DDL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>bytescheduler在byteps中的实现</title>
      <link href="/2019/11/16/bytescheduler%E5%9C%A8byteps%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/"/>
      <url>/2019/11/16/bytescheduler%E5%9C%A8byteps%E4%B8%AD%E7%9A%84%E5%AE%9E%E7%8E%B0/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>cuda Stream</title>
      <link href="/2019/11/14/cuda-Stream/"/>
      <url>/2019/11/14/cuda-Stream/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>python的with语句以及__enter__和__exit__</title>
      <link href="/2019/11/10/python%E7%9A%84with%E8%AF%AD%E5%8F%A5%E4%BB%A5%E5%8F%8A-enter-%E5%92%8C-exit/"/>
      <url>/2019/11/10/python%E7%9A%84with%E8%AF%AD%E5%8F%A5%E4%BB%A5%E5%8F%8A-enter-%E5%92%8C-exit/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>想写一个DL的框架，前端想模仿tensorflow来写，看到 with tf.Graph().as_default, 就想看看Graph类如何定义，看到一个simpleflow的github项目，里面Graph的定义里有 <strong>enter</strong>和<strong>exit</strong>方法，不明白是什么意思就随手一查，反而让我知道了with语法的内部机制,算是一个比较大的收获。</p><h2 id><a href="#" class="headerlink" title=" "></a> </h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/lipijin/p/4460487.html" target="_blank" rel="noopener">python <strong>enter</strong> 与 <strong>exit</strong>的作用，以及与 with 语句的关系</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>阅读论文《A Generic Communication Scheduler for Distributed DNN Training Acceleration》</title>
      <link href="/2019/10/28/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E3%80%8AA-Generic-Communication-Scheduler-for-Distributed-DNN-Training-Acceleration%E3%80%8B/"/>
      <url>/2019/10/28/%E9%98%85%E8%AF%BB%E8%AE%BA%E6%96%87%E3%80%8AA-Generic-Communication-Scheduler-for-Distributed-DNN-Training-Acceleration%E3%80%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2>]]></content>
      
      
      
        <tags>
            
            <tag> DDL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++ typename的用法</title>
      <link href="/2019/09/24/c-typename%E7%9A%84%E7%94%A8%E6%B3%95/"/>
      <url>/2019/09/24/c-typename%E7%9A%84%E7%94%A8%E6%B3%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>生产者消费者模型</title>
      <link href="/2019/09/19/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/"/>
      <url>/2019/09/19/%E7%94%9F%E4%BA%A7%E8%80%85%E6%B6%88%E8%B4%B9%E8%80%85%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>c++11 std::thread</title>
      <link href="/2019/09/18/c-11-std-thread/"/>
      <url>/2019/09/18/c-11-std-thread/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>同步sgd/异步sgd</title>
      <link href="/2019/09/10/%E5%90%8C%E6%AD%A5sgd-%E5%BC%82%E6%AD%A5sgd/"/>
      <url>/2019/09/10/%E5%90%8C%E6%AD%A5sgd-%E5%BC%82%E6%AD%A5sgd/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>论文阅读: Parameter Server for Distributed Machine Learning</title>
      <link href="/2019/08/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Parameter-Server-for-Distributed-Machine-Learning/"/>
      <url>/2019/08/30/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB-Parameter-Server-for-Distributed-Machine-Learning/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>参数同步架构和算法</title>
      <link href="/2019/08/30/%E5%8F%82%E6%95%B0%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/"/>
      <url>/2019/08/30/%E5%8F%82%E6%95%B0%E5%90%8C%E6%AD%A5%E6%9E%B6%E6%9E%84%E5%92%8C%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>ring AllReduce</title>
      <link href="/2019/08/27/ring-AllReduce/"/>
      <url>/2019/08/27/ring-AllReduce/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>FM/DeepFM/NFM</title>
      <link href="/2019/08/22/FM-DeepFM-NFM/"/>
      <url>/2019/08/22/FM-DeepFM-NFM/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>分布式爬虫+推荐</title>
      <link href="/2019/08/16/%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB-%E6%8E%A8%E8%8D%90/"/>
      <url>/2019/08/16/%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB-%E6%8E%A8%E8%8D%90/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>分布式文件系统</title>
      <link href="/2019/08/16/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"/>
      <url>/2019/08/16/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GBDT</title>
      <link href="/2019/08/07/GBDT/"/>
      <url>/2019/08/07/GBDT/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>meta learning</title>
      <link href="/2019/08/05/meta-learning/"/>
      <url>/2019/08/05/meta-learning/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>自动微分</title>
      <link href="/2019/08/05/%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/"/>
      <url>/2019/08/05/%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86/</url>
      
        <content type="html"><![CDATA[<h2 id="符号微分"><a href="#符号微分" class="headerlink" title="符号微分"></a>符号微分</h2><h2 id="数值微分"><a href="#数值微分" class="headerlink" title="数值微分"></a>数值微分</h2><h2 id="前向自动微分"><a href="#前向自动微分" class="headerlink" title="前向自动微分"></a>前向自动微分</h2><h2 id="反向自动微分"><a href="#反向自动微分" class="headerlink" title="反向自动微分"></a>反向自动微分</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://zhuanlan.zhihu.com/p/47592565" target="_blank" rel="noopener">一天实现自己的自动微分</a><br><a href="https://blog.csdn.net/aws3217150/article/details/70214422" target="_blank" rel="noopener">自动微分简介</a><br><a href="https://github.com/apachecn/hands-on-ml-zh/blob/master/docs/D.%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86.md" target="_blank" rel="noopener">tensorflow 自动微分</a><br><a href="https://blog.csdn.net/daniel_ustc/article/details/77133329" target="_blank" rel="noopener">c++ 实现自动微分</a><br><a href="https://en.wikipedia.org/wiki/Automatic_differentiation" target="_blank" rel="noopener">wikpedia automatic differentiation</a><br><a href="https://www.zhihu.com/question/48356514/answer/123290631" target="_blank" rel="noopener">自动微分法是如何用c++实现的</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> deep learning system </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>beam search</title>
      <link href="/2019/08/05/beam-search/"/>
      <url>/2019/08/05/beam-search/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>隐马尔可夫模型</title>
      <link href="/2019/08/05/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/"/>
      <url>/2019/08/05/%E9%9A%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>RNN梯度消失和爆炸</title>
      <link href="/2019/08/04/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8/"/>
      <url>/2019/08/04/RNN%E6%A2%AF%E5%BA%A6%E6%B6%88%E5%A4%B1%E5%92%8C%E7%88%86%E7%82%B8/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BPTT</title>
      <link href="/2019/08/04/BP%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-BPTT/"/>
      <url>/2019/08/04/BP%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD-BPTT/</url>
      
        <content type="html"><![CDATA[<h2 id="BPTT-BackPropagation-Through-Time"><a href="#BPTT-BackPropagation-Through-Time" class="headerlink" title="BPTT (BackPropagation Through Time)"></a>BPTT (BackPropagation Through Time)</h2><p>用于循环神经网络的BP算法.首先我们回顾一下RNN,RNN的网络结构图如下：</p><p><img src="/images/BPTT/RNN.png" alt title="RNN"><br>标准RNN的数学表达式如下:</p><script type="math/tex; mode=display">s(t) = Ux(t)+Wh(t-1)+b</script><script type="math/tex; mode=display">h(t) = tanh(s(t))</script><script type="math/tex; mode=display">z(t) = Vh(t)+c</script><script type="math/tex; mode=display">\hat{y(t)} = softmax(z(t))</script><p>$ U $ , $ W $ , $ V $ 是待优化的参数. 上面的表达式可以和图对应,<strong>RNN网络的参数在所有时间步都是共享的</strong>.<br>我们参考材料[2]的记法，将公式细化如下:<br>首先我们计算s(t)的第j个元素,我们假设输入 $ x(t) $的维度是 $ l $ , 隐藏层的状态 $ h(t) $ 的维度是 $ m $ :</p><script type="math/tex; mode=display">s_j(t) = \sum_{i=1}^{l} {x_i(t)u_{ji} + \sum_{k=1}^{m} { h_k(t-1)wjk }}+b_j</script><p>下面我们计算 $ z(t) $ 的第j个元素:</p><script type="math/tex; mode=display">z_j(t) = \sum_{q=1}^{m} {h_q(t)v_{jq}}+c_j</script><p>因为每个时间步的输出都是一个概率向量，表示该时间步的词所属于各标签的概率分布，如果用one-hot向量表示真实标签$ y_t $，那么输出$ \hat{y_t} $和$ y_t $可以计算交叉熵损失$ E_t $, 公式如下：</p><script type="math/tex; mode=display">E_t = -{y_t}^T \ln( \hat{y_t})</script><script type="math/tex; mode=display">E = \sum_{t=1}^{T} {E_t}</script><p>我们要用损失函数 $ E $ 对 $ U $ , $ W $ , $ V $ 求偏导数，然后更新参数,我么使用sgd 算法所以每次更新使用一个样本.</p><p>我们拿出一个时间步的损失，并就该损失求$ U $ , $ W $ , $ V $ 的更新，结构图如下：<br><img src="/images/BPTT/model.png" alt title="model"><br>我们假设输出的概率分布的维度是 $ o $ , 并把时间维度 $ t $ 舍去, 那么:</p><script type="math/tex; mode=display">E_t = -{y_t}^T \ln( \hat{y_t})  = \sum_{k=1}^{o} { -y_k \ln \hat{y_k} }</script><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial \hat{y_k} } = - \frac{y_k}{ \hat{y_k} }</script><script type="math/tex; mode=display">\frac{ \partial \hat{y_k} }{ \partial z_j } = \begin{cases}\hat{y_j}(1- \hat{y_j} ) & \text{ k = j  }\\- \hat{y_k} { \hat{y_j }}& \text{ j != k }\end{cases}</script><p>综上：</p><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial z_j } = \sum_{k=1}^{o} { \frac{ \partial E_t}{ \partial \hat{y_k} } \frac{ \partial \hat{y_k} }{ \partial z_j } } = y_j - \hat{y_j}</script><script type="math/tex; mode=display">\frac{ \partial z_j  }{ \partial v_{jq}}  = h_q</script><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial v_{jq}} =  \frac{ \partial E_t }{ \partial z_j } \frac{ \partial z_j  }{ \partial v_{jq}} =( y_j - \hat{y_j} )h_q</script><p>根据上面的式子，我们能对矩阵v的所有元素进行更新.<br>下面我们计算 $ U $ 和 $ W $ 的更新式子:</p><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial h_q } = \sum_{j=1}^{o} {\frac{ \partial E_t}{ \partial z_j} \frac{ \partial z_j}{ \partial h_q} } = \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}}</script><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial  s_q  } = \frac{ \partial E_t }{ \partial h_q }  \frac{ \partial h_q }{ \partial s_q } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2)</script><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial  u_{qi}  } = \frac{ \partial E_t }{ \partial  s_q  } \frac{ \partial s_q }{ \partial  u_{qi}  } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2) x_i</script><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial  w_{qk}  } = \frac{ \partial E_t }{ \partial  s_q  } \frac{ \partial s_q }{ \partial  w_{qk}  } = [ \sum_{j=1}^{o} {(y_j - \hat{y_j} )v_{jq}} ](1- {tanh(s_q)}^2) h_k(t-1)</script><h3 id="W更新"><a href="#W更新" class="headerlink" title="W更新"></a>W更新</h3><script type="math/tex; mode=display">\frac{ \partial E}{ \partial W} = \sum_{t=1}^{T} { \frac{ \partial E_t }{ \partial W }}</script><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial W } = \frac{ \partial E_t }{ \partial \hat{y_t}  } \frac{ \partial \hat{y_t} }{ \partial W  }</script><script type="math/tex; mode=display">\frac{ \partial E_t }{ \partial \hat{y_t}  } = - \frac{y_t}{ \hat{y_t}}</script><p>这里的除法是element-wise的.<br>然后是$ \frac{ \partial \hat{y_t} }{ \partial W  } $ ，<del>这是一个向量对一个矩阵求偏导</del></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/wacc/p/5341670.html" target="_blank" rel="noopener">BPTT blog</a><br><a href="https://pan.baidu.com/s/1BQkpjxfhDWhPnGy9E_h12w" target="_blank" rel="noopener">bptt</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>simpledb(1)</title>
      <link href="/2019/08/04/simpledb-1/"/>
      <url>/2019/08/04/simpledb-1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>c++11之std::move</title>
      <link href="/2019/07/29/c-11%E4%B9%8Bstd-move/"/>
      <url>/2019/07/29/c-11%E4%B9%8Bstd-move/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>c++11之多线程std::thread</title>
      <link href="/2019/07/29/c-11%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8Bstd-thread/"/>
      <url>/2019/07/29/c-11%E4%B9%8B%E5%A4%9A%E7%BA%BF%E7%A8%8Bstd-thread/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> c++11 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>sqlite architecture</title>
      <link href="/2019/07/29/sqlite-architecture/"/>
      <url>/2019/07/29/sqlite-architecture/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> database </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>database index</title>
      <link href="/2019/07/29/database-index/"/>
      <url>/2019/07/29/database-index/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> database </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>B/B+ Tree</title>
      <link href="/2019/07/27/B-B-Tree/"/>
      <url>/2019/07/27/B-B-Tree/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> database </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习分布式训练</title>
      <link href="/2019/07/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
      <url>/2019/07/26/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/</url>
      
        <content type="html"><![CDATA[<h2 id="并行模式"><a href="#并行模式" class="headerlink" title="并行模式"></a>并行模式</h2><h3 id="数据并行"><a href="#数据并行" class="headerlink" title="数据并行"></a>数据并行</h3><h3 id="模型并行"><a href="#模型并行" class="headerlink" title="模型并行"></a>模型并行</h3><h3 id="混合并行"><a href="#混合并行" class="headerlink" title="混合并行"></a>混合并行</h3><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
      
      
      
        <tags>
            
            <tag> DDL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>architerture of sqlite</title>
      <link href="/2019/07/26/architerture-of-sqlite/"/>
      <url>/2019/07/26/architerture-of-sqlite/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>sqlite源码剖析(1)</title>
      <link href="/2019/07/26/sqlite%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-1/"/>
      <url>/2019/07/26/sqlite%E6%BA%90%E7%A0%81%E5%89%96%E6%9E%90-1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>傅立叶变换及其加强版FFT</title>
      <link href="/2019/07/26/%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%8A%A0%E5%BC%BA%E7%89%88FFT/"/>
      <url>/2019/07/26/%E5%82%85%E7%AB%8B%E5%8F%B6%E5%8F%98%E6%8D%A2%E5%8F%8A%E5%85%B6%E5%8A%A0%E5%BC%BA%E7%89%88FFT/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>caffe中的卷积计算的实现im2col</title>
      <link href="/2019/07/26/caffe%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0im2col/"/>
      <url>/2019/07/26/caffe%E4%B8%AD%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%AE%A1%E7%AE%97%E7%9A%84%E5%AE%9E%E7%8E%B0im2col/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>GPU内存层次</title>
      <link href="/2019/07/25/GPU%E5%86%85%E5%AD%98%E5%B1%82%E6%AC%A1/"/>
      <url>/2019/07/25/GPU%E5%86%85%E5%AD%98%E5%B1%82%E6%AC%A1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> CUDA Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>knowledge distillation</title>
      <link href="/2019/07/24/knowledge-distillation/"/>
      <url>/2019/07/24/knowledge-distillation/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>cudnn实践</title>
      <link href="/2019/07/23/cudnn%E5%AE%9E%E8%B7%B5/"/>
      <url>/2019/07/23/cudnn%E5%AE%9E%E8%B7%B5/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>矩阵乘法优化cublas</title>
      <link href="/2019/07/23/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E4%BC%98%E5%8C%96cublas/"/>
      <url>/2019/07/23/%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%E4%BC%98%E5%8C%96cublas/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>SIMD</title>
      <link href="/2019/07/23/intel%E5%90%91%E9%87%8F%E5%8C%96%E6%8C%87%E4%BB%A4/"/>
      <url>/2019/07/23/intel%E5%90%91%E9%87%8F%E5%8C%96%E6%8C%87%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<h2 id="SIMD"><a href="#SIMD" class="headerlink" title="SIMD"></a>SIMD</h2><p><img src="/images/simd/simd_1.png" alt title="simd"></p><h2 id="向量化"><a href="#向量化" class="headerlink" title="向量化"></a>向量化</h2><p><img src="/images/simd/simd_2.png" alt title="向量化"></p>]]></content>
      
      
      
        <tags>
            
            <tag> parallel programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>model parallelism/data parallelism</title>
      <link href="/2019/07/23/model-parallelism-data-parallelism/"/>
      <url>/2019/07/23/model-parallelism-data-parallelism/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BSP/SSP/A-SGD</title>
      <link href="/2019/07/23/BSP-SSP-A-SGD/"/>
      <url>/2019/07/23/BSP-SSP-A-SGD/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>BatchNormalization</title>
      <link href="/2019/07/23/BatchNormalization/"/>
      <url>/2019/07/23/BatchNormalization/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Dropout</title>
      <link href="/2019/07/23/Dropout/"/>
      <url>/2019/07/23/Dropout/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>mixed precision</title>
      <link href="/2019/07/23/mixed-precision/"/>
      <url>/2019/07/23/mixed-precision/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>REVISITING DISTRIBUTED SYNCHRONOUS SGD</title>
      <link href="/2019/07/23/REVISITING-DISTRIBUTED-SYNCHRONOUS-SGD/"/>
      <url>/2019/07/23/REVISITING-DISTRIBUTED-SYNCHRONOUS-SGD/</url>
      
        <content type="html"><![CDATA[<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://pan.baidu.com/s/1exPQ5bdliOc4qtUrG7Wc2A" target="_blank" rel="noopener">revisiting distributed synchronous sgd</a><br><a href="https://pan.baidu.com/s/1KIhI2_I-kehMmKnyVozE_g" target="_blank" rel="noopener">DC-ASGD</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> distributed machine learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>caffe源码(1)</title>
      <link href="/2019/07/22/caffe%E6%BA%90%E7%A0%81-1/"/>
      <url>/2019/07/22/caffe%E6%BA%90%E7%A0%81-1/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> caffe </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>词向量</title>
      <link href="/2019/07/20/%E8%AF%8D%E5%90%91%E9%87%8F/"/>
      <url>/2019/07/20/%E8%AF%8D%E5%90%91%E9%87%8F/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>模型压缩</title>
      <link href="/2019/07/20/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/"/>
      <url>/2019/07/20/%E6%A8%A1%E5%9E%8B%E5%8E%8B%E7%BC%A9/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>bert</title>
      <link href="/2019/07/20/bert/"/>
      <url>/2019/07/20/bert/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>attention is all you need</title>
      <link href="/2019/07/20/attention-is-all-you-need/"/>
      <url>/2019/07/20/attention-is-all-you-need/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>各种激活函数总结</title>
      <link href="/2019/07/20/%E5%90%84%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/"/>
      <url>/2019/07/20/%E5%90%84%E7%A7%8D%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>sequence2sequence实现机器翻译</title>
      <link href="/2019/07/12/sequence2sequence%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/"/>
      <url>/2019/07/12/sequence2sequence%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%99%A8%E7%BF%BB%E8%AF%91/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>bilstm+CRF实现命名实体识别</title>
      <link href="/2019/07/12/bilstm-CRF%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/"/>
      <url>/2019/07/12/bilstm-CRF%E5%AE%9E%E7%8E%B0%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="BiLSTM"><a href="#BiLSTM" class="headerlink" title="BiLSTM"></a>BiLSTM</h2><h2 id="CRF"><a href="#CRF" class="headerlink" title="CRF"></a>CRF</h2><h2 id="Tensorflow-implementation"><a href="#Tensorflow-implementation" class="headerlink" title="Tensorflow implementation"></a>Tensorflow implementation</h2><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://pan.baidu.com/s/1bReq4ygj7ZKUmZSUNdONEQ" target="_blank" rel="noopener">Bidirectional LSTM-CRF Models for Sequence Tagging</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++11之Deducing Types</title>
      <link href="/2019/07/12/c-11%E4%B9%8BDeducing-Types/"/>
      <url>/2019/07/12/c-11%E4%B9%8BDeducing-Types/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
        <tags>
            
            <tag> c++11 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA in Actions</title>
      <link href="/2019/07/12/CUDA-in-Actions/"/>
      <url>/2019/07/12/CUDA-in-Actions/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>&nbsp;&nbsp;之前我们用cuda实现了打印hello world以及向量相加. 这篇博客，我们来探究如何利用gpu的并行性. gpu的power就在于它的并行性.</p><h2 id="lt-lt-lt-…-gt-gt-gt-语法"><a href="#lt-lt-lt-…-gt-gt-gt-语法" class="headerlink" title="&lt;&lt;&lt;…&gt;&gt;&gt;语法"></a>&lt;&lt;&lt;…&gt;&gt;&gt;语法</h2><p>&nbsp;&nbsp;&lt;&lt;&lt;…&gt;&gt;&gt;是kernel函数执行的设置，比如用几个线程来执行核函数. cuda 将线程组织成线程块(thread block)，kernel可以启动很多线程块，并且把它们组织成grid数据结构(grid).<br>&nbsp;&nbsp;核函数启动设置的语法是<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">&lt;&lt;&lt; M , T &gt;&gt;&gt;</span><br></pre></td></tr></table></figure></p><p>&nbsp;&nbsp;表示kernel函数执行启动了一个grid,这个grid有M个线程块(thread block),每个线程块有T个线程.</p><h3 id="threadIdx-x-blockDim-x-以及-blockIdx-x"><a href="#threadIdx-x-blockDim-x-以及-blockIdx-x" class="headerlink" title="threadIdx.x, blockDim.x 以及 blockIdx.x"></a>threadIdx.x, blockDim.x 以及 blockIdx.x</h3><p>&nbsp;&nbsp;cuda提供了内置变量来获取线程信息，这里我们使用两个. threadIdx.x 表示线程块中的线程编号(以0开始), blockIdx.x表示线程块中的线程数.因为可以获取线程信息，所以函数内部可以根据当前的线程来决定执行怎样的操作.比如向量相加，不同的线程计算不同区间的向量相加，先获取当前线程号，根据线程号确定向量相加的范围，执行计算.blockDim.x表示一个thread block的线程数量.</p><h2 id="并行计算-vector-addition"><a href="#并行计算-vector-addition" class="headerlink" title="并行计算 vector addition"></a>并行计算 vector addition</h2><p>假设一个线程块有256个线程</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> index = threadIdx.x;</span><br><span class="line">    <span class="keyword">int</span> stride = blockDim.x;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = index; i &lt; n; i += stride)&#123;</span><br><span class="line">        out[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的代码的想法可以用如下的图来说明:<br><img src="/images/CUDA_in_Actions/01_parallel_thread.png" alt title="向量相加"></p><p>不同线程写不同位置，不存在冲突，这里突然想起来，如果按照一个线程的写法但是开多个线程运行，是不是应该会冲突？？之后可以学习一下gpu线程的同步问题.</p><p>我们用nvprof来验证用1个线程和256个线程完成向量相加的执行情况，主要看执行时间：<br><img src="/images/CUDA_in_Actions/thread_block.png" alt title="对比图"></p><p>可以看到执行时间明显缩短了.现在的程序只开了一个thread block,我们尝试开启多个thread block.</p><h2 id="多个thread-block并行计算向量相加"><a href="#多个thread-block并行计算向量相加" class="headerlink" title="多个thread block并行计算向量相加"></a>多个thread block并行计算向量相加</h2><p>&nbsp;&nbsp;cuda提供了内置的变量来获取thread block的信息，包括block的编号(blockIdx.x)，一个grid有多少个blocks(gridDim.x).<br>&nbsp;&nbsp;使用多个grid来并行化向量相加的示意图如下所示:<br><img src="/images/CUDA_in_Actions/02_parallel_block.png" alt title="multithread"></p><p>&nbsp;&nbsp;想法就是每个block有256个thread,每个thread负责一个计算一个元素相加，然后总共有 N/256个thread block,因为N不一定是256的倍数，所以在核函数中还要判断index是否小于N..<br>这样每个元素都是同时计算的,进一步加大并行化.期望的执行时间也应该减少.</p><p>对应的程序如下：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">   <span class="keyword">int</span> i = blockIdx.x*blockDim.x+threadIdx.x;</span><br><span class="line">   <span class="keyword">if</span>(i&lt;n)out[i] = a[i] + b[i];</span><br></pre></td></tr></table></figure></p><p>程序运行性能如下:<br><img src="/images/CUDA_in_Actions/multiblock.png" alt title="multiblock"></p><h2 id="性能比较"><a href="#性能比较" class="headerlink" title="性能比较"></a>性能比较</h2><p>N=700000</p><div class="table-container"><table><thead><tr><th>version</th><th>Execution Time(ms)</th><th>Speedup</th></tr></thead><tbody><tr><td>1 thread</td><td>977.26</td><td>1.00x</td></tr><tr><td>1 block</td><td>5.5417</td><td>176.35x </td></tr><tr><td>Multiple blocks</td><td>0.13274</td><td>7360.25x</td></tr></tbody></table></div><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>这篇博客分别通过用一个thread block,每个thread block 256个线程和多个thread block，每个thread block 256个线程计算向量相加来展示如何使用gpu来并行计算.其中强调了三个概念 grid , threadblock以及 thread….接下来我们学习一个gpu的架构.</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial02/" target="_blank" rel="noopener">Tutorial 02 CUDA in Actions</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> CUDA Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM,RNN,Bi-LSTM,On-LSTM</title>
      <link href="/2019/07/12/LSTM-RNN-Bi-LSTM-On-LSTM/"/>
      <url>/2019/07/12/LSTM-RNN-Bi-LSTM-On-LSTM/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CUDA线程层次-从硬件和软件角度</title>
      <link href="/2019/07/10/GPU%E6%9E%B6%E6%9E%84/"/>
      <url>/2019/07/10/GPU%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<h2 id="Streaming-Multiprocessors"><a href="#Streaming-Multiprocessors" class="headerlink" title="Streaming Multiprocessors"></a>Streaming Multiprocessors</h2><p>GPU是由流多处理器构成的，每个SM内部有多个core, 每个core跑一个thread.</p><blockquote><p>The idea is that the CPU spawns a thread per element, and the GPU then executes those threads.Not all of the thousands or millions of threads actually run in parallel, but many do.Specifically, an NVIDIA GPU contains several largely independent processors called “Streaming Multiprocessors” (SMs), each SM hosts several “cores”, and each “core” runs a thread.For instance, Fermi has up to 16 SMs with 32 cores per SM – so up to 512 thread can run in parallel.</p><p>All threads running on the cores of an SM at a given cycle are executing the same instruction – hence Single Instruction, Multiple Threads. However, each thread has its own registers, so these instructions process different data.</p></blockquote><p><img src="/images/gpu_arch/SMs.png" alt title="SMs"><br>&nbsp;&nbsp;可以看到一个thread block跑在一个SM上面. 一个block执行完毕，SM可以调度另一个block执行，在一个SM上,block的执行是顺序的.</p><p><img src="/images/gpu_arch/inside_1.png" alt title="inside a gpu"><br>&nbsp;&nbsp;一个SM只有一个指令单元，SM内的所有thread共享这个指令单元.</p><h3 id="Warps"><a href="#Warps" class="headerlink" title="Warps"></a>Warps</h3><p>&nbsp;&nbsp;一个thread block在一个SM上执行,block中的thread可以继续分成warp,一个warp包括32个线程.<strong>warp是SM调度运行的基本单元</strong>,一个warp中的所有线程执行相同的指令.<strong>任意时刻，SM中只会有一个warp在运行，其余的warp都处于就绪等其他状态</strong>. 不同架构的gpu一个SM包含不同数量的cuda核心，Turing架构的一个SM包含128个cuda core,也就是4个warp. SM中有硬件warp scheduler，用来调度warp运行. </p><h2 id="thread-block-organization"><a href="#thread-block-organization" class="headerlink" title="thread block organization"></a>thread block organization</h2><p><img src="/images/gpu_arch/thread_block_org.png" alt title="thread block organization"><br>&nbsp;&nbsp;grid由多个blocks组成，每个kernel function的调用都会create一个grid,所以&lt;&lt;&lt;…&gt;&gt;&gt;语法指定的是block的数量和每个block的线程数量,因为这个kernel函数只会在一个grid上运行.</p><p><img src="/images/gpu_arch/thread_cooperate.png" alt title="thread cooperate"><br>&nbsp;&nbsp;同一个block内的所有thread可以合作(因为有个Shared Memory??)，不同block的thread不可以合作.</p><p>&nbsp;&nbsp;从上图中可以看到每个block都有一个Shared Memory,是所有block中的thread所共享的,每个thread有自己的Local Memory和Registers. 一个Grid中的所有blocks共享Global Memory, Constant Memory以及Texture Memory. 所有blocks中的thread都共享这三种Memory,内存层次会在后序的博客中进行总结.</p><h2 id="uint3和Dim3-数据结构"><a href="#uint3和Dim3-数据结构" class="headerlink" title="uint3和Dim3 数据结构"></a>uint3和Dim3 数据结构</h2><h3 id="uint3"><a href="#uint3" class="headerlink" title="uint3"></a>uint3</h3><p><img src="/images/gpu_arch/uint3.png" alt title="uint3"></p><h3 id="dim3"><a href="#dim3" class="headerlink" title="dim3"></a>dim3</h3><p><img src="/images/gpu_arch/dim3.png" alt title="dim3"></p><h3 id="block-and-grid-dimensions"><a href="#block-and-grid-dimensions" class="headerlink" title="block and grid dimensions"></a>block and grid dimensions</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">checkIndex</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"threadIdx:(%d, %d, %d) blockIdx:(%d, %d, %d) blockDim:(%d, %d, %d) "</span></span><br><span class="line"><span class="string">"gridDim:(%d, %d, %d)\n"</span>, threadIdx.x, threadIdx.y, threadIdx.z, blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z, gridDim.x,gridDim.y,gridDim.z);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123; <span class="comment">// define total data element</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> nElem = <span class="number">6</span>;</span><br><span class="line"><span class="comment">// define grid and block structure</span></span><br><span class="line"></span><br><span class="line"><span class="function">dim3 <span class="title">block</span> <span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span> <span class="params">((nElem+block.x<span class="number">-1</span>)/block.x)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// check grid and block dimension from host side</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"grid.x %d grid.y %d grid.z %d\n"</span>,grid.x, grid.y, grid.z);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"block.x %d block.y %d block.z %d\n"</span>,block.x, block.y, block.z);</span><br><span class="line"></span><br><span class="line"><span class="comment">// check grid and block dimension from device side </span></span><br><span class="line">checkIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// reset device before you leave</span></span><br><span class="line">cudaDeviceReset();</span><br><span class="line"><span class="keyword">return</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>&nbsp;&nbsp;可以看到上面的程序定义了两个dim3类型的变量block和grid. 因为nElem的值是6,所以grid的x分量是2,因为这两个变量都只是指定了x分量，所以其余的分量都初始化为1.目前来看&lt;&lt;…&gt;&gt;&gt;传入的两个参数都是dim3类型的，之前的程序中直接传入int类型的,它应该会转换成dim3类型的.<br>&nbsp;&nbsp;<strong>一个很直观的想法是可以把一个grid想像成一个三维的直角坐标系，然后一个block就是坐标系中的一个点,如果指定gridDim的x,y,z分别为2,3,4那么就有24个block,每个block可以用一个三维坐标来表示,同理可以可以把一个block想象成一个直角坐标系，thread也是其中的点.</strong></p><h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p><img src="/images/gpu_arch/result.png" alt title="result.png"></p><h3 id="cudaDeviceReset"><a href="#cudaDeviceReset" class="headerlink" title="cudaDeviceReset"></a>cudaDeviceReset</h3><p><img src="/images/gpu_arch/cudaDeviceRest.png" alt title="cudaDeviceRest"><br>可以看到这个函数是用来<strong>销毁一个CUDA的上下文的</strong>. It will reset the device immediately.虽然它表现上具有同步的功能，但是靠这个函数来同步是不安全的，所以建议如果是想同步的话，使用cudaDeviceSynchronize或者cudaMemcpy.</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&nbsp;&nbsp;这篇博客总结了cuda线程的组织层次，从逻辑上讲一个kernel launch启动一个grid，可以用&lt;&lt;&lt;…&gt;&gt;&gt;语法设置gridDim和blockDim，从3个维度上进行设置. 一个grid包含多个block，这就是线程的两个层次的组织结构. 从硬件角度来讲，gpu的计算核心是多个SMs（streaming multiprocessors)，一个block在一个SM上运行，SM内部包含多个cuda core(不同的gpu架构的数目不同，Turing包含128个),32个cuda core构成一个warp.任意时刻一个SM中只会有一个warp处于活跃状态，其他的warp处于就绪或者挂起状态，从这点来看, SM非常类似于cpu中的core.</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://pan.baidu.com/s/1AveSDmaRpR6ZyVisNTHm1A" target="_blank" rel="noopener">Programming Massively Parallel Processors</a><br><a href="https://stackoverflow.com/questions/36012289/what-is-the-role-of-cudadevicereset-in-cuda" target="_blank" rel="noopener">What is the role of cudaDeviceReset() in Cuda</a><br><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#scalable-programming-model" target="_blank" rel="noopener">CUDA C Programming Guide</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> CUDA Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CUDA Hello World</title>
      <link href="/2019/07/09/CUDA-Hello-World/"/>
      <url>/2019/07/09/CUDA-Hello-World/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>&nbsp;&nbsp;第一次尝试编写CUDA C程序，主要是和以后的希望从事的方向有关，想从事大规模机器学习和深度学习系统的开发，CUDA是不可避免的需要掌握的并行计算的框架. 正好实验室有gpu服务器，趁着暑假来学习一波CUDA编程.<br>&nbsp;&nbsp;CUDA是NVIDIA推出的运算平台，是一种并行计算的架构，使用GPU来进行通用计算.</p><h2 id="编译CUDA程序的流程"><a href="#编译CUDA程序的流程" class="headerlink" title="编译CUDA程序的流程"></a>编译CUDA程序的流程</h2><p>&nbsp;&nbsp;编译一个CUDA程序和C程序一样，CUDA程序的编译器是nvcc, CUDA程序文件的后缀是.cu. 开设我们有一个CUDA程序文件，命名为hello.cu, 那么我们用nvcc将它编译为可执行文件的命令如下:</p><p><code>nvcc hello.cu -o hello</code></p><h2 id="CUDA-Hello-World"><a href="#CUDA-Hello-World" class="headerlink" title="CUDA Hello World"></a>CUDA Hello World</h2><p>&nbsp;&nbsp;学习任何程序设计语言的入门都是打印Hello World.CUDA程序也不例外， 下面我们以打印Hello World为例来解释CUDA程序的要素.</p><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"> <span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"hello world from GPU\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="built_in">printf</span>(<span class="string">"hello world from CPU\n"</span>);</span><br><span class="line">  hello&lt;&lt;&lt;<span class="number">1</span>, <span class="number">10</span>&gt;&gt;&gt;();</span><br><span class="line">  cudaDeviceSynchronize();</span><br><span class="line">  <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们用与之功能相近的普通C程序作为对比<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">hello</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"Hello World!\n"</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    hello();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>可以看到cuda程序相对于普通的c程序，有几点不同:</p><h3 id="global-限定符"><a href="#global-限定符" class="headerlink" title="__global__ 限定符"></a>__global__ 限定符</h3><p>&nbsp;&nbsp;在cuda程序中，cpu和gpu都用来做计算。我们把cpu叫做host，gpu叫做device. cpu和gpu拥有各自的存储空间。通常我们在cpu上顺序执行代码，在gpu上进行并行计算(Typically, we run serial workload on CPU and offload parallel computation to GPUs).<br>&nbsp;&nbsp;__global__限定符表示hello函数是在gpu上执行的，是device代码,而且<strong>被该修饰符修饰的函数可以被host上的代码调用</strong>(这一点很重要，之后我们会看到，有的限定符表示只能被device代码或者只能被host代码调用)，我们的例子里hello函数就是被host上的main函数调用的,这样的函数也叫”kernels”. <strong>A kernel function must have a void return type(核函数的返回类型必须是void)</strong></p><h3 id="lt-lt-lt-…-gt-gt-gt-语法"><a href="#lt-lt-lt-…-gt-gt-gt-语法" class="headerlink" title="&lt;&lt;&lt;…&gt;&gt;&gt; 语法"></a>&lt;&lt;&lt;…&gt;&gt;&gt; 语法</h3><p>&nbsp;&nbsp;当我们调用kernel的时候，它的执行的配置是通过&lt;&lt;&lt;…&gt;&gt;&gt;语法提供的，所谓的配置包括执行这个kernel用几个线程块，每个线程块开几个线程（这个涉及到gpu的结构）.比如上面的例子中，<code>hello&lt;&lt;&lt;1, 10&gt;&gt;&gt;();</code>. 在cuda中，这个叫做”kernel launch”(核启动). 具体参数之后的博客来说明.</p><h3 id="cudaDeviceSynchronize"><a href="#cudaDeviceSynchronize" class="headerlink" title="cudaDeviceSynchronize"></a>cudaDeviceSynchronize</h3><p>&nbsp;&nbsp; a kernel launch is asynchronous.因为kernel launch是异步执行的，当执行到device code的时候，在gpu上开启进程的时候，程序控制权就会回到cpu，不管gpu上的程序是否执行完毕。在我们的cuda程序中，如果没有cudaDeviceSynchronize函数，我们的程序就结束退出了，这样的话gpu端打印的hello world就不能打印到标准输出了. 而有了cudaDeviceSynchronize，cpu端的程序就会等device上的程序执行完后才退出，所以cudaDeviceSynchronize函数会阻塞直到device上的代码执行完毕.</p><h2 id="Vector-Addition"><a href="#Vector-Addition" class="headerlink" title="Vector Addition"></a>Vector Addition</h2><p>&nbsp;&nbsp;下面我们来看使用gpu进行向量相加运算的代码. 首先是使用cpu进行运算的代码：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 10000000</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">        out[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *a, *b, *out; </span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate memory</span></span><br><span class="line">    a   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    b   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    out = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize array</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = <span class="number">1.0f</span>; b[i] = <span class="number">2.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Main function</span></span><br><span class="line">    vector_add(out, a, b, N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>下面我们把向量相加的部分放到gpu上进行并行运算:<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> N 100000</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">vector_add</span><span class="params">(<span class="keyword">float</span> *out, <span class="keyword">float</span> *a, <span class="keyword">float</span> *b, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++)&#123;</span><br><span class="line">        out[i] = a[i] + b[i];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    <span class="keyword">float</span> *a, *b, *out;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Allocate memory</span></span><br><span class="line">    a   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    b   = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    out = (<span class="keyword">float</span>*)<span class="built_in">malloc</span>(<span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Initialize array</span></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; i++)&#123;</span><br><span class="line">        a[i] = <span class="number">1.0f</span>; b[i] = <span class="number">2.0f</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">float</span> *d_a;</span><br><span class="line">    <span class="keyword">float</span> *d_b;</span><br><span class="line">    <span class="keyword">float</span> *d_out;</span><br><span class="line"></span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMalloc((<span class="keyword">void</span>**)&amp;d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N);</span><br><span class="line">    cudaMemcpy(d_a, a, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(d_b, b, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line">    cudaMemcpy(d_out, out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Main function</span></span><br><span class="line">    vector_add&lt;&lt;&lt;<span class="number">1</span>,<span class="number">10</span>&gt;&gt;&gt;(d_out, d_a, d_b, N);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//cudaMemcpy(a, d_a, sizeof(float) * N, cudaMemcpyDeviceToHost);</span></span><br><span class="line">    <span class="comment">//cudaMemcpy(b, d_b, sizeof(float) * N, cudaMemcpyDeviceToHost);</span></span><br><span class="line">    cudaMemcpy(out, d_out, <span class="keyword">sizeof</span>(<span class="keyword">float</span>) * N, cudaMemcpyDeviceToHost);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="keyword">int</span> i=<span class="number">0</span>;i&lt; N;i++)&#123;</span><br><span class="line">     <span class="built_in">printf</span>(<span class="string">"%f\n"</span>,out[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">"end!!!!"</span>);</span><br><span class="line">    cudaFree(d_a);</span><br><span class="line">    cudaFree(d_b);</span><br><span class="line">    cudaFree(d_out);</span><br><span class="line">    <span class="built_in">free</span>(a);</span><br><span class="line">    <span class="built_in">free</span>(b);</span><br><span class="line">    <span class="built_in">free</span>(out);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p><p>&nbsp;&nbsp;可能一开始，我们会向hello world程序一样，给vector_add函数添加__global__修饰符.然后在main函数中调用vector_add函数的地方添加&lt;&lt;&lt;…&gt;&gt;&gt;. 这样以后我们编译运行程序但是发现程序的执行结果和我们预想的不一样. 是什么原因导致的呢？<br>&nbsp;&nbsp;原因是cpu和gpu是各自拥有自己的存储空间,cpu无法直接获取gpu存储上的内容，gpu也无法直接获取到cpu存储上的内容. 在 cuda的术语里, cpu的存储叫做host memory, gpu的存储叫做device memory. 指向cpu内存的指针叫做host pointer, 指向gpu内存的指针叫做device pointer. 如果要让gpu能够获取到数据，那么数据必须在device memory上，cuda提供了分配device memory和在host和device之间进行数据迁移的api,cuda 程序的一个常见的流程如下:</p><ul><li>分配host memory并初始化host上的数据</li><li>分配device memory(cudaMalloc)</li><li>将kernel函数要用的数据从host迁移到device上(cudaMemcpy)</li><li>执行kernel函数</li><li>将kernel函数的输出从device迁移到host上(cudaMemcpy)</li></ul><p>vec_add在gpu上运行，而out,a,b这三个向量，传入的是cpu上的地址空间，因此结果和我们预期的不一样. 这里还有个疑问，那为啥程序不报错呢，传入的是cpu上的存储地址.核函数内部居然不报错？</p><p>这个程序相比于前面的hello world程序有很多新的函数需要解释：</p><h3 id="cudaMalloc和cudaFree"><a href="#cudaMalloc和cudaFree" class="headerlink" title="cudaMalloc和cudaFree"></a>cudaMalloc和cudaFree</h3><p>&nbsp;&nbsp;这两个函数类似于c语言中的malloc和free函数，只不过这两个函数是在device memory上分配空间, 他们的函数原型如下：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">cudaMalloc(<span class="keyword">void</span> **devPtr, <span class="keyword">size_t</span> count);</span><br><span class="line">cudaFree(<span class="keyword">void</span> *devPtr);</span><br></pre></td></tr></table></figure></p><p>cudaMalloc函数在device memory上分配size为count的空间，然后让devPtr指向分配的空间. 而 cudaFree将devPtr指向的空间给free了.</p><h3 id="cudaMemcpy"><a href="#cudaMemcpy" class="headerlink" title="cudaMemcpy"></a>cudaMemcpy</h3><p>&nbsp;&nbsp;cudaMemcpy函数用来在host和device之间传递数据，和c中的memcpy函数很像. 语法如下：<br><figure class="highlight c"><table><tr><td class="code"><pre><span class="line">cudaMemcpy(<span class="keyword">void</span> *dst, <span class="keyword">void</span> *src, <span class="keyword">size_t</span> count, cudaMemcpyKind kind)</span><br></pre></td></tr></table></figure></p><p>这个函数将size为count的存储从src复制到dst,kind指示复制的方向，最常用的值是cudaMemcpyHostToDevice 以及 cudaMemcpyDeviceToHost，分别表示从host复制到device以及从device复制到host.</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial01/" target="_blank" rel="noopener">Tutorial 01:Say Hello to CUDA</a><br><a href="https://stackoverflow.com/questions/19193468/why-do-we-need-cudadevicesynchronize-in-kernels-with-device-printf" target="_blank" rel="noopener">why do we need cudaDeviceSynchronize</a><br><a href="https://stackoverflow.com/questions/25332476/cudadevicesynchronize-and-performing-sequential-work?noredirect=1" target="_blank" rel="noopener">cudaDeviceSynchronize and performing sequential work</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> CUDA Programming </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>逻辑回归交叉熵损失函数梯度推导</title>
      <link href="/2019/07/06/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/"/>
      <url>/2019/07/06/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E6%A2%AF%E5%BA%A6%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<h2 id="what-is-softmax"><a href="#what-is-softmax" class="headerlink" title="what is softmax"></a>what is softmax</h2><p>&nbsp;&nbsp;softmax通常用于神经网络的输出层，用于多分类任务, 为每个类别产生一个概率，公式如下:</p><script type="math/tex; mode=display">softmax(\overrightarrow{z})=\overrightarrow{s}</script><script type="math/tex; mode=display">\overrightarrow{z} \in R^n , \overrightarrow{s} \in R^n</script><script type="math/tex; mode=display">s_i = \frac{e^{z_i}}{ \sum_{k=1}^{n} {e^{z_k}} }</script><h2 id="what-is-logistic-regression"><a href="#what-is-logistic-regression" class="headerlink" title="what is logistic regression"></a>what is logistic regression</h2><p>&nbsp;&nbsp;逻辑回归是一个分类模型,假设输入$ \overrightarrow{x} \in R^n $, 模型参数$ \overrightarrow{w} \in R^n $, $ b \in R $ ,真实标签$ y \in \{0,1\} $ ,模型的预测输出是$ \hat{y} \in R $ 表示模型预测该实例为1的概率:</p><script type="math/tex; mode=display">P(Y=1|x) =  \hat{y} = sigmoid(w*x+b) = \frac{1}{1+e^{-(w*x+b)}} =\frac{e^{w*x+b}}{1+e^{w*x+b}}</script><p>从而:</p><script type="math/tex; mode=display">P(Y=0|x) = \frac{1}{1+e^{w*x+b}}</script><h2 id="sigmoid损失函数"><a href="#sigmoid损失函数" class="headerlink" title="sigmoid损失函数"></a>sigmoid损失函数</h2><p>&nbsp;&nbsp;sigmoid损失函数的公式和形状如下，它能将输入的范围转化到[0,1]之间,作为概率值.</p><script type="math/tex; mode=display">sigmoid(z) = \frac{1}{1+e^{-z}}</script><p><img src="/images/cross_entropy/sigmoid.png" alt title="sigmoid"></p><h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>&nbsp;&nbsp;极大似然估计是一种模型参数的估计方法，是频率学派的方法（另一个是贝叶斯学派，它们的参数估计方法是最大后验概率估计,两者的区别在于极大似然估计假设模型参数是一个固定值而最大后验概率估计假设模型参数也服从一定的分布，称为先验分布). 极大似然估计假设模型的参数是$ \Theta $,那么我们观察到的数据可以表示成参数的函数$ f(xi|\Theta) $,一般我们是有一个数据集，根据这个数据集合来估计模型参数，那么使得这个数据集合被我们观察到的概率是</p><script type="math/tex; mode=display">f(x_1,x_2,x_3,...x_n|\Theta)</script><p>因为数据集的数据是独立同分布的，所以下面的式子成立:</p><script type="math/tex; mode=display">f(x_1,x_2,x_3,...x_n|\Theta) = \prod_{i=1}^{n} {f(x_i|\Theta)}</script><p>上面的式子就是似然函数，它是一个关于参数$ \Theta $的函数,而我们要选取使得该似然函数取得最大值的参数，这就是极大似然估计,也就是选取参数使得我们观察到的结果的可能性最大.</p><h2 id="逻辑回归的损失函数推导"><a href="#逻辑回归的损失函数推导" class="headerlink" title="逻辑回归的损失函数推导"></a>逻辑回归的损失函数推导</h2><p>&nbsp;&nbsp;逻辑回归的参数估计可以采用极大似然法，似然函数如下：</p><script type="math/tex; mode=display">\prod_{i=1}^{n} {P(Y=1|x)^{y_i}P(Y=0|x)^{1-y_i} } = \prod_{i=1}^{n} {\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}}</script><p>我们使用负对数似然，对数函数不改变单调性，取负数使得该值大于0，那么损失函数如下：</p><script type="math/tex; mode=display">L = - \ln( \prod_{i=1}^{n} {\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}} ) = - \sum_{i=1}^{n} {y_i\ln\hat{y_i}+(1-y_i)\ln(1-\hat{y_i})}</script><p>我们采用梯度下降法sgd来求解模型参书$ w $, $ b $:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial w}}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial \hat{y_i} }  = - [ y_i \frac{\partial \ln\hat{y_i}}{\partial \hat{y_i}}+(1-y_i) \frac{\partial \ln(1- \hat{y_i})}{\partial \hat{y_i}} ] = - [ y_i \frac{1}{ \hat{y_i} } +(1-y_i) \frac{-1}{1- \hat{y_i}} ]  = - \frac{y_i - \hat{y_i} }{ \hat{y_i} (1- \hat{y_i})}</script><script type="math/tex; mode=display">\frac{ \partial \hat{y_i} }{ \partial w} = \hat{y_i}(1- \hat{y_i} ) x_i</script><p>综上:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial w}} = - \sum_{i=1}^{n} {(y_i - \hat{y_i})x_i }</script><p>$ w $的更新公式为:</p><script type="math/tex; mode=display">w = w + \eta \sum_{i=1}^{n}{(y_i- \hat{y_i})x_i }</script><p>同理, $ b $的梯度如下:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial b} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial b}} = - \sum_{i=1}^{n} {(y_i - \hat{y_i}) }</script><p>b的更新公式如下:</p><script type="math/tex; mode=display">b = b + \eta \sum_{i=1}^{n}{(y_i- \hat{y_i}) }</script><h2 id="交叉熵损失函数以及softmax函数的梯度推导"><a href="#交叉熵损失函数以及softmax函数的梯度推导" class="headerlink" title="交叉熵损失函数以及softmax函数的梯度推导"></a>交叉熵损失函数以及softmax函数的梯度推导</h2><p>&nbsp;&nbsp;交叉熵用来衡量两个分布之间的距离,假设两个分布$ p $和$ q $,交叉熵的计算公式如下：</p><script type="math/tex; mode=display">H(p,q) = \sum_{i=1}^{n} {p_i \log \frac{1}{q_i}} = \sum_{i=1}^{n} {- p_i \log q_i}</script><p>&nbsp;&nbsp;softmax的输出是一个概率分布，而真实标签的one-hot向量也是一个概率分布，真实类别的概率为1，其余类别的概率是0，所以用交叉熵来衡量两个分布之间的距离作为损失函数是合适的，其实该损失函数也可以使用极大似然估计推导出来，下面是推导过程:</p><script type="math/tex; mode=display">L = - \ln(\prod_{i=1}^{n} {\prod_{j=1}^{c} { \hat{y_{ij}}^{y_{ij}}} }) = \sum_{i=1}^{n} { [\sum_{j=1}^{c} {-y_{ij} \ln \hat{y_{ij}}} ] }</script><p>其实数据集的损失是每个数据点的损失之和，[]内部就是交叉熵，下面我们推导损失函数的梯度，首先我们假设全连接中$ z_{ij} = w_j * x_i + b_j  $ , 这里的 $ wj $ 表示所有与输出层的第$ j $个神经元连接的权值，这些权值构成一个向量:</p><script type="math/tex; mode=display">\frac{\partial L}{\partial w_j } = \sum_{i=1}^{n} {\frac{ \partial L }{ \partial z_{ij} } \frac{\partial z_{ij} }{\partial w_j }}</script><script type="math/tex; mode=display">\frac{ \partial L }{ \partial z_{ij} } = \sum_{k=1}^{c} { \frac{ \partial L}{ \partial \hat{y_{ik}} } \frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} }</script><script type="math/tex; mode=display">\frac{\partial L }{\partial \hat{y_{ik}}} = \frac{ \partial [ -y_{ik} \ln \hat{y_{ik}}] }{ \partial \hat{y_{ik}}}= \frac{-y_{ik}}{ \hat{y_{ik}}}</script><p>下面得分两种情况考虑:<br>如果 $ k=j  $,</p><script type="math/tex; mode=display">\frac{ \partial \hat{y_{ik}}}{\partial z_{ij}}  = \frac{ \partial \frac{e^{z_{ij}}}{ \sum_{q=1}^{c} {e^{z_{iq}}}} }{ \partial z_{ij} } = \frac{ e^{z_{ij}} \sum_{q=1}^{c}{ e^{z_{iq}}} - e^{z_{ij}} e^{z_{ij}}  }{ [ \sum_{q=1}^{c} {e^{z_{iq}}} ]^{2} } = \hat{y_{ij}} (1- \hat{y_{ij}} )</script><p>如果 $ k \ne j $,</p><script type="math/tex; mode=display">\frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} =  \frac{ \partial \frac{e^{z_{ik}}}{ \sum_{q=1}^{c} {e^{z_{iq}}}} }{ \partial z_{ij} } = \frac{ 0- e^{z_{ik}} e^{z_{ij}} }{ [ \sum_{q=1}^{c} {e^{z_{iq}}} ]^{2} } = - \hat{y_{ik}} \hat{y_{ij}}</script><p>将上面的两个式子综合一下，得到如下的式子：</p><script type="math/tex; mode=display">\frac{ \partial L }{ \partial z_{ij} } = - y_{ij} (1- \hat{ y_{ij} })  + \sum_{k \ne j}^{c}{ y_{ik} \hat{y_{ij}} } = -y_{ij} + \hat{y_{ij}} \sum_{q=1}^{c} {y_{ik}} = \hat{ y_{ij}} -y_{ij}</script><p>因为:</p><script type="math/tex; mode=display">\frac{\partial z_{ij}}{ \partial w_j } = x_i</script><script type="math/tex; mode=display">\frac{\partial z_{ij}}{ \partial b_j } = 1</script><p>综上,$ w_j $的更新公式如下：</p><script type="math/tex; mode=display">w_j = w_j - \eta \sum_{i=1}^{n}{ ( \hat{ y_{ij}} -y_{ij}) x_i }</script><p>同理,b的更新公式如下：</p><script type="math/tex; mode=display">b_j = b_j - \eta \sum_{i=1}^{n}{ ( \hat{ y_{ij}} -y_{ij}) }</script><p>上面的公式可以由下面的代码来验证:</p><p><img src="/images/cross_entropy/code.png" alt title="code"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>]]></content>
      
      
      
        <tags>
            
            <tag> deep learning </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
