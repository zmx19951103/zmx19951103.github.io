<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="what is softmax&amp;nbsp;&amp;nbsp;softmax通常用于神经网络的输出层，用于多分类任务, 为每个类别产生一个概率，公式如下: softmax(\overrightarrow{z})=\overrightarrow{s}\overrightarrow{z} \in R^n , \overrightarrow{s} \in R^ns_i = \frac{e^{z_i}}{ \su">
<meta name="keywords" content="deep learning">
<meta property="og:type" content="article">
<meta property="og:title" content="逻辑回归交叉熵损失函数梯度推导">
<meta property="og:url" content="http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/index.html">
<meta property="og:site_name" content="ZMX&#39;s Blog">
<meta property="og:description" content="what is softmax&amp;nbsp;&amp;nbsp;softmax通常用于神经网络的输出层，用于多分类任务, 为每个类别产生一个概率，公式如下: softmax(\overrightarrow{z})=\overrightarrow{s}\overrightarrow{z} \in R^n , \overrightarrow{s} \in R^ns_i = \frac{e^{z_i}}{ \su">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/cross_entropy/sigmoid.png">
<meta property="og:image" content="http://yoursite.com/images/cross_entropy/code.png">
<meta property="og:updated_time" content="2019-08-05T01:53:25.335Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="逻辑回归交叉熵损失函数梯度推导">
<meta name="twitter:description" content="what is softmax&amp;nbsp;&amp;nbsp;softmax通常用于神经网络的输出层，用于多分类任务, 为每个类别产生一个概率，公式如下: softmax(\overrightarrow{z})=\overrightarrow{s}\overrightarrow{z} \in R^n , \overrightarrow{s} \in R^ns_i = \frac{e^{z_i}}{ \su">
<meta name="twitter:image" content="http://yoursite.com/images/cross_entropy/sigmoid.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>逻辑回归交叉熵损失函数梯度推导</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">tags</a></li>
         
          <li><a href="/links/">links</a></li>
         
          <li><a href="https://github.com/zmx19951103">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/07/09/CUDA-Hello-World/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&text=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&is_video=false&description=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=逻辑回归交叉熵损失函数梯度推导&body=Check out this article: http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&name=逻辑回归交叉熵损失函数梯度推导&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-softmax"><span class="toc-number">1.</span> <span class="toc-text">what is softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-logistic-regression"><span class="toc-number">2.</span> <span class="toc-text">what is logistic regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sigmoid损失函数"><span class="toc-number">3.</span> <span class="toc-text">sigmoid损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#极大似然估计"><span class="toc-number">4.</span> <span class="toc-text">极大似然估计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#逻辑回归的损失函数推导"><span class="toc-number">5.</span> <span class="toc-text">逻辑回归的损失函数推导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#交叉熵损失函数以及softmax函数的梯度推导"><span class="toc-number">6.</span> <span class="toc-text">交叉熵损失函数以及softmax函数的梯度推导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">7.</span> <span class="toc-text">参考</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        逻辑回归交叉熵损失函数梯度推导
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">ZMX's Blog</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-07-06T05:44:14.000Z" itemprop="datePublished">2019-07-06</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/deep-learning/">deep learning</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="what-is-softmax"><a href="#what-is-softmax" class="headerlink" title="what is softmax"></a>what is softmax</h2><p>&nbsp;&nbsp;softmax通常用于神经网络的输出层，用于多分类任务, 为每个类别产生一个概率，公式如下:</p>
<script type="math/tex; mode=display">softmax(\overrightarrow{z})=\overrightarrow{s}</script><script type="math/tex; mode=display">\overrightarrow{z} \in R^n , \overrightarrow{s} \in R^n</script><script type="math/tex; mode=display">s_i = \frac{e^{z_i}}{ \sum_{k=1}^{n} {e^{z_k}} }</script><h2 id="what-is-logistic-regression"><a href="#what-is-logistic-regression" class="headerlink" title="what is logistic regression"></a>what is logistic regression</h2><p>&nbsp;&nbsp;逻辑回归是一个分类模型,假设输入$ \overrightarrow{x} \in R^n $, 模型参数$ \overrightarrow{w} \in R^n $, $ b \in R $ ,真实标签$ y \in \{0,1\} $ ,模型的预测输出是$ \hat{y} \in R $ 表示模型预测该实例为1的概率:</p>
<script type="math/tex; mode=display">P(Y=1|x) =  \hat{y} = sigmoid(w*x+b) = \frac{1}{1+e^{-(w*x+b)}} =\frac{e^{w*x+b}}{1+e^{w*x+b}}</script><p>从而:</p>
<script type="math/tex; mode=display">P(Y=0|x) = \frac{1}{1+e^{w*x+b}}</script><h2 id="sigmoid损失函数"><a href="#sigmoid损失函数" class="headerlink" title="sigmoid损失函数"></a>sigmoid损失函数</h2><p>&nbsp;&nbsp;sigmoid损失函数的公式和形状如下，它能将输入的范围转化到[0,1]之间,作为概率值.</p>
<script type="math/tex; mode=display">sigmoid(z) = \frac{1}{1+e^{-z}}</script><p><img src="/images/cross_entropy/sigmoid.png" alt title="sigmoid"></p>
<h2 id="极大似然估计"><a href="#极大似然估计" class="headerlink" title="极大似然估计"></a>极大似然估计</h2><p>&nbsp;&nbsp;极大似然估计是一种模型参数的估计方法，是频率学派的方法（另一个是贝叶斯学派，它们的参数估计方法是最大后验概率估计,两者的区别在于极大似然估计假设模型参数是一个固定值而最大后验概率估计假设模型参数也服从一定的分布，称为先验分布). 极大似然估计假设模型的参数是$ \Theta $,那么我们观察到的数据可以表示成参数的函数$ f(xi|\Theta) $,一般我们是有一个数据集，根据这个数据集合来估计模型参数，那么使得这个数据集合被我们观察到的概率是</p>
<script type="math/tex; mode=display">f(x_1,x_2,x_3,...x_n|\Theta)</script><p>因为数据集的数据是独立同分布的，所以下面的式子成立:</p>
<script type="math/tex; mode=display">f(x_1,x_2,x_3,...x_n|\Theta) = \prod_{i=1}^{n} {f(x_i|\Theta)}</script><p>上面的式子就是似然函数，它是一个关于参数$ \Theta $的函数,而我们要选取使得该似然函数取得最大值的参数，这就是极大似然估计,也就是选取参数使得我们观察到的结果的可能性最大.</p>
<h2 id="逻辑回归的损失函数推导"><a href="#逻辑回归的损失函数推导" class="headerlink" title="逻辑回归的损失函数推导"></a>逻辑回归的损失函数推导</h2><p>&nbsp;&nbsp;逻辑回归的参数估计可以采用极大似然法，似然函数如下：</p>
<script type="math/tex; mode=display">\prod_{i=1}^{n} {P(Y=1|x)^{y_i}P(Y=0|x)^{1-y_i} } = \prod_{i=1}^{n} {\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}}</script><p>我们使用负对数似然，对数函数不改变单调性，取负数使得该值大于0，那么损失函数如下：</p>
<script type="math/tex; mode=display">L = - \ln( \prod_{i=1}^{n} {\hat{y_i}^{y_i}(1-\hat{y_i})^{1-y_i}} ) = - \sum_{i=1}^{n} {y_i\ln\hat{y_i}+(1-y_i)\ln(1-\hat{y_i})}</script><p>我们采用梯度下降法sgd来求解模型参书$ w $, $ b $:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial w}}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial \hat{y_i} }  = - [ y_i \frac{\partial \ln\hat{y_i}}{\partial \hat{y_i}}+(1-y_i) \frac{\partial \ln(1- \hat{y_i})}{\partial \hat{y_i}} ] = - [ y_i \frac{1}{ \hat{y_i} } +(1-y_i) \frac{-1}{1- \hat{y_i}} ]  = - \frac{y_i - \hat{y_i} }{ \hat{y_i} (1- \hat{y_i})}</script><script type="math/tex; mode=display">\frac{ \partial \hat{y_i} }{ \partial w} = \hat{y_i}(1- \hat{y_i} ) x_i</script><p>综上:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial w}} = - \sum_{i=1}^{n} {(y_i - \hat{y_i})x_i }</script><p>$ w $的更新公式为:</p>
<script type="math/tex; mode=display">w = w + \eta \sum_{i=1}^{n}{(y_i- \hat{y_i})x_i }</script><p>同理, $ b $的梯度如下:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial b} = \sum_{i=1}^{n} {\frac{\partial L}{\partial \hat{y_i}} \frac{\partial \hat{y_i}}{\partial b}} = - \sum_{i=1}^{n} {(y_i - \hat{y_i}) }</script><p>b的更新公式如下:</p>
<script type="math/tex; mode=display">b = b + \eta \sum_{i=1}^{n}{(y_i- \hat{y_i}) }</script><h2 id="交叉熵损失函数以及softmax函数的梯度推导"><a href="#交叉熵损失函数以及softmax函数的梯度推导" class="headerlink" title="交叉熵损失函数以及softmax函数的梯度推导"></a>交叉熵损失函数以及softmax函数的梯度推导</h2><p>&nbsp;&nbsp;交叉熵用来衡量两个分布之间的距离,假设两个分布$ p $和$ q $,交叉熵的计算公式如下：</p>
<script type="math/tex; mode=display">H(p,q) = \sum_{i=1}^{n} {p_i \log \frac{1}{q_i}} = \sum_{i=1}^{n} {- p_i \log q_i}</script><p>&nbsp;&nbsp;softmax的输出是一个概率分布，而真实标签的one-hot向量也是一个概率分布，真实类别的概率为1，其余类别的概率是0，所以用交叉熵来衡量两个分布之间的距离作为损失函数是合适的，其实该损失函数也可以使用极大似然估计推导出来，下面是推导过程:</p>
<script type="math/tex; mode=display">L = - \ln(\prod_{i=1}^{n} {\prod_{j=1}^{c} { \hat{y_{ij}}^{y_{ij}}} }) = \sum_{i=1}^{n} { [\sum_{j=1}^{c} {-y_{ij} \ln \hat{y_{ij}}} ] }</script><p>其实数据集的损失是每个数据点的损失之和，[]内部就是交叉熵，下面我们推导损失函数的梯度，首先我们假设全连接中$ z_{ij} = w_j * x_i + b_j  $ , 这里的 $ wj $ 表示所有与输出层的第$ j $个神经元连接的权值，这些权值构成一个向量:</p>
<script type="math/tex; mode=display">\frac{\partial L}{\partial w_j } = \sum_{i=1}^{n} {\frac{ \partial L }{ \partial z_{ij} } \frac{\partial z_{ij} }{\partial w_j }}</script><script type="math/tex; mode=display">\frac{ \partial L }{ \partial z_{ij} } = \sum_{k=1}^{c} { \frac{ \partial L}{ \partial \hat{y_{ik}} } \frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} }</script><script type="math/tex; mode=display">\frac{\partial L }{\partial \hat{y_{ik}}} = \frac{ \partial [ -y_{ik} \ln \hat{y_{ik}}] }{ \partial \hat{y_{ik}}}= \frac{-y_{ik}}{ \hat{y_{ik}}}</script><p>下面得分两种情况考虑:<br>如果 $ k=j  $,</p>
<script type="math/tex; mode=display">\frac{ \partial \hat{y_{ik}}}{\partial z_{ij}}  = \frac{ \partial \frac{e^{z_{ij}}}{ \sum_{q=1}^{c} {e^{z_{iq}}}} }{ \partial z_{ij} } = \frac{ e^{z_{ij}} \sum_{q=1}^{c}{ e^{z_{iq}}} - e^{z_{ij}} e^{z_{ij}}  }{ [ \sum_{q=1}^{c} {e^{z_{iq}}} ]^{2} } = \hat{y_{ij}} (1- \hat{y_{ij}} )</script><p>如果 $ k \ne j $,</p>
<script type="math/tex; mode=display">\frac{ \partial \hat{y_{ik}}}{\partial z_{ij}} =  \frac{ \partial \frac{e^{z_{ik}}}{ \sum_{q=1}^{c} {e^{z_{iq}}}} }{ \partial z_{ij} } = \frac{ 0- e^{z_{ik}} e^{z_{ij}} }{ [ \sum_{q=1}^{c} {e^{z_{iq}}} ]^{2} } = - \hat{y_{ik}} \hat{y_{ij}}</script><p>将上面的两个式子综合一下，得到如下的式子：</p>
<script type="math/tex; mode=display">\frac{ \partial L }{ \partial z_{ij} } = - y_{ij} (1- \hat{ y_{ij} })  + \sum_{k \ne j}^{c}{ y_{ik} \hat{y_{ij}} } = -y_{ij} + \hat{y_{ij}} \sum_{q=1}^{c} {y_{ik}} = \hat{ y_{ij}} -y_{ij}</script><p>因为:</p>
<script type="math/tex; mode=display">\frac{\partial z_{ij}}{ \partial w_j } = x_i</script><script type="math/tex; mode=display">\frac{\partial z_{ij}}{ \partial b_j } = 1</script><p>综上,$ w_j $的更新公式如下：</p>
<script type="math/tex; mode=display">w_j = w_j - \eta \sum_{i=1}^{n}{ ( \hat{ y_{ij}} -y_{ij}) x_i }</script><p>同理,b的更新公式如下：</p>
<script type="math/tex; mode=display">b_j = b_j - \eta \sum_{i=1}^{n}{ ( \hat{ y_{ij}} -y_{ij}) }</script><p>上面的公式可以由下面的代码来验证:</p>
<p><img src="/images/cross_entropy/code.png" alt title="code"></p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2>
  </div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">tags</a></li>
         
          <li><a href="/links/">links</a></li>
         
          <li><a href="https://github.com/zmx19951103">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-softmax"><span class="toc-number">1.</span> <span class="toc-text">what is softmax</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#what-is-logistic-regression"><span class="toc-number">2.</span> <span class="toc-text">what is logistic regression</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#sigmoid损失函数"><span class="toc-number">3.</span> <span class="toc-text">sigmoid损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#极大似然估计"><span class="toc-number">4.</span> <span class="toc-text">极大似然估计</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#逻辑回归的损失函数推导"><span class="toc-number">5.</span> <span class="toc-text">逻辑回归的损失函数推导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#交叉熵损失函数以及softmax函数的梯度推导"><span class="toc-number">6.</span> <span class="toc-text">交叉熵损失函数以及softmax函数的梯度推导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">7.</span> <span class="toc-text">参考</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&text=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&is_video=false&description=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=逻辑回归交叉熵损失函数梯度推导&body=Check out this article: http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&title=逻辑回归交叉熵损失函数梯度推导"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/07/06/逻辑回归交叉熵损失函数梯度推导/&name=逻辑回归交叉熵损失函数梯度推导&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2019 Zmx
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">tags</a></li>
         
          <li><a href="/links/">links</a></li>
         
          <li><a href="https://github.com/zmx19951103">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'zmxdream';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>


</body>
</html>
