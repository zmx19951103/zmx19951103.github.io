<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <meta name="description" content="Streaming MultiprocessorsGPU是由流多处理器构成的，每个SM内部有多个core, 每个core跑一个thread.  The idea is that the CPU spawns a thread per element, and the GPU then executes those threads.Not all of the thousands or millio">
<meta name="keywords" content="CUDA Programming">
<meta property="og:type" content="article">
<meta property="og:title" content="CUDA线程层次-从硬件和软件角度">
<meta property="og:url" content="http://yoursite.com/2019/07/10/GPU架构/index.html">
<meta property="og:site_name" content="ZMX&#39;s Blog">
<meta property="og:description" content="Streaming MultiprocessorsGPU是由流多处理器构成的，每个SM内部有多个core, 每个core跑一个thread.  The idea is that the CPU spawns a thread per element, and the GPU then executes those threads.Not all of the thousands or millio">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://yoursite.com/images/gpu_arch/SMs.png">
<meta property="og:image" content="http://yoursite.com/images/gpu_arch/inside_1.png">
<meta property="og:image" content="http://yoursite.com/images/gpu_arch/thread_block_org.png">
<meta property="og:image" content="http://yoursite.com/images/gpu_arch/thread_cooperate.png">
<meta property="og:image" content="http://yoursite.com/images/gpu_arch/uint3.png">
<meta property="og:image" content="http://yoursite.com/images/gpu_arch/dim3.png">
<meta property="og:image" content="http://yoursite.com/images/gpu_arch/result.png">
<meta property="og:image" content="http://yoursite.com/images/gpu_arch/cudaDeviceRest.png">
<meta property="og:updated_time" content="2019-07-25T16:22:16.138Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CUDA线程层次-从硬件和软件角度">
<meta name="twitter:description" content="Streaming MultiprocessorsGPU是由流多处理器构成的，每个SM内部有多个core, 每个core跑一个thread.  The idea is that the CPU spawns a thread per element, and the GPU then executes those threads.Not all of the thousands or millio">
<meta name="twitter:image" content="http://yoursite.com/images/gpu_arch/SMs.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-192x192.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>CUDA线程层次-从硬件和软件角度</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">tags</a></li>
         
          <li><a href="/links/">links</a></li>
         
          <li><a href="https://github.com/zmx19951103">Projects</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2019/07/12/LSTM-RNN-Bi-LSTM-On-LSTM/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2019/07/09/CUDA-Hello-World/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/07/10/GPU架构/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/07/10/GPU架构/&text=CUDA线程层次-从硬件和软件角度"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/07/10/GPU架构/&is_video=false&description=CUDA线程层次-从硬件和软件角度"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=CUDA线程层次-从硬件和软件角度&body=Check out this article: http://yoursite.com/2019/07/10/GPU架构/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/07/10/GPU架构/&name=CUDA线程层次-从硬件和软件角度&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Streaming-Multiprocessors"><span class="toc-number">1.</span> <span class="toc-text">Streaming Multiprocessors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Warps"><span class="toc-number">1.1.</span> <span class="toc-text">Warps</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#thread-block-organization"><span class="toc-number">2.</span> <span class="toc-text">thread block organization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#uint3和Dim3-数据结构"><span class="toc-number">3.</span> <span class="toc-text">uint3和Dim3 数据结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#uint3"><span class="toc-number">3.1.</span> <span class="toc-text">uint3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dim3"><span class="toc-number">3.2.</span> <span class="toc-text">dim3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#block-and-grid-dimensions"><span class="toc-number">3.3.</span> <span class="toc-text">block and grid dimensions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#运行结果"><span class="toc-number">3.4.</span> <span class="toc-text">运行结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cudaDeviceReset"><span class="toc-number">3.5.</span> <span class="toc-text">cudaDeviceReset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        CUDA线程层次-从硬件和软件角度
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">ZMX's Blog</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2019-07-09T16:36:31.000Z" itemprop="datePublished">2019-07-10</time>
        
      
    </div>


      

      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/CUDA-Programming/">CUDA Programming</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h2 id="Streaming-Multiprocessors"><a href="#Streaming-Multiprocessors" class="headerlink" title="Streaming Multiprocessors"></a>Streaming Multiprocessors</h2><p>GPU是由流多处理器构成的，每个SM内部有多个core, 每个core跑一个thread.</p>
<blockquote>
<p>The idea is that the CPU spawns a thread per element, and the GPU then executes those threads.Not all of the thousands or millions of threads actually run in parallel, but many do.Specifically, an NVIDIA GPU contains several largely independent processors called “Streaming Multiprocessors” (SMs), each SM hosts several “cores”, and each “core” runs a thread.For instance, Fermi has up to 16 SMs with 32 cores per SM – so up to 512 thread can run in parallel.</p>
<p>All threads running on the cores of an SM at a given cycle are executing the same instruction – hence Single Instruction, Multiple Threads. However, each thread has its own registers, so these instructions process different data.</p>
</blockquote>
<p><img src="/images/gpu_arch/SMs.png" alt title="SMs"><br>&nbsp;&nbsp;可以看到一个thread block跑在一个SM上面. 一个block执行完毕，SM可以调度另一个block执行，在一个SM上,block的执行是顺序的.</p>
<p><img src="/images/gpu_arch/inside_1.png" alt title="inside a gpu"><br>&nbsp;&nbsp;一个SM只有一个指令单元，SM内的所有thread共享这个指令单元.</p>
<h3 id="Warps"><a href="#Warps" class="headerlink" title="Warps"></a>Warps</h3><p>&nbsp;&nbsp;一个thread block在一个SM上执行,block中的thread可以继续分成warp,一个warp包括32个线程.<strong>warp是SM调度运行的基本单元</strong>,一个warp中的所有线程执行相同的指令.<strong>任意时刻，SM中只会有一个warp在运行，其余的warp都处于就绪等其他状态</strong>. 不同架构的gpu一个SM包含不同数量的cuda核心，Turing架构的一个SM包含128个cuda core,也就是4个warp. SM中有硬件warp scheduler，用来调度warp运行. </p>
<h2 id="thread-block-organization"><a href="#thread-block-organization" class="headerlink" title="thread block organization"></a>thread block organization</h2><p><img src="/images/gpu_arch/thread_block_org.png" alt title="thread block organization"><br>&nbsp;&nbsp;grid由多个blocks组成，每个kernel function的调用都会create一个grid,所以&lt;&lt;&lt;…&gt;&gt;&gt;语法指定的是block的数量和每个block的线程数量,因为这个kernel函数只会在一个grid上运行.</p>
<p><img src="/images/gpu_arch/thread_cooperate.png" alt title="thread cooperate"><br>&nbsp;&nbsp;同一个block内的所有thread可以合作(因为有个Shared Memory??)，不同block的thread不可以合作.</p>
<p>&nbsp;&nbsp;从上图中可以看到每个block都有一个Shared Memory,是所有block中的thread所共享的,每个thread有自己的Local Memory和Registers. 一个Grid中的所有blocks共享Global Memory, Constant Memory以及Texture Memory. 所有blocks中的thread都共享这三种Memory,内存层次会在后序的博客中进行总结.</p>
<h2 id="uint3和Dim3-数据结构"><a href="#uint3和Dim3-数据结构" class="headerlink" title="uint3和Dim3 数据结构"></a>uint3和Dim3 数据结构</h2><h3 id="uint3"><a href="#uint3" class="headerlink" title="uint3"></a>uint3</h3><p><img src="/images/gpu_arch/uint3.png" alt title="uint3"></p>
<h3 id="dim3"><a href="#dim3" class="headerlink" title="dim3"></a>dim3</h3><p><img src="/images/gpu_arch/dim3.png" alt title="dim3"></p>
<h3 id="block-and-grid-dimensions"><a href="#block-and-grid-dimensions" class="headerlink" title="block and grid dimensions"></a>block and grid dimensions</h3><figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cuda_runtime.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line">__<span class="function">global__ <span class="keyword">void</span> <span class="title">checkIndex</span><span class="params">(<span class="keyword">void</span>)</span> </span>&#123;</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"threadIdx:(%d, %d, %d) blockIdx:(%d, %d, %d) blockDim:(%d, %d, %d) "</span></span><br><span class="line"><span class="string">"gridDim:(%d, %d, %d)\n"</span>, threadIdx.x, threadIdx.y, threadIdx.z, blockIdx.x, blockIdx.y, blockIdx.z, blockDim.x, blockDim.y, blockDim.z, gridDim.x,gridDim.y,gridDim.z);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">int</span> argc, <span class="keyword">char</span> **argv)</span> </span>&#123; <span class="comment">// define total data element</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">int</span> nElem = <span class="number">6</span>;</span><br><span class="line"><span class="comment">// define grid and block structure</span></span><br><span class="line"></span><br><span class="line"><span class="function">dim3 <span class="title">block</span> <span class="params">(<span class="number">3</span>)</span></span>;</span><br><span class="line"><span class="function">dim3 <span class="title">grid</span> <span class="params">((nElem+block.x<span class="number">-1</span>)/block.x)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// check grid and block dimension from host side</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"grid.x %d grid.y %d grid.z %d\n"</span>,grid.x, grid.y, grid.z);</span><br><span class="line"><span class="built_in">printf</span>(<span class="string">"block.x %d block.y %d block.z %d\n"</span>,block.x, block.y, block.z);</span><br><span class="line"></span><br><span class="line"><span class="comment">// check grid and block dimension from device side </span></span><br><span class="line">checkIndex&lt;&lt;&lt;grid, block&gt;&gt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="comment">// reset device before you leave</span></span><br><span class="line">cudaDeviceReset();</span><br><span class="line"><span class="keyword">return</span>(<span class="number">0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>&nbsp;&nbsp;可以看到上面的程序定义了两个dim3类型的变量block和grid. 因为nElem的值是6,所以grid的x分量是2,因为这两个变量都只是指定了x分量，所以其余的分量都初始化为1.目前来看&lt;&lt;…&gt;&gt;&gt;传入的两个参数都是dim3类型的，之前的程序中直接传入int类型的,它应该会转换成dim3类型的.<br>&nbsp;&nbsp;<strong>一个很直观的想法是可以把一个grid想像成一个三维的直角坐标系，然后一个block就是坐标系中的一个点,如果指定gridDim的x,y,z分别为2,3,4那么就有24个block,每个block可以用一个三维坐标来表示,同理可以可以把一个block想象成一个直角坐标系，thread也是其中的点.</strong></p>
<h3 id="运行结果"><a href="#运行结果" class="headerlink" title="运行结果"></a>运行结果</h3><p><img src="/images/gpu_arch/result.png" alt title="result.png"></p>
<h3 id="cudaDeviceReset"><a href="#cudaDeviceReset" class="headerlink" title="cudaDeviceReset"></a>cudaDeviceReset</h3><p><img src="/images/gpu_arch/cudaDeviceRest.png" alt title="cudaDeviceRest"><br>可以看到这个函数是用来<strong>销毁一个CUDA的上下文的</strong>. It will reset the device immediately.虽然它表现上具有同步的功能，但是靠这个函数来同步是不安全的，所以建议如果是想同步的话，使用cudaDeviceSynchronize或者cudaMemcpy.</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>&nbsp;&nbsp;这篇博客总结了cuda线程的组织层次，从逻辑上讲一个kernel launch启动一个grid，可以用&lt;&lt;&lt;…&gt;&gt;&gt;语法设置gridDim和blockDim，从3个维度上进行设置. 一个grid包含多个block，这就是线程的两个层次的组织结构. 从硬件角度来讲，gpu的计算核心是多个SMs（streaming multiprocessors)，一个block在一个SM上运行，SM内部包含多个cuda core(不同的gpu架构的数目不同，Turing包含128个),32个cuda core构成一个warp.任意时刻一个SM中只会有一个warp处于活跃状态，其他的warp处于就绪或者挂起状态，从这点来看, SM非常类似于cpu中的core.</p>
<h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://pan.baidu.com/s/1AveSDmaRpR6ZyVisNTHm1A" target="_blank" rel="noopener">Programming Massively Parallel Processors</a><br><a href="https://stackoverflow.com/questions/36012289/what-is-the-role-of-cudadevicereset-in-cuda" target="_blank" rel="noopener">What is the role of cudaDeviceReset() in Cuda
</a><br><a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#scalable-programming-model" target="_blank" rel="noopener">CUDA C Programming Guide</a></p>

  </div>
</article>

    <div class="blog-post-comments">
        <div id="disqus_thread">
            <noscript>Please enable JavaScript to view the comments.</noscript>
        </div>
    </div>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">tags</a></li>
         
          <li><a href="/links/">links</a></li>
         
          <li><a href="https://github.com/zmx19951103">Projects</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Streaming-Multiprocessors"><span class="toc-number">1.</span> <span class="toc-text">Streaming Multiprocessors</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Warps"><span class="toc-number">1.1.</span> <span class="toc-text">Warps</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#thread-block-organization"><span class="toc-number">2.</span> <span class="toc-text">thread block organization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#uint3和Dim3-数据结构"><span class="toc-number">3.</span> <span class="toc-text">uint3和Dim3 数据结构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#uint3"><span class="toc-number">3.1.</span> <span class="toc-text">uint3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#dim3"><span class="toc-number">3.2.</span> <span class="toc-text">dim3</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#block-and-grid-dimensions"><span class="toc-number">3.3.</span> <span class="toc-text">block and grid dimensions</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#运行结果"><span class="toc-number">3.4.</span> <span class="toc-text">运行结果</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cudaDeviceReset"><span class="toc-number">3.5.</span> <span class="toc-text">cudaDeviceReset</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#总结"><span class="toc-number">4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#参考"><span class="toc-number">5.</span> <span class="toc-text">参考</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://yoursite.com/2019/07/10/GPU架构/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://yoursite.com/2019/07/10/GPU架构/&text=CUDA线程层次-从硬件和软件角度"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://yoursite.com/2019/07/10/GPU架构/&is_video=false&description=CUDA线程层次-从硬件和软件角度"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=CUDA线程层次-从硬件和软件角度&body=Check out this article: http://yoursite.com/2019/07/10/GPU架构/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://yoursite.com/2019/07/10/GPU架构/&title=CUDA线程层次-从硬件和软件角度"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://yoursite.com/2019/07/10/GPU架构/&name=CUDA线程层次-从硬件和软件角度&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 Zmx
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/about/">About</a></li>
         
          <li><a href="/archives/">Writing</a></li>
         
          <li><a href="/tags/">tags</a></li>
         
          <li><a href="/links/">links</a></li>
         
          <li><a href="https://github.com/zmx19951103">Projects</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

    <!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Disqus Comments -->

    <script type="text/javascript">
        var disqus_shortname = 'zmxdream';

        (function(){
            var dsq = document.createElement('script');
            dsq.type = 'text/javascript';
            dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        }());
    </script>


</body>
</html>
